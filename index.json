[{"categories":["CentOS"],"content":" 因为每次都要去网上找教程 (😂), 所以记录一下自己的配置过程 VMware 默认配置就为 NAT 模式 NAT 模式 关闭防火墙 # 关闭 \u003e systemctl stop firewalld # 禁用 \u003e systemctl disable firewalld # 查看状态 \u003e systemctl status firewalld 禁用 selinux # 将 SELINUX 设置为 `disabled` \u003e vi /etc/selinux/config 禁用 selinux 安全增强型 Linux（Security-Enhanced Linux）简称 SELinux, 它是一个 Linux 内核模块, 也是 Linux 的一个安全子系统. 获取 mac vmnet8 的 gateway 地址 \u003e cat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf 找到 # NAT gateway address 一行, 然后记下 ip 和 netmask 修改虚拟机中的网卡配置 \u003e vi /etc/sysconfig/network-scripts/ifcfg-ens33 ifcfg-ens33 上图红框为修改内容, 绿框为新增内容 BOOTPROTO=static ONBOOT=yes IPADDR=172.16.143.101 GATEWAY=172.16.143.2 NETMASK=255.255.255.0 DNS1=114.114.114.114 DNS2=8.8.8.8 # 其中 GATEWAY 为第 4 步中的 ip, NETMASK 为第 4 步 中的 netmask 重启虚拟机网卡 \u003e systemctl restart network ping 一下外网就可以啦! \u003e ping www.baidu.com 修改主机名 \u003e vi /etc/hostname # 将其中内容删除, 改为: `buli_server1` \u003e vi /etc/hosts /etc/hosts 重启 \u003e reboot -f ","date":"2023-07-11","objectID":"/mac_vmware_set_net/:0:0","tags":["CentOS","Operating System","Network"],"title":"mac 配置 VMware 的 CentOS 网络 (NAT 模式)","uri":"/mac_vmware_set_net/"},{"categories":["MySQL"],"content":"之前项目里一直在用的 SeaTunnel 版本是 2.1.3, 有些旧了. 而且 Spark 和 Flink 的脚本还要写两套. 所以就在考虑要升级到 v2.3.2. 所以最近在做 SeaTunnel v2.3.2 的性能调研. 之前做 v2.3.0 的调研的时候就发现在写入 MySQL 的时候性能特别低, 写入速度竟然 200/s. 有些不能接受… 因为 v2.1.3 写入 MySQL 的速度可以达到 2000+/s. 但是当时也没有深究这个问题, 就搁置了. 最近这个问题又被提起的时候, 就再次去社区找找看有没有类似的问题. 然后发现了也有人面临着这个问题: issue. 里面提出在链接 URI 后面加上参数 rewriteBatchStatements=true. 但是这个参数在这里的提高的并不明显.. 我甚至觉得这里增加这个参数之后, 其实并没有提升. 😂 这个参数的意义是数据库会更高性能的执行批量处理. 当写入的时候, 再去查看服务器的时候发现, 硬盘的 IO 有明显的增高. 于是就猜想, 是不是 v2.3.2 版本里, 写入的时候, 每条都作为一个事务提交的, 然后就想到了 innodb_flush_log_at_trx_commit 这个参数, 在补充这部分的知识的时候, 发现了也可以调整一下相关的参数sync_binlog 以达到调优的目的. 于是就有了这篇博客记录一下. ","date":"2023-07-03","objectID":"/increase_write/:0:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["MySQL"],"content":"innodb_flush_log_at_trx_commit 这个参数是用来配置 MySQL 日志何时写入硬盘的参数. 查看 MySQL 配置: SHOW VARIABLES LIKE 'innodb_flush_log_at_trx_commit'; 0: 日志缓存区将每隔 1 秒写到日志中, 并且将日志文件的数据刷新 (flush) 到磁盘上. 该模式下在事务提交时不会主动触发写入磁盘的操作. 1: 每次提交事务时, MySQL 都会把日志文件的数据写入, 并且刷新到磁盘上, 默认为该模式. 2: 每次提交事务时, MySQL 都会把日志文件的数据写入, 但是刷新到磁盘的操作不会同时进行, 而是每秒执行一次刷新到磁盘的操作. 所以说: 当设置为 0 的时候, 速度最快, 但是不安全, mysqld 进程的崩溃会导致上一秒所有事务数据的丢失. 当设置为 1 的时候, 速度最慢, 但是最安全. 在 mysqld 服务崩溃或服务器崩溃的情况下, 日志只可能丢失最多一个语句或一个事务. 当设置为 2 的时候, 速度快, 也比设置为 0 时安全, 只有在操作系统崩溃或系统断电的情况下, 上一秒所有事务数据才可能丢失. 所以我将 MySQL 改成该模式. ","date":"2023-07-03","objectID":"/increase_write/:1:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["MySQL"],"content":"sync_binlog 默认情况下, 并不是每次写入时都将 binlog 日志文件与磁盘同步. 因此如果操作系统或服务器崩溃. 有可能 binlog 中最后的语句丢失. 为了防止这种情况, 可以使用 sync_binlog 全局变量 (1 是最安全的值, 但也是最慢的), 使 binlog 在每 N 次 binlog 日志文件写入后与磁盘同步. 查看 MySQL 配置: SHOW VARIABLES LIKE 'sync_binlog'; 所以最终, 我将 innodb_flush_log_at_trx_commit 设置为 2, sync_binlog 设置为 100 . 然后经过测试, v2.3.2 对于 MySQL 的写入速度达到了 2000+/s 的速度. 😄 ","date":"2023-07-03","objectID":"/increase_write/:2:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["Java"],"content":" 传送门: JJTree ","date":"2023-06-13","objectID":"/jjtree/:0:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"概述 JJTree 是 JavaCC 的预处理器, 可在 JavaCC 源代码的不同位置插入解析树构建操作. JJTree 的输出通过 JavaCC 运行以创建解析器. 此文档描述了如何使用 JJTree, 以及如何将解析器连接到 JJTree. 默认情况下, JJTree 生成代码来为语言中的每个非终节点结构解析书节点. 但是可以修改这个行为, 为了某些非终节点不会生成节点, 或者为生产扩展的一部分生成节点. JJTree 定义了一个 Java 接口 Node , 并且所有解析树节点都必须实现它. 这个接口提供了例如设置父节点、添加和检索子节点等操作的方法. JJTree 以两种模式之一运行, 简单模式和多模式 (需要更好的术语). 在简单模式下, 每个解析树节点都是具体类型的 SimpleNode, 在多模式下, 解析树节点的类型源自节点的名称. 如果不提供节点类的实现, 那么 JJTree 会生成基于 SimpleNode 的示例实现. 之后可以修改该实现来进行适配. 虽然 JavaCC 是一个自上而下的解析器, 但是 JJTree 是自下而上地构造解析树. 为此, 它使用一个堆栈, 在创建节点后推送节点. 当它为它们找到父级时, 它会从堆栈中弹出 (pop) 子级并将它们添加到父级, 并且最后推送新的父节点本身. 堆栈是开放的, 这意味着可以从语法操作中访问它: 可以用认为合适的方式进行推送 (push), 弹出 (pop)和其他方式操作其内容 (详细信息可以参考 节点范围和用户操作 ) JJTree 提供了两种基本类型的节点的装饰, 以及一些语法速记以方便它们的使用. 确定节点 一个确定节点是用特定数量的子节点构成的. 许多节点从堆栈中弹出并成为新节点的子节点, 然后将其推送到堆栈本身上. 可以标记一个确定节点, 类似这样: #ADefiniteNode(INTEGER EXPRESSION) 确定节点描述符表达式可以是任何整数表达式, 尽管文字整数常量是迄今为止最常见的表达式. 条件节点 当且仅当条件计算为真 (true) 时才使用其节点范围内被推送到堆栈的所有子节点构造条件节点. 如果计算条件为假 (false) 时, 该节点不会被构造, 并且其所有子节点会留在节点堆栈中. 可以标记一个条件节点, 类似这样: #ConditionalNode(BOOLEAN EXPRESSION) 条件节点描述符表达式可以是任何布尔表达式. 条件节点有两种常见的简写: 1. 不定节点 (Indefinite nodes) #IndefiniteNode is short for #IndefiniteNode(true) 2. 大于节点 (Greater-than nodes) #GTNode(\u003e1) is short for #GTNode(jjtree.arity() \u003e 1) 不定节点速记 (1) 在后面跟着带括号的展开时可能会导致 JJTree 源中的歧义. 在这种情况下, 速记必须替换为完整表达式: ( ... ) #N ( a() ) 上面这个是有歧义的, 所以必须使用显示条件: ( ... ) #N(true) ( a() ) 注意: 节点描述符表达式不应该有副作用. JJTree没有指定表达式将被计算多少次. 默认情况下, JJTree 将每个非终节点视为不定节点, 并从其生产名称派生节点的名称. 可以使用以下语法为其命名: void P1() #MyNode : { ... } { ... } 当解析器识别出一个 P1 非终节点时, 它会开始一个不定节点. 它标记堆栈, 以便在 P1 扩展中由非终节点创建并推送到堆栈上的任何解析树节点都将被弹出并成为节点 MyNode 的子节点. 如果要禁止为生产创建节点, 可以使用以下语法: void P2() #void : { ... } { ... } 现在, 在 P2 的扩展中由非终节点推送的任何解析树节点都将保留在堆栈上, 以便弹出并使其成为树上更向上的生产子节点. 可以使用 NODE_DEFAULT_VOID 选项将此设置为未装饰节点的默认行为. void P3() : {} { P4() ( P5() )+ P6() } 在这个例子中, 开始一个不定节点 P3, 标记堆栈, 然后解析一个 P4 节点、一个或多个 P5 节点和一个 P6 节点. 可以进一步自定义生成的树: void P3() : {} { P4() ( P5() )+ #ListOfP5s P6() } 现在, P3 节点将有一个 P4 节点、一个 ListOfP5s 节点和一个 P6 节点作为子节点. #Name 构造充当后缀运算符, 其作用域是紧接在前的扩展单元. ","date":"2023-06-13","objectID":"/jjtree/:1:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点范围和用户操作 每个节点都与一个节点作用域 (node scope) 相关联. 此范围内的用户操作可以通过使用特殊标识符 jjtThis 来访问正在构建的节点, 以引用该节点. 该标识符被隐式声明为该节点的正确类型, 因此可以轻松地访问该节点拥有的任何字段和方法. 作用域是紧靠在节点修饰之前的扩展单元. 这可以是带括号的表达式. 当对生产签名进行装饰时 (可能隐式带有默认节点), 作用域是生产的整个右侧, 包括其声明块. 还可以在扩展引用的左侧使用涉及 jjtThis 的表达式. 🌰: ... ( jjtThis.my_foo = foo() ) #Baz ... 这里的 jjtThis 指向的是一个 Baz 节点, 他有一个名为 my_foo 的字段. 将解析生成的 foo() 的结果赋给 myfoo. 节点范围内的最终用户操作与所有其他操作不同. 当其中的代码执行时, 节点的子节点已经从堆栈中弹出并添加到节点中, 节点本身已被推送到堆栈中. 现在, 可以通过节点的方法 (比如 jjtGetChild()) 来访问子节点. 除最后一个操作之外的其他用户操作只能访问堆栈上的子级. 它们还没有被加入到节点中, 所以还不能通过节点的方法进行访问. 如果条件节点的节点描述符表达式的计算结果为 false, 则不会将其添加到堆栈中, 也不会向其添加子节点. 条件节点范围内的最终用户操作可以通过调用 nodeCreated() 方法来确定节点是否被创建. 只有当节点的条件被满足且被创建且被推入到节点堆栈中时才返回 true, 否则返回 false. ","date":"2023-06-13","objectID":"/jjtree/:1:1","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"异常处理 由节点作用域中的扩展抛出的异常不在该节点作用域内捕获, 由 JJTree 本身捕获. 发生这种情况时, 任何已被压入节点范围内的堆栈的节点都将被弹出并丢弃. 然后重新抛出异常. 这么做的目的是使解析器能够实现错误恢复, 并在已知状态下继续使用节点堆栈. 注意: JJTree 目前无法检测节点作用域内的用户操作是否引发异常. 这样的异常可能会被错误地处理. ","date":"2023-06-13","objectID":"/jjtree/:1:2","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点作用域钩子 如果 NODE_SCOPE_HOOK 设置为 true, 那么 JJTree 将在每个节点作用域的入口和出口上生成对两个用户定义的解析器方法的调用. 这些方法必须具有以下声明: void jjtreeOpenNodeScope(Node n) void jjtreeCloseNodeScope(Node n) 如果解析器是 STATIC, 那么这些方法也必须被声明为 static. 它们都以当前节点作为参数调用. 一种用途可能是将解析器对象本身存储在节点中, 以便可以提供应该由该解析器生成的所有节点共享的状态. 🌰, 解析器可能维护一个符号表. void jjtreeOpenNodeScope(Node n) { ((SimpleNode)n).jjtSetValue(getSymbolTable()); } void jjtreeCloseNodeScope(Node n) {} 其中 getSymbolTable() 是一个用户自定义的方法, 用来返回节点的符号表的结构. ","date":"2023-06-13","objectID":"/jjtree/:1:3","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"跟踪令牌 跟踪每个节点的最初和最后的令牌通常很有用, 这样就可以轻松地再次复制输入. 通过设置 TRACK_TOKENS 选项, 生成的 SimpleNode 类将包含 4 个额外的方法: public Token jjtGetFirstToken() public void jjtSetFirstToken(Token token) public Token jjtGetLastToken() public void jjtSetLastToken(Token token) 当解析器运行的时候, 将自动设置每个节点的最初和最后的令牌. ","date":"2023-06-13","objectID":"/jjtree/:1:4","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点的生命周期 节点在创建的时候将进行一系列的确定的步骤. 这是从节点本身的角度来看的序列: 使用唯一的整数参数调用节点的构造函数. 此参数标识节点的类型, 在简单模式下特别有用. JJTree 会自动生成一个叫做 \u003cparser\u003eTreeConstants.java 的文件, 用来声明有效常量. 常量的命名是通过在节点的大写名称前加上 JJT 来派生的, 点 (.) 替换为下划线 (_). 为方便起见, 在同一文件中维护了一个名为 jjtNodeName[] 的字符串数组, 它将常量映射到未修改的节点名称. 节点的 jjtOpen() 方法被调用. 如果设置了 NODE_SCOPE_HOOK 选项, 用户自定义解析器方法 openNodeScope() 会被调用, 并且节点作为参数进行传递. 该方法可以初始化节点中的字段或调用其方法. 🌰, 它可以存储节点的第一个令牌. 如果在解析节点时抛出未处理的异常, 则将该节点抛弃. JJTree 再也不会引用它了. 它不会被关闭, 并且不会使用它作为参数来调用用户定义的节点作用域钩子 CloseNodeHook(). 否则, 如果该节点是有条件的, 并且其条件表达式的计算结果为 false, 则该节点将被放弃. 它不会被关闭, 尽管可以使用它作为参数调用用户定义的节点作用域钩子 CloseNodeHook(). 否则, 由确定节点的整数表达式指定的节点的所有子节点, 或在条件节点范围内推送到堆栈上的所有节点都将添加到该节点. 它们添加的顺序是不确定的. 节点的 jjtClose() 方法被调用. 节点被推到堆栈中. 如果设置了 NODE_SCOPE_HOOK 选项, 用户自定义解析器方法 closeNodeScope() 会被调用, 并且节点作为参数进行传递. 如果节点不是根节点, 那么它将被添加为另一个节点的子节点, 并调用其 jjtSetParent() 方法. ","date":"2023-06-13","objectID":"/jjtree/:1:5","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"访客支持 JJTree 为访问者设计模式提供了一些基本支持. 如果 VISITOR 选项设置为 true, JJTree 将把 jjtAccept() 方法插入到它生成的所有节点类中, 并且还生成可以实现并传递给节点以接受的访问者接口. 访问者接口的命名是通过将 Visitor 附加到解析器的名称来构造的. 每次运行 JJTree 时都会重新生成接口, 以便准确表示解析器使用的节点集. 如果没有为新节点更新实现类, 这将导致编译时错误. 这是一个功能. ","date":"2023-06-13","objectID":"/jjtree/:1:6","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"选项 选项 缺省值 描述 BUILD_NODE_FILES true 为 SimpleNode 和语法中使用的任何其他节点生成示例实现. MULTI false 生成多模式解析树. 缺省值为 false, 生成一个简单模式解析树. NODE_DEFAULT_VOID false 与其使每个未装饰的产品成为不定节点, 不如使其无效. NODE_CLASS \"\" 如果设置定义将扩展 SimpleNode 的用户提供的类名称, 那么创建的任何树节点都将是 NODE_CLASS 的子类. NODE_FACTORY \"\" 指定一个包含工厂方法的类, 该方法具有以下签名以构造节点: public static Node jjtCreate(int id). 为了向后兼容, 也可以指定值 false, 这意味着 SimpleNode 将用作工厂类. NODE_PACKAGE \"\" 生成节点类的包. 默认为解析器包. NODE_EXTENDS \"\" 弃用. SimpleNode 类的超类. 通过提供自定义超类, 可以避免编辑生成的SimpleNode.java. NODE_PREFIX \"AST\" 用于在多模式下从节点标识符构造节点类名的前缀. NODE_SCOPE_HOOK false 在每个节点作用域的入口和出口上生成对两个用户定义的解析器方法的调用. NODE_USES_PARSER false JJTree 将使用另一种形式的节点构造例程, 在其中传递解析器对象. 🌰: public static Node MyNode.jjtCreate(MyParser p, int id);MyNode(MyParser p, int id); TRACK_TOKENS false 在 SimpleNode 中插入 jjtGetFirstToken(), jjtSetFirstToken(), getLastToken() 和 jjtSetLastToken() 方法. FirstToken 在进入节点作用域时自动设置; LastToken 在退出节点作用域时自动设置. STATIC true 为静态解析器生成代码. 这必须与等效的 JavaCC 选项一致使用. 该选项的值在 JavaCC 源中发出. VISITOR false 在节点类中插入一个 jjtAccept() 方法, 并为语法中使用的每个节点类型生成一个包含条目的访问者实现. VISITOR_DATA_TYPE \"Object\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中用作数据参数的类型. VISITOR_RETURN_TYPE \"Object\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中用作返回参数的类型. VISITOR_EXCEPTION \"\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中. JJTREE_OUTPUT_DIRECTORY \"OUTPUT_DIRECTORY\" 默认情况下, JJTree 在全局 OUTPUT_DIRECTORY 设置中指定的目录中生成其输出. 显式设置此选项允许用户将解析器与树文件分开. ","date":"2023-06-13","objectID":"/jjtree/:1:7","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"JJTree 接口 ","date":"2023-06-13","objectID":"/jjtree/:2:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"JJTree 状态 JJTree 将其状态保存在一个名为 jjtree 的解析器类字段中. 可以使用此成员中的方法来操作节点堆栈. final class JJTreeState { // 调用它来重新初始化节点堆栈 void reset(); // 返回抽象语法树 (AST) 的根节点 Node rootNode(); // 判断当前节点是否实际关闭并推送 boolean nodeCreated(); // 返回节点上当前推送的节点数 // 当前节点作用域内的堆栈 int arity(); // 推送节点到堆栈 void pushNode(Node n); // 返回堆栈顶部的节点, 并将其从堆栈中移除 Node popNode(); // 返回当前位于堆栈顶部的节点 Node peekNode(); } ","date":"2023-06-13","objectID":"/jjtree/:2:1","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点对象 /** * 所有 AST 节点必须实现该接口. * 它为构建节点之间的父子关系提供了基本机制. */ public interface Node { // 将节点设为当前节点后调用此方法. // 表示现在可以向其中添加子节点. public void jjtOpen(); // 添加所有子节点后调用此方法. public void jjtClose(); // 这对方法用于通知节点其父节点. public void jjtSetParent(Node n); public Node jjtGetParent(); // 此方法通知节点将其参数添加到节点的子节点列表中. public void jjtAddChild(Node n, int i); // 此方法返回子节点. // 子节点编号从 0 开始, 从左至右. public Node jjtGetChild(int i); // 返回节点拥有的子节点数. int jjtGetNumChildren(); } SimpleNode 类实现了 Node 接口, 如果它不存在, 则由 JJTree 自动生成. 可以使用该类作为节点实现的模板或超类, 或修改其进行适配. SimpleNode 还提供了递归转储节点及其子节点的基本机制. 可以像这样使用这个命令: { ((SimpleNode)jjtree.rootNode()).dump(\"\u003e\"); } dump() 的字符串参数用作填充, 以指示树层次结构. 如果设置了 VISITOR 选项, 则会生成另一个实用程序方法: { public void childrenAccept(MyParserVisitor visitor); } 这会轮流遍历节点的子节点, 要求它们接受访问者. 这在实现前序和后序遍历时很有用. ","date":"2023-06-13","objectID":"/jjtree/:2:2","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":" 因为最近的项目里会有大量的 SQL 生成/解析, 所以就打算自己写一个. 于是就有了今天的这篇 blog. 传送门: JavaCC ","date":"2023-06-13","objectID":"/javacc/:0:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"概述 JavaCC 的全称是 Java Compiler Compiler (没错就是俩 Compiler), 是用于 Java 应用程序的最流行的解析器生成器. 它是一个工具, 用来读取语法规范并将其转换为可以识别与语法匹配的 Java 程序. 除了解析器生成器本身之外, JavaCC 和提供与解析器生成相关的其他标准功能, 例如树构建 (通过 JavaCC 中包含的名为 JJTree 的工具, 这个工具之后会重点用他来完成 SQL 语法解析)、操作和调试. ","date":"2023-06-13","objectID":"/javacc/:1:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"特点 JavaCC 的生成是自上而下 (递归下降) 解析器, 而不是像类似 YACC 的工具生成的自下而上的解析器. 这就允许使用更通用的语法, 尽管做递归是不被允许的. 自上而下的解析器还有许多的有点 (除了更通用的语法之外), 例如更容易调试, 能够解析语法中的任何非终端, 以及能够向上传递值 (属性) 并且在解析期间沿着解析树向下移动. 默认情况下, JavaCC 生成一个 LL(1) 解析器. 但是, 可能有部分语法不是 LL(1). JavaCC 提供了句法和语义前瞻的能力, 能够在这些点直接解决轮班歧义 (shift-shift ambiguities). 🌰, 解析器仅在这些点是 LL(k), 但在其他的地方都保持 LL(1) 以获得更好的性能表现. 移位归约 (Shift-reduce) 和 reduce-reduce 冲突对于自上而下的解析器来说不是问题. JavaCC 生成百分之百纯 Java 的解析器, 因此对 JavaCC 没有运行时依赖性, 也不需要在不同机器平台上运行的特殊移植工作. JavaCC 允许在此法和语法规范中扩展巴克斯范式 (extended BNF), 例如 (A)*, (A)+ 等等. 扩展巴克斯范式在一定程度上减少了对于左递归的需求. 实际上, 扩展巴克斯范式通常更容易阅读, 例如 A::=y(x)* 与 A::=Ax|y. 词法规范 (比如正则表达式、字符串) 和语法规范 (比如巴克斯范式) 都写在同一个文件中. 这样使得语法更易读, 因为可以在语法规范中内联使用正则表达式, 并且也更易于维护. JavaCC 的词法分析器可以处理完整的 Unicode 输入, 并且词法规范也可以包括任何 Unicode 字符. 这个特性有助于描述语言元素, 例如允许某些 Unicode 字符 (不是 ASCII ) 但不允许其他字符的 Java 标识符. JavaCC 提供了类 Lex 的词法状态和词法动作功能. JavaCC 中优于其他公祖的特定方面是一流状态, 它提供了诸如 TOKEN, MORE, SKIP 和状态更改等概念. 这就允许更清晰的规范以及来自 JavaCC 的更好的错误和警告消息. 词法规范中顶一个为特殊标记的标记 (Token) 在解析期间将被忽略. 一个有用的应用是处理评论. 最后一句话, 没看明白, 原文是: A useful application of this is in the processing of comments. 词法规范可以在整个词法规范的全局级别或在单个词法规范的基础上定义不区分大小写的标记. JavaCC 带有 JJTree, 一个非常强大的树构建预处理器. JavaCC 还包括 JJDoc, 这是一种将愈发文件转换为文档文件的工具, 可选择采用 HTML 格式. JavaCC 提供了许多自定义它的行为和生成的解析器的行为的选项. 此类选型的示例是对输入流执行的 Unicode 处理的种类、要执行的歧义检查的标记数等. JavaCC 的错误报告在解析器生成器中也是前列的. JavaCC 生成的解析器能够通过完整的诊断信息清楚地解析错误的位置. 通过使用 DEBUG_PARSER, DEBUG_LOOKAHEAD 和 DEBUG_TOKEN_MANAGER 选项, 用户可以深入分析解析和 token 处理步骤. JavaCC 版本包含范围广泛的实例, 包括 Java 和 HTML 语法. 这些实例及其文档是熟悉 JavaCC 的好方法. ","date":"2023-06-13","objectID":"/javacc/:2:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"示例 这个示例识别匹配的大括号后跟零个或多个行终止符, 然后是文件结尾. 此语法中合法字符串的示例是: {}, }}} 等等 非法字符串的例子是: {}{}, }{}}, { }, {x} 等等 ","date":"2023-06-13","objectID":"/javacc/:3:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"语法 PARSER_BEGIN(Example) /** 简单大括号匹配器 */ public class Example { /** 主入口 */ public static void main(String args[]) throws ParseException { Example parser = new Example(System.in); parser.Input(); } } PARSER_END(Example) /** 根生产 */ void Input() : {} { \"{\" [ MatchedBaraces() ] \"}\" } ","date":"2023-06-13","objectID":"/javacc/:3:1","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"输出 $ java Example \u003creturn\u003e $ java Example {x\u003creturn\u003e Lexical error at line 1, column 2. Encountered: \"x\" TokenMgrError: Lexical error at line 1, column 2. Encountered: \"x\" (120), after : \"\" at ExampleTokenManager.getNextToken(ExampleTokenManager.java:146) at Example.getToken(Example.java:140) at Example.MatchedBraces(Example.java:51) at Example.Input(Example.java:10) at Example.main(Example.java:6) $ java Example {}}\u003creturn\u003e ParseException: Encountered \"}\" at line 1, column 3. Was expecting one of: \u003cEOF\u003e \"\\n\" ... \"\\r\" ... at Example.generateParseException(Example.java:184) at Example.jj_consume_token(Example.java:126) at Example.Input(Example.java:32) at Example.main(Example.java:6) ","date":"2023-06-13","objectID":"/javacc/:3:2","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["big data"],"content":"简介 在大多数生产环境中, 密码等敏感配置项需要加密, 不能以明文存储, SeaTunnel 为此提供了便捷的一站式解决方案. ","date":"2023-05-25","objectID":"/seatunnel_encryption/:1:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"使用方法 SeaTunnel 自带 base64 加解密功能, 但不建议用于生产使用, 建议用户实现自定义加解密逻辑, 可以参考本章 如何实现用户自定义加解密 获取更多关于它的详细信息. Base64 加密支持加密以下参数: username password auth 接下来展示怎么使用 base64 进行加密: 在配置文件中添加新的配置项 shade.identifier, 这个配置项指示要使用的加密方法, 在这个示例中, 应该在配置中添加 shade.identifier = base64, 如下所示： # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # env { execution.parallelism = 1 shade.identifier = \"base64\" } source { MySQL-CDC { result_table_name = \"fake\" parallelism = 1 server-id = 5656 port = 56725 hostname = \"127.0.0.1\" username = \"seatunnel\" password = \"seatunnel_password\" database-name = \"inventory_vwyw0n\" table-name = \"products\" base-url = \"jdbc:mysql://localhost:56725\" } } transform { } sink { # choose stdout output plugin to output data to console Clickhouse { host = \"localhost:8123\" database = \"default\" table = \"fake_all\" username = \"seatunnel\" password = \"seatunnel_password\" # cdc options primary_key = \"id\" support_upsert = true } } 使用基于不同计算引擎的命令行来加密配置文件, 🌰中使用 Zeta (SeaTunnel 自研): \u003e ${SEATUNNEL_HOME}/bin/seatunnel.sh --config config/v2.batch.template --encrypt 然后就可以在命令行中看到加密的配置: { \"env\" : { \"execution.parallelism\" : 1, \"shade.identifier\" : \"base64\" }, \"source\" : [ { \"base-url\" : \"jdbc:mysql://localhost:56725\", \"hostname\" : \"127.0.0.1\", \"password\" : \"c2VhdHVubmVsX3Bhc3N3b3Jk\", \"port\" : 56725, \"database-name\" : \"inventory_vwyw0n\", \"parallelism\" : 1, \"result_table_name\" : \"fake\", \"table-name\" : \"products\", \"plugin_name\" : \"MySQL-CDC\", \"server-id\" : 5656, \"username\" : \"c2VhdHVubmVs\" } ], \"transform\" : [], \"sink\" : [ { \"database\" : \"default\", \"password\" : \"c2VhdHVubmVsX3Bhc3N3b3Jk\", \"support_upsert\" : true, \"host\" : \"localhost:8123\", \"plugin_name\" : \"Clickhouse\", \"primary_key\" : \"id\", \"table\" : \"fake_all\", \"username\" : \"c2VhdHVubmVs\" } ] } 当然, 不仅支持加密配置文件, 如果用户想查看解密后的配置文件, 可以执行以下命令: \u003e ${SEATUNNEL_HOME}/bin/seatunnel.sh --config config/v2.batch.template --decrypt ","date":"2023-05-25","objectID":"/seatunnel_encryption/:2:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"如何实现用户自定义加解密 如果想自定义加密方法和加密配置, 本节将帮助来解决问题. 创建一个 Java 的 maven 项目 添加 seatunnel-api 模块在 pom.xml中: \u003cdependency\u003e \u003cgroupId\u003eorg.apache.seatunnel\u003c/groupId\u003e \u003cartifactId\u003eseatunnel-api\u003c/artifactId\u003e \u003cversion\u003e${seatunnel.version}\u003c/version\u003e \u003c/dependency\u003e 创建一个新的类, 并实现 ConfigShade 接口, 这个接口有下列方法: /** * The interface that provides the ability to encrypt and decrypt {@link * org.apache.seatunnel.shade.com.typesafe.config.Config} */ public interface ConfigShade { /** * The unique identifier of the current interface, used it to select the correct {@link * ConfigShade} */ String getIdentifier(); /** * Encrypt the content * * @param content The content to encrypt */ String encrypt(String content); /** * Decrypt the content * * @param content The content to decrypt */ String decrypt(String content); /** To expand the options that user want to encrypt */ default String[] sensitiveOptions() { return new String[0]; } } 在 resources/META-INF/services 添加 org.apache.seatunnel.api.configuration.ConfigShade. 将其打成 jar 包, 并添加到 ${SEATUNNEL_HOME}/lib. 将选项 shade.identifier 的值更改为上面定义在配置文件中的 ConfigShade#getIdentifier 的值. 完成! ","date":"2023-05-25","objectID":"/seatunnel_encryption/:3:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"连接器 V2 与 V1 的区别 自从 https://github.com/apache/incubator-seatunnel/issues/1608 之后添加了连接器 V2 功能. 连接器 V2 是基于 Seatunnel Connector API 接口定义的. 和连接器 V1 不同的是, 连接器 V2 支持以下功能: 多引擎支持: SeaTunnel Connector API 是一套独立与引擎的 API. 在这套 API 的基础上进行研发的连接器 V2 是可以运行在多个引擎上的. 现在是支持 Flink 和 Spark 的, 未来还会支持更多的引擎. 多引擎版本支持: 通过翻译层 (translation layer) 将连接器与引擎解耦, 解决了大多数连接器需要修改代码才能支持新版本的底层引擎的问题. 统一批处理和流处理: 连接器 V2 支持批处理和流处理. 不需要分别开发批处理和流处理的连接器. 多路复用的 JDBC/Log 连接器: 连接器 V2 支持 JDBC 资源重用和共享数据库日志解析. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:1:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"源连接器特点 (Source) 源连接器拥有一些共同的特点, 并且每种连接器不同程度的支持它们. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"exactly-once (精确一次) 如果数据源中的每条数据只会被源发送到下游一次, 则认为这个源连接器支持只写一次. 在 SeaTunnel 中, 可以在检查点时将读取的分割 (Split) 及其偏移量 (offset) (当时拆分中读取数据的位置, 如行号、字节大小、偏移量等) 保存为状态快照 (StateSnapshot) . 如果任务重新启动, 我们将获得最后一个状态快照, 然后定位上次读取的分割和偏移量并继续向下游发送数据. 例如: File, Kafka ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:1","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"column projection (列投影) 如果连接器支持仅从数据源读取指定的列 (注意! 如果先读取所有的列, 然后通过 schema 过滤不需要的列, 那么这种不是真正的列投影). 例如 JDBC 源 可以用 sql 定义读取的列. Kafka 源 会读取指定主题的所有内容, 然后使用 schema 过滤不需要的列, 这个就不是列投影. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:2","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"batch (批) 批处理模式, 数据读取是有边界的, 当所有数据被读取完之后作业就会结束. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:3","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"stream (流) 流处理模式, 数据读取是无边界的, 并且作业是不会停止的. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:4","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"parallelism (并行度) 并行源连接器支持配置 parallelism, 每个并行度都回创建一个任务来读取数据. 在并行源连接器中, 源会被拆分为多个, 然后通过枚举器 (enumerator) 分配给源阅读器 (SourceReader) 进行处理. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:5","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"support user-defined split (支持用户定义分割) 用户可以配置分割规则. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:6","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"接收连接器特点 (Sink) 接收连接器拥有一些共同的特点, 并且每种连接器不同程度的支持它们. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"exactly-once (精确一次) 当任何数据流入分布式系统时, 如果系统在整个处理过程中只对任何一条数据准确地处理了一次, 并且处理结果是正确的, 则认为系统满足了精确的一次一致性. 对于接收连接器, 如果任何数据只写入目标一次, 则接收连接器支持精确一次. 通常有两种方法可以实现这一点: 目标数据源支持键消重 (key deduplication), 例如: MySQL, Kudu. 目标支持 XA Transaction (此事务可以跨会话使用. 即使创建事务的程序已经结束了, 新创建的程序只需要知道上一个事务的 ID 来重新提交或回滚事务). 所以可以使用两段提交 (Two-phase Commit) 来确保 exactly-once. 例如: File, MySQL. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:1","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"cdc (change data capture 变更数据捕获) 如果接收连接器支持基于主键的写入行类型 (INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE), 那么则认为支持 cdc. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:2","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["DatasourceX"],"content":"前提 之前项目里用的数据源连接都是 DatasourceX , 但是这个项目从2022年5月之后就没有维护过了, 其中也有一些bug都没有修复. 所以想着要不要自己写一个, 或者再看看有没有其他的. 前两天在ChatGPT上询问了一下有没有类似的工具的时候, 推荐了同样是袋鼠云的 Taier , 突然发现从 Taier 的 1.3.0 版本之后 DatasourceX 就移到这个项目中了. 所以就想着从中剥离出来 DatasourceX 放到自己的项目中去. 现在记录一下这个过程. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:1","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"开始 第一步肯定是先把代码下载下来: (这里下载的是1.3.0-RELEASE) \u003e git clone https://github.com/DTStack/Taier.git -b 1.3.0-RELEASE 把代码下载下来之后找到 DatasoruceX 的目录: taier-datasource 这个模块中有两个子模块: taier-datasource-api 和 taier-datasource-plugin. 其中: taier-datasource-api 这个就是需要剥离出来的接口. taier-datasource-plugin 这个里面就是各种数据源的依赖, 现在就不要这个了, 因为 GitHub 上已经下载好了现成的: plugins 传送门 在打包之前先修改一下 taier-datasource.pom.xml: 将 slf4j-api 的 \u003cscope\u003e 去掉, 否则打完包会报找不到 slf4j 的错: \u003cdependency\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-api\u003c/artifactId\u003e \u003cversion\u003e1.7.21\u003c/version\u003e \u003c/dependency\u003e 在 \u003cbuild\u003e 中加入 maven-assembly-plugin 插件, 目的是将 taier-datasource-api 的依赖也打到包里面去: \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-assembly-plugin\u003c/artifactId\u003e \u003cversion\u003e2.4.1\u003c/version\u003e \u003cconfiguration\u003e \u003cdescriptorRefs\u003e \u003cdescriptorRef\u003ejar-with-dependencies\u003c/descriptorRef\u003e \u003c/descriptorRefs\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003emake-assembly\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e 这样打包之后会同时生成一个后面带 jar-with-dependencies 的包, 这个就是我们需要的了. mvn clean install 或者 mvn clean package. 这样就打包成功了. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:2","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"导入 这样就生成了一个本地的 jar 包, 这里我用的导入方法是先将这个 jar 包导入到本地的 maven 仓库中. 导入到 maven 本地仓库: \u003e mvn install:install-file -Dfile=上面打包的jar包路径 -DgroupId=com.dtstack.taier -DartifactId=taier.datasource.x -Dversion=1.0.0 -Dpackaging=jar 在项目的 pom 中引入: \u003cdependency\u003e \u003cgroupId\u003ecom.dtstack.taier\u003c/groupId\u003e \u003cartifactId\u003etaier.datasource.x\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:3","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"配置 这里需要对插件的载入重写一下: import com.dtstack.taier.datasource.api.base.ClientCache; import com.dtstack.taier.datasource.api.constant.ConfigConstants; import com.dtstack.taier.datasource.api.context.ClientEnvironment; import com.dtstack.taier.datasource.api.manager.list.ClientManager; import com.dtstack.taier.datasource.api.config.Configuration; private void p_initDatasourcePluginsDir() { String pluginsDir = 这里是插件路径 Map\u003cString, Object\u003e config = new HashMap\u003c\u003e(); config.put(ConfigConstants.PLUGIN_DIR, pluginsDir); Configuration configuration = new Configuration(config); ClientEnvironment clientEnvironment = new ClientEnvironment(configuration); clientEnvironment.start(); ClientCache.setEnv(clientEnvironment.getManagerFactory().getManager(ClientManager.class)); } 至此, 这个就已经把 Taier 中的 DatasourceX 移进来了. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:4","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["tools"],"content":" IINA 作为一款开源的 Swift 编写的播放器, 我愿称之为 mac 最好用的播放器 ","date":"2023-04-21","objectID":"/iina/:0:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装IINA IINA的安装可以点开上面的地址进行安装. ","date":"2023-04-21","objectID":"/iina/:1:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装youtube-dl \u003e brew install youtube-dl 安装好之后可以也可以把ffmpeg安装一下 \u003e brew install ffmpeg ","date":"2023-04-21","objectID":"/iina/:2:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"配置IINA 配置IINA youtube-dl路径: /usr/local/bin 额外参数: format=\"bestvideo[height\u003c=?480]+bestaudio/best[height\u003c=?480]\" 参数意义为播放视频分辨率 ","date":"2023-04-21","objectID":"/iina/:2:1","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装IINA浏览器插件 Open In IINA - Chrome 应用商店 (google.com) ","date":"2023-04-21","objectID":"/iina/:2:2","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["big data"],"content":" 搭建 CDH v6.2.0 (离线版) ","date":"2023-03-01","objectID":"/cdh/:0:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"简介 ","date":"2023-03-01","objectID":"/cdh/:1:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"官网介绍 CDH是Cloudera的100%开源平台发行版, 包括Apache Hadoop, 专为满足企业需求而构建. CDH 提供开箱即用的企业使用所需的一切. 通过将 Hadoop与十几个其他关键的开源项目集成, Cloudera 创建了一个功能先进的系统, 可帮助您执行端到端的大数据工作流程. ","date":"2023-03-01","objectID":"/cdh/:1:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"简单来说 CDH 是一个拥有集群自动化安装、中心化管理、集群监控、报警功能的一个工具(软件), 使得集群的安装可以从几天的时间缩短到几个小时, 运维人数也会从几个人降低到几个人, 极大的提高了集群管理的效率. 提示: 所有的操作, 均在root用户下进行!!! ","date":"2023-03-01","objectID":"/cdh/:1:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"环境准备 ","date":"2023-03-01","objectID":"/cdh/:2:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"集群机器 虚拟机IP hostname Hadoop MySQL Spark ZooKeeper 192.168.100.101 cdh1 ✅ ✅ ✅ ✅ 192.168.100.102 cdh2 ✅ ✅ ✅ 192.168.100.103 cdh3 ✅ ✅ ✅ ","date":"2023-03-01","objectID":"/cdh/:2:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"准备安装包 jdk: oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm MySQL: mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar mysql-connector-java-5.1.47.jar cloudera-repos-6.2.0: cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm parcel-6.2.0: CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha PHOENIX-1.0.jar PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel.sha manifest.json 下载地址: 百度网盘 ","date":"2023-03-01","objectID":"/cdh/:2:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"环境安装 如果没关防火墙, 关一下: systemctl stop firewalld.service 因为这是我自己的虚拟机, 所以我索性让防火墙不启动了: systemctl disable firewalld.service ","date":"2023-03-01","objectID":"/cdh/:3:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"修改 hostname 和 hosts (所有节点) 设置 hostname [root@localhost ~]# vi /etc/hostname cdh1 [root@localhost ~]# vi /etc/sysconfig/network # Created by anaconda NETWORKING=yes HOSTNAME=server4 设置 hosts [root@localhost ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.100.101 cdh1 192.168.100.102 cdh2 192.168.100.103 cdh3 重启! [root@localhost ~]# reboot 同样的操作在三台服务器上! 记得对应不同的 hostsname 这里之后我还是习惯用vim, 所以就安装了vim: yum install -y vim ","date":"2023-03-01","objectID":"/cdh/:3:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"SSH免密登录 (所有节点) 执行指令: [root@cdh1 ~]# ssh-keygen -t rsa -P \"\" Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:jGyqRTlDiqwQOP/Id2hVDhKNuigVipeSrkafdVXzP4s root@cdh1 The key's randomart image is: +---[RSA 2048]----+ | .o | |. . ... o | |=o +o . . . o | |*==o + * . . | |+*o.= = S . | |=+.+ O . o | |= + X o . o| |.. B . E . | |. . | +----[SHA256]-----+ 查看生成的公钥和私钥 [root@cdh1 ~]# cd .ssh/ [root@cdh1 .ssh]# ll 总用量 8 -rw-------. 1 root root 1679 3月 1 08:20 id_rsa -rw-r--r--. 1 root root 391 3月 1 08:20 id_rsa.pub 生成 authorized_keys 文件, 并把公钥刚进去 [root@cdh1 .ssh]# cp id_rsa.pub authorized_keys 然后将 cdh2 和 cdh3 的公钥拷贝进去 [root@cdh1 .ssh]# cat authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRqj0kcgwR68Ci2otjFu5QYQe/wBlyDpX9eI68XU6q6XzyxTrvLkbEpDpHkNwMpmqfr+y/tOMpk5/dAIBUn3pOiRKBpXVDxA4xZkpIpC1PXHi3BxY8zz58QXgBQbm1tFxFplAzkCjlxsYE08Oq0X/xD3vR6T4ZRsFfMdKMo7R2LmyoPohDTf8aiiqvE6ftF+Tv8YmNdb0TKbNwgD76f5vatrYhRZ+XitHCC8NGffDyOA62ogkd04G9mXOKYEhvsu7eYHa98xddhiXmK0mdeBRcV4BfeEkIEBdlcYl1LpYNjKR1oePFzudfG7czeKltgdPTqy5wPvAEPic9HBw3t2St root@cdh1 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDEhVBXXBXUiz21kxafL7FadgIrKgNJlvds+Gwf5G9fB+28kXyDgvRFL5F1twCGlIakt266K2kuZioffcwWJ3Co5N6XlyF/ABHq4cxjD1js9uKbXVENDove8CCR8zCIMmojJqEZO53TUljJjau+TEQQrXBaSUcQ6dkmMSEo7XuPYbCukXcZY4xX4tXX4XQyUFnX0T3xGkFkgHZ6JjPMBEo1deWpJn0igt4LEZcdfBNCywmZr4bIFDVnpER7gmZJB15AW1cEqy8++2qL5a/acee85olRzLeGWFZTkP4XRH5Rqkva7U924f+8uWZQefz7VTlcdruhmLJNhN+DJntrL/Sr root@cdh2 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsBR+l8JM2NMR9JJ8XeQaFV50VFwto6ZJkkdygj6fQPbGms7XYQLTutNRaERXgQPCNgfz2Y7ZncIu0zNoK5ibmFH8qU1RGvpkscTX0AaBnrRtpESBH5R/Gn0ZCqsjSE829GC+VkSxwHgKkrgS63iWVhUjNnkcoHiBueiyR6c8NHxpC0PrM9ePazYGFo/LvjlFFtHXzQYHlA5Dldk6oPm2pKOof1oEuWYhPvsIPluyExZpDeg0dSZaJSWNXJlaHgnO0lp2O2rUjY+A6FVxVh1VbKZlj0R4ZP5WHKkkGZjibdz810iR8VK+COsGXRwLhc+BmGVf10ik/Z2k2qISHKBUZ root@cdh3 再将 authorized_keys 分发到 cdh2 和 cdh3 [root@cdh1 .ssh]# scp authorized_keys root@cdh2:/root/.ssh/ [root@cdh1 .ssh]# scp authorized_keys root@cdh3:/root/.ssh/ 测试连接 [root@cdh1 .ssh]# ssh cdh2 Last failed login: Wed Mar 1 08:29:01 EST 2023 from cdh1 on ssh:notty There were 4 failed login attempts since the last successful login. Last login: Wed Mar 1 08:10:27 2023 from 192.168.100.1 [root@cdh2 ~]# 到此, 免密登录就完成啦! 😁 ","date":"2023-03-01","objectID":"/cdh/:3:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 NTP 服务 (所有节点) 使用 NTP 的目的是对网络内所有具有时钟的设备进行时钟同步, 使网络内所有设备的时钟保持一致, 从而使设备能够提供基于统一时间的多种应用. 关闭 chronyd 服务 (所有节点) 因为 CentOS 默认的使用的是 chronyd 服务进行时间同步的, 所以先关掉 [root@cdh1 ~]# systemctl stop chronyd [root@cdh1 ~]# systemctl disable chronyd 修改时区 (改为中国标准时区) (所有节点) [root@cdh1 ~]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 安装 NTP (所有节点) [root@cdh1 ~]# yum -y install ntp 修改 NTP 服务器配置 (cdh1) [root@cdh1 ~]# vim /etc/ntp.conf ... # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). # server 0.centos.pool.ntp.org iburst # server 1.centos.pool.ntp.org iburst # server 2.centos.pool.ntp.org iburst # server 3.centos.pool.ntp.org iburst server 0.cn.pool.ntp.org iburst server 1.cn.pool.ntp.org iburst server 2.cn.pool.ntp.org iburst server 3.cn.pool.ntp.org iburst ... 修改同步配置 (cdh1) [root@cdh1 ~]# vim /etc/sysconfig/ntpd # Command line options for ntpd OPTIONS=\"-g\" SYNC_HWCLOCK=yes 重启 NTP 服务 (cdh1) [root@cdh1 ~]# systemctl restart ntpd 配置其余服务器 (cdh2, cdh3) 修改服务器配置 [root@cdh2 ~]# vim /etc/ntp.conf ... # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server cdh1 iburst ... 手动同步时间 [root@cdh3 ~]# systemctl stop ntpd [root@cdh3 ~]# date 2023年 03月 01日 星期三 22:22:46 CST [root@cdh3 ~]# ntpdate cdh1 3 Mar 10:53:47 ntpdate[10943]: step time server 192.168.100.101 offset 131446.778897 sec [root@cdh3 ~]# systemctl start ntpd [root@cdh3 ~]# date 配置开机启动(所有节点) [root@cdh3 ~]# systemctl enable ntpd ","date":"2023-03-01","objectID":"/cdh/:3:3","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 httpd (所有节点) [root@cdh1 ~]# yum -y install httpd [root@cdh1 ~]# systemctl start httpd [root@cdh1 ~]# systemctl enable httpd ","date":"2023-03-01","objectID":"/cdh/:3:4","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 xsync (所有节点) xsync 是对 rsync 的二次封装, 所以先安装 rsync [root@cdh1 ~]# yum install -y rsync 在用户主目录的bin目录下添加脚本: [root@cdh1 ~]# mkdir bin [root@cdh1 ~]# vim ./bin/xsync 脚本内容: #!/bin/sh #1. 判断参数个数 if [ $# -lt 1 ] then echo Not Enought Arguement! exit; fi #2. 遍历集群所有机器 for host in cdh1 cdh2 cdh3 do echo ============== $host ============== #3. 遍历所有目录, 挨个发送 for file in $@ do #4. 判断文件是否存在 if [ -e $file ] then #5. 获取父目录 pdir=$(cd -P $(dirname $file); pwd) #6. 获取当前文件的名称 fname=$(basename $file) ssh $host \"mkdir -p $pdir\" rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi done done 赋权限: [root@cdh1 ~]# chmod 755 ./bin/xsync 测试一下: [root@cdh1 ~]# xsync ./bin ============== cdh1 ============== sending incremental file list sent 60 bytes received 12 bytes 48.00 bytes/sec total size is 824 speedup is 11.44 ============== cdh2 ============== sending incremental file list ./bin ./bin/xsync sent 934 bytes received 38 bytes 1,944.00 bytes/sec total size is 824 speedup is 0.85 ============== cdh3 ============== sending incremental file list ./bin/ ./bin/xsync sent 934 bytes received 38 bytes 1,944.00 bytes/sec total size is 824 speedup is 0.85 然后到其他机器上看一下文件是不是真的分发下去了. ","date":"2023-03-01","objectID":"/cdh/:3:5","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 xcall (所有节点) xcall的作用就是所有节点都执行命令的时候, 只需要在一台服务器上执行就可以了. #!/bin/bash # 获取控制台指令 cmd=$* # 判断指令是否为空 if [ ! -n \"$cmd\" ] then echo \"command can not be null !\" exit fi # 获取当前登录用户 user=`whoami` # 在从机执行指令,这里需要根据你具体的集群情况配置，host与具体主机名一致 for host in cdh1 cdh2 cdh3 do echo \"================current host is $host=================\" echo \"--\u003e excute command \\\"$cmd\\\"\" ssh $user@$host $cmd done echo \"excute successfully !\" 测试一下: [root@cdh1 ~]# xcall ip addr /root/bin/xcall:行8: ((: #ip addr -eq # : 语法错误: 期待操作数 （错误符号是 \"#ip addr -eq # \"） ================current host is cdh1================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:7a:6b:88 brd ff:ff:ff:ff:ff:ff inet 192.168.100.101/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe7a:6b88/64 scope link valid_lft forever preferred_lft forever ================current host is cdh2================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:ee:a3:5c brd ff:ff:ff:ff:ff:ff inet 192.168.100.102/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feee:a35c/64 scope link valid_lft forever preferred_lft forever ================current host is cdh3================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:fe:d4:de brd ff:ff:ff:ff:ff:ff inet 192.168.100.103/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fefe:d4de/64 scope link valid_lft forever preferred_lft forever excute successfully ! ","date":"2023-03-01","objectID":"/cdh/:3:6","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 JDK (所有节点) 因为我这里是新的 minimal 的虚拟机, 所以肯定没有安装过 jdk, 这里就直接安装了. 有一点需要说明一下, 一定要安装 cdh 对应版本的 jdk!!! [root@cdh1 ~]# rpm -ivh /opt/oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm 这一步这三台一起安装, 安装包就可以用 xsync 同步啦. 配置环境变量 [root@cdh1 ~]# vim /etc/profile.d/java.sh export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera export PATH=$JAVA_HOME/bin:$PATH 然后分发一下, 后面需要分发的地方就不写命令了 [root@cdh1 ~]# xsync /etc/profile.d/java.sh 所有服务器都刷新一下环境变量 [root@cdh1 ~]# source /etc/profile [root@cdh1 ~]# java -version java version \"1.8.0_181\" Java(TM) SE Runtime Environment (build 1.8.0_181-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) ","date":"2023-03-01","objectID":"/cdh/:3:7","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 MySQL (cdh1) 创建目录 [root@cdh1 ~]# mkdir /usr/share/java 上传 mysql-connector-java.jar [root@cdh1 ~]# ll /usr/share/java/ 总用量 984 -rw-r--r--. 1 root root 1007502 3月 3 11:34 mysql-connector-java.jar 解压安装包 [root@cdh1 mysql]# cd /opt/mysql/ [root@cdh1 mysql]# ll 总用量 1036896 -rw-r--r--. 1 root root 530882560 3月 3 11:52 mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar [root@cdh1 mysql]# tar -xf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar [root@cdh1 mysql]# ll 总用量 1036896 -rw-r--r--. 1 root root 530882560 3月 3 11:52 mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar -rw-r--r--. 1 7155 31415 25381952 4月 15 2019 mysql-community-client-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 280904 4月 15 2019 mysql-community-common-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 3838100 4月 15 2019 mysql-community-devel-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 47076368 4月 15 2019 mysql-community-embedded-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 24086952 4月 15 2019 mysql-community-embedded-compat-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 130023844 4月 15 2019 mysql-community-embedded-devel-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 2274268 4月 15 2019 mysql-community-libs-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 2118444 4月 15 2019 mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 173541272 4月 15 2019 mysql-community-server-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 122249684 4月 15 2019 mysql-community-test-5.7.26-1.el7.x86_64.rpm 安装 因为我安装的时候, 遇到了 mariadb 的依赖冲突, 所以先解决一下这个问题: [root@cdh1 mysql]# rpm -qa | grep mariadb mariadb-libs-5.5.56-2.el7.x86_64 [root@cdh1 mysql]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 然后继续安装 MySQL [root@cdh1 mysql]# rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-devel-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpm 启动 MySQL [root@cdh1 mysql]# mysqld --initialize --user=root [root@cdh1 mysql]# cat /var/log/mysqld.log 2023-03-03T04:18:29.973382Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2023-03-03T04:18:30.126918Z 0 [Warning] InnoDB: New log files created, LSN=45790 2023-03-03T04:18:30.149953Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2023-03-03T04:18:30.218530Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 740806b3-b97a-11ed-96b3-000c297a6b88. 2023-03-03T04:18:30.220950Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2023-03-03T04:18:30.221618Z 1 [Note] A temporary password is generated for root@localhost: 7uTGbVWuja,. [root@cdh1 mysql]# systemctl enable mysqld [root@cdh1 mysql]# systemctl start mysqld 更改 MySQL 密码, 给 root 赋权限, 这几步就不写了, 网上有很多. 重新登录 root, 创建数据表: create database cmserver default charset utf8 collate utf8_general_ci; grant all on cmserver.* to 'cmserveruser'@'%' identified by 'root'; create database metastore default charset utf8 collate utf8_general_ci; grant all on metastore.* to 'hiveuser'@'%' identified by 'root'; create database amon default charset utf8 collate utf8_general_ci; grant all on amon.* to 'amonuser'@'%' identified by 'root'; create database rman default charset utf8 collate utf8_general_ci; grant all on rman.* to 'rmanuser'@'%' identified by 'root'; ","date":"2023-03-01","objectID":"/cdh/:3:8","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 CM 组件 (所有节点) 上传安装包到各服务器 [root@cdh1 opt]# ll 总用量 1169544 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 9984 3月 3 12:47 cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm [root@cdh2 opt]# ll 总用量 1169532 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm [root@cdh3 opt]# ll 总用量 1169532 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm 安装 (cdh1 安装 3 个, cdh2 和 cdh3 安装 2 个) yum localinstall -y cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm yum localinstall -y cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm yum localinstall -y cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm ","date":"2023-03-01","objectID":"/cdh/:3:9","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"CDH6.2.0 (cdh1) 上传安装包 [root@cdh1 parcel-repo]# cd /opt/cloudera/parcel-repo/ [root@cdh1 parcel-repo]# ll 总用量 2431572 -rw-r--r--. 1 root root 2087665645 3月 3 13:02 CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel -rw-r--r--. 1 root root 41 3月 3 13:00 CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha -rw-r--r--. 1 root root 33725 3月 3 13:00 manifest.json -rw-r--r--. 1 root root 402216960 3月 3 13:00 PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel -rw-r--r--. 1 root root 41 3月 3 12:59 PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel.sha [root@cdh1 parcel-repo]# cd /opt/cloudera/csd/ [root@cdh1 csd]# ll 总用量 8 -rw-r--r--. 1 root root 5306 3月 3 13:03 PHOENIX-1.0.jar 修改配置 cdh1 [root@cdh1 ~ ]# vim /etc/cloudera-scm-server/db.properties # Auto-generated by scm_prepare_database.sh on 2020年 10月 16日 星期五 14:26:26 CST # # For information describing how to configure the Cloudera Manager Server # to connect to databases, see the \"Cloudera Manager Installation Guide.\" # com.cloudera.cmf.db.type=mysql com.cloudera.cmf.db.host=localhost com.cloudera.cmf.db.name=cmserver com.cloudera.cmf.db.user=root com.cloudera.cmf.db.setupType=EXTERNAL com.cloudera.cmf.db.password=root cdh1, cdh2, cdh3 [root@cdh1 ~]# vi /etc/cloudera-scm-agent/config.ini ... [General] # Hostname of the CM server. server_host=cdh1 ... [root@cdh1 ~]# xsync /etc/cloudera-scm-agent/config.ini ","date":"2023-03-01","objectID":"/cdh/:3:10","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"修改Linux swappiness参数 (所有节点） [root@cdh1 ~]# vim /etc/sysctl.conf # sysctl settings are defined through files in # /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/. # # Vendors settings live in /usr/lib/sysctl.d/. # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.d/ and put new settings there. # # For more information, see sysctl.conf(5) and sysctl.d(5). vm.swappiness=0 [root@cdh1 ~]# xsync /etc/sysctl.conf 所有节点执行 echo 0 \u003e /proc/sys/vm/swappiness ","date":"2023-03-01","objectID":"/cdh/:3:11","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"禁用透明页 (所有节点) [root@cdh1 ~]# vim /etc/rc.local #!/bin/bash # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES # # It is highly advisable to create own systemd services or udev rules # to run scripts during boot instead of using this file. # # In contrast to previous versions due to parallel execution during boot # this script will NOT be run after all other services. # # Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure # that this script will be executed during boot. touch /var/lock/subsys/local echo never \u003e /sys/kernel/mm/transparent_hugepage/defrag echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled [root@cdh1 ~]# xsync /etc/rc.local 所有节点执行 [root@cdh1 ~]# chmod +x /etc/rc.d/rc.local [root@cdh1 ~]# source /etc/rc.local ","date":"2023-03-01","objectID":"/cdh/:3:12","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"启动 CM 和 CDH 集群 启动 server (cdh1) [root@cdh1 ~]# systemctl start cloudera-scm-server [root@cdh1 ~]# systemctl status cloudera-scm-server ● cloudera-scm-server.service - Cloudera CM Server Service Loaded: loaded (/usr/lib/systemd/system/cloudera-scm-server.service; enabled; vendor preset: disabled) Active: active (running) since 五 2023-03-03 13:11:11 CST; 7s ago Main PID: 13692 (java) CGroup: /system.slice/cloudera-scm-server.service └─13692 /usr/java/jdk1.8.0_181-cloudera/bin/java -cp .:/usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:lib/* -server -Dlog4j.configuration=fil... 启动 agent (所有节点) [root@cdh2 opt]# systemctl start cloudera-scm-agent [root@cdh2 opt]# systemctl status cloudera-scm-agent ● cloudera-scm-agent.service - Cloudera Manager Agent Service Loaded: loaded (/usr/lib/systemd/system/cloudera-scm-agent.service; enabled; vendor preset: disabled) Active: active (running) since 五 2023-03-03 13:13:03 CST; 10s ago Main PID: 11790 (cmagent) CGroup: /system.slice/cloudera-scm-agent.service └─11790 /usr/bin/python2 /opt/cloudera/cm-agent/bin/cm agent ","date":"2023-03-01","objectID":"/cdh/:3:13","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"登录 Web 界面安装 登录 打开地址: http://cdh1:7180/cmf/login 登录名/密码: admin/admin 登录界面 勾选同意 勾选同意 选择免费版 选择免费版 进入安装页面啦! 安装页面 输入集群名 输入集群名 添加集群 添加集群 选择存储库 (默认就可以) 选择存储库 开始安装 安装 安装完成","date":"2023-03-01","objectID":"/cdh/:3:14","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":" 该文档针对 Seatunnel v2.3.0版本. ","date":"2023-02-28","objectID":"/seatunnel_engine/:0:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"1. 下载 Seatunnel Engine是 Seatunnel 的默认引擎. Seatunnel 的安装包已经包含了所有Seatunnel Engine的内容. ","date":"2023-02-28","objectID":"/seatunnel_engine/:1:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"2. 配置SEATUNNEL_HOME 添加文件: /etc/profile.d/seatunnel.sh 添加内容: export SEATUNNEL_HOME=${seatunnel install path} export PATH=$PATH:$SEATUNNEL_HOME/bin ","date":"2023-02-28","objectID":"/seatunnel_engine/:2:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"3. 配置Seatunnel Engine的 JVM 参数 Seatunnel Engine支持两种方式设置 jvm 参数 在$SEATUNNEL_HOME/bin/seatunnel-cluster.sh中添加. 修改该文件: 在第一行添加内容JAVA_OPTS=\"-Xms2G -Xmx2G\" 在启动Seatunnel Engine的时候添加 jvm 参数. 🌰: seatunnel-cluster.sh -DJvmOption=\"-Xms2G -Xmx2G\" ","date":"2023-02-28","objectID":"/seatunnel_engine/:3:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4. 配置Seatunnel Engine Seatunnel Engine提供了很多需要在seatunnel.yaml(在$SEATUNNEL_HOME/config/下)中设置的参数. ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.1 Backup count (备份数) Seatunnel Engine实现了基于Hazelcast IMDG的集群管理. 集群的状态数据(任务运行状态, 资源状态)存放在Hazelcast IMap中. 这些状态数据在 Hazelcast IMap 中是分布式的并且是存储在所有集群节点中的. Hazelcast 会将数据分区存储在 Imap 中. 每个分区可以指定备份的数量. 因此, Seatunnel Engine可以在不使用其他服务(例如 zookeeper)的情况下实现集群. backup count参数定义了同步备份的数量. 举个🌰, 如果设置为1, 分区的备份将会存放在另一个分区上; 设置为2, 将会存放在另外两个分区上. backup count建议为min(1, max(5, N/2)), 其中N为集群节点数 seatunnel: engine: backup-count: 1 # other config ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.2 Slot service (插槽服务) 插槽的数量决定了集群节点可以并行运行的任务组的数量. Seatunnel Engine是一个数据同步引擎, 并且大多数的任务都是IO密集型的. 建议使用动态插槽: seatunnel: engine: slot-service: dynamic-slot: true # other config ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.3 Checkpoint Manager (检查点管理) 像是 FLink, Seatunnel Engine也支持 Chandy–Lamport 算法(分布式快照算法). Seatunnel Engine可以在没有数据丢失和重复的前提下实现数据同步. interval (间隔) 两个检查点之间的间隔单位是毫秒. 如果checkpoint.interval参数在任务的配置文件的env块中设置了, 这个值将会被覆盖掉. timeout (超时) 检查点的超时. 如果一个检查点没有在超时范围内完成的话, 那么这个检查点就触发失败. 因此, 任务就会被恢复. max-concurrent (最大并发) 最多可以同时执行多少个检查点. tolerable-failure (重试) 检查点失败后最大重试次数. seatunnel: engine: backup-count: 1 print-execution-info-interval: 10 slot-service: dynamic-slot: true checkpoint: interval: 300000 timeout: 10000 max-concurrent: 1 tolerable-failure: 2 checkpoint storage (检查点存储) todo ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:3","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5. 配置Seatunnel Engine服务 所有Seatunnel Engine服务配置都在hazelcast.yaml文件中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5.1 cluster-name (集群名称) Seatunnel Engine节点通过集群名来判断对方是否和自身属于同一集群. 如果两个节点之间的集群名是不同的, 那么Seatunnel Engine会拒绝服务请求. ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5.2 Network (网络) 基于Hazelcast,Seatunnel Engine集群是运行Seatunnel Engine服务的集群成员网络. 集群成员自动连接在一起组成一个集群. 这种自动加入是通过集群成员用来相互查找的各种发现机制进行的. 需要注意一点, 一旦集群建立, 无论是通过什么样的发现机制, 各集群成员之间的通信总是通过 TCP/IP 完成的. Seatunnel Engine使用以下发现机制: TCP 可以将Seatunnel Engine配置为一个完整的 TCP/IP 集群. 🌰 hazelcast: cluster-name: seatunnel network: join: tcp-ip: enabled: true member-list: - hostname1 port: auto-increment: false port: 5801 properties: hazelcast.logging.type: log4j2 TCP是在单独的SeaTunnel引擎集群中的建议方式. 另一方面, Hazelcast 提供了一些其他服务发现方法. 点此查看详情: Hazelcast Network ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6. 配置SeaTunnel Engine客户端 所有SeaTunnel Engine客户端配置都在hazelcast-client.yaml文件中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6.1 cluster-name (集群名称) 客户端必须和Seatunnel Engine的cluster-name一致. 否则, Seatunnel Engine将会拒绝客户端的请求. ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6.2 Network (网络) cluster-members (集群成员) 所有Seatunnel Engine服务节点地址需要添加在这: hazelcast-client: cluster-name: seatunnel properties: hazelcast.logging.type: log4j2 network: cluster-members: - hostname1:5801 ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"7. 启动SeaTunnel Engine服务节点 mkdir -p $SEATUNNEL_HOME/logs nohup seatunnel-cluster.sh \u0026 日志会写在$SEATUNNEL_HOME/logs/seatunnel-server.log中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:7:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"8. 安装Seatunnel Engine客户端 只需要将Seatunnel Engine节点上的$Seatunnel Engine目录复制到客户端节点, 并像Seatunnel Engine服务节点一样配置$Seatunnel Engine就可以了. ","date":"2023-02-28","objectID":"/seatunnel_engine/:8:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data","Hadoop"],"content":"介绍 HDFS (Hadoop Distributed File System) 是 Hadoop下的分布式文件系统, 具有高容错、高吞吐量等特性, 可以部署在低成本的硬件上. ","date":"2023-02-27","objectID":"/hdfs/:1:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"设计原理 ","date":"2023-02-27","objectID":"/hdfs/:2:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"架构 HDFS 遵从主/从架构, 由单个 NameNode(NN) 和 多个 DataNote(DN) 组成: NameNode: 负责执行有关文件系统命名空间的操作, 🌰: 打开, 关闭、重命名文件和目录等. 同事还负责集群元数据的存储, 记录着文件中各个数据块的位置信息. 也就是说主要是用来存储元数据的. DataNode: 负责提供来自文件系统客户端的读写请求, 执行块的创建, 删除等操作. 用来存储具体文件的. ","date":"2023-02-27","objectID":"/hdfs/:2:1","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"文件系统命名空间 HDFS 的文件系统命名空间的层次结构与大多数文件系统类似(🌰: Linux), 支持目录和文件的创建、移动、删除和重命名等操作, 支持配置用户和访问权限, 但不支持硬链接和软连接. NameNode负责维护文件系统名称空间, 记录对名称空间或其属性的任何更改. ","date":"2023-02-27","objectID":"/hdfs/:2:2","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"数据复制 由于 Hadoop 是被设计运行在廉价的机器上的, 这就一位置硬件其实是不可靠的. 所以为了保证高容错, HDFS 提供了数据复制机制. HDFS 将每个文件存储为一系列块, 每个块由多个副本来保证容错, 块的大小和复制因子可配置 (默认下, 块为128M, 复制因子为3). 数据复制的实现原理 大型的 HDFS 实例通常分布在多个机架的多台服务器上, 不同机架上的两台服务器之间通过交换机进行通讯. 大多数情况下, 同一机架中的服务期间的网络带宽大于不同机架中的服务器之间的带宽. 所以, HDFS 采用机架感知副本放置策略. 🌰, 对于默认情况, 当复制因子为3时, HDFS 的放置策略是: 在写入程序位于 DN 上时, 就优先将写入文件的一个副本放置在该 DN 上, 否则放在随机 DN 上. 之后另一个远程机架上的任意一个节点上放置另一个副本, 并在该机加上的另一个节点放置最后一个副本. 这么做的好处就是可以减少机架间的写入流量, 从而提高写入性能. 如果复制因子大于3, 则随机确定第4个和之后副本的放置位置, 同时需要保持每个机架的副本数量低于上限, 上限值通常为 (复制系数 - 1)/机架数 + 2. 注意: 不允许同一个 DN 上具有同一块的相同副本. 每个机架最多存储两份备份. 但是在这两个条件无法被满足的一些情况下, 这些条件会被忽略. 并且 HDFS 允许自定义布局算法. 副本的选择 副本的选择为了最大限度地减少带宽消耗和读取延迟, HDFS 在执行读取请求时, 优先读取距离读取其最近的副本(物理层面上). 也就是说, 如果与读取器节点相同的机架上存在副本, 则优先选择该副本. 如果 HDFS 集群跨越多个数据中心, 优先选择本地数据中心上的副本. ","date":"2023-02-27","objectID":"/hdfs/:2:3","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"架构的稳定性 心跳机制和重新复制 每个 DN 定期向 NN 发送心跳, 如果超过指定时间没有收到心跳, 会将该 DN 标记为已死亡. NN 不会将任何新的 IO请求 转发给标记为死亡的 DN, 也不会再使用已死亡的 DN 上的数据. DN 也会将其所有数据块列表定时发送给 NN, 并且在发送之前 DN 会检测校验和是否正常, 若不正常, 则不会发送给 NN. 所以 NN 会检测出那些数据块已经损坏. 由于数据不再可用, 可能会导致某些块副本数小于复制因子, 所以 NN 会跟踪这些块, 并在必要的时候进行重新复制. 数据的完整性 由于存储设备故障等原因, 存储在 DN 上的数据块也会发生损坏. 为了避免读取到一损坏的数据而导致错误, HDFS 提供了数据完整性机制来保存数据的完整性: 当客户端创建 HDFS 文件时, 它会计算文件的每个块的校验和, 并将校验和存储在同一 HDFS 命名空间下的单独的隐藏文件中. 当客户端检索文件内容时, 它会校验从每个 DN 接收的数据是否与存储在关联校验和文件中的校验和匹配. 如果不匹配, 代表数据已经损坏, 这个时候客户端会选择其他 DN 获取副本, 并重新校验. 元数据的磁盘故障 FsImage 和 EditLog 是 HDFS 的核心数据, 这些数据的以为丢失可能导致整个 HDFS 服务不可用. 为了避免, 可以配置 NN 使其支持 FsImage 和 EditLog 多副本同步, 这样 FsImage 或 EditLog 的任何改变都会引起每个副本的同步更新. 支持快照 快照支持在特定时刻存储数据副本, 在数据意外损坏时, 可以通过回滚操作恢复到简况的数据状态. ","date":"2023-02-27","objectID":"/hdfs/:2:4","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"特点 高容错 上面已经介绍了 HDFS 采用数据的多副本方案, 所以部分硬件的损坏并不会导致全部数据的丢失. 高吞吐量 HDFS 设计的重点是支持高吞吐量的数据访问, 而不是低延迟的数据访问. 大文件支持 文件大小应是 GB 到 TB 级别的. 简单一致性模型 HDFS 更适合一次写入多次读取 (write-once-read-many) 的访问模型. 支持将内容追加到文件末尾, 但不支持数据的随机访问, 不能从文件任意位置新增数据. 跨平台移植性 HDFS 具有良好的跨平台移植性, 使得其他大数据计算框架都将其作为数据持久化存储的首选方案. ","date":"2023-02-27","objectID":"/hdfs/:3:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["Kafka"],"content":"生产者消息发送流程 ","date":"2022-09-02","objectID":"/kafka_3/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"发送原理 在消息发送的过程中, 涉及到了两个线程–main线程和Sender线程. 在 main 线程中创建了一个双端队列RecordAccumulator. main 线程将消息发送个 RecordAccumulator, Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker. 发送流程 ","date":"2022-09-02","objectID":"/kafka_3/:1:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者重要参数 参数名称 描述 bootstrap.servers 生产者连接集群所需的 boker 地址清单. 可以设置一个或多个, 中间用,隔开. 注意: 并非需要所有的 broker 地址, 因为生产者从给定的 broker 里查找其他 broker 信息 key.serializer和value.serializer 指定发送消息的 key 和 value 的序列化类型. 一定要写全类名! buffer.memory RecordAccumulator 缓冲区总大小, 默认32m. batch.size 缓冲区一批数据最大值, 默认16k. (适当增加可以提高吞吐量; 但是如果过大的话, 会导致数据传输延迟增加). linger.ms 如果数据一直未到达 batch.size, sender 等待 linger.ms 之后就会发送数据, 单位是ms. 默认0ms, 表示没有延迟. (生产环境一般 5-100ms 之间) acks 0: 生产者发送过来的数据, 不需要等数据落盘应答; 1: 生产者发送过来的数据, Leader 收到数据后应答; -1(all): 生产者发送过来的数据, Leader+ 和 isr 队列里面的所有节点收齐数据后应答. 默认是-1, -1和all是等价的. max.in.flight.request.per.connection 允许最多没有返回 ack 的次数, 默认为5, 开启幂等性要保证该值是 1-5 的数字. retries 当消息发送出现错误的时候, 系统会重发消息. 该值表示重复次数. 默认为int最大值,2147483647. 如果设置了重试, 还想保证消息的有序性, 需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候, 其他的消息可能发送成功了. retry.backoff.ms 两次重试之间的时间间隔, 默认为100ms. enable.idempotence 是否开启幂等性, 默认为true, 开启幂等性. compression.type 生产者发送的所有数据的压缩方式. 默认为none, 也就是不压缩. 支持压缩类型: none, gzip, snappy, lz4和zstd. ","date":"2022-09-02","objectID":"/kafka_3/:1:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者分区 ","date":"2022-09-02","objectID":"/kafka_3/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"分区的好处 便于合理使用存储资源, 每个 Partition 在一个 Broker 上存储, 可以吧海量的数据按照分区切割成小块存储在多台 Broker 上. 合理控制分区的任务, 可以实现负载均衡的效果. 提高并行度, 生产者可以以分区为单位发送数据; 消费者可以以分区为单位消费数据. ","date":"2022-09-02","objectID":"/kafka_3/:2:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"数据传递语义 至少一次(At Least Once) = ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2 最多一次(At Most Once) = ACK级别设置为0. 精确一次(Exactly Once): 对于一些非常重要的信息, 例如和钱相关的数据, 要求数据既不能重复也不能丢失. 也就是说, At Least Once 可以保证数据不丢失, 但是不能保证数据不重复; At Most Once 可以保证数据不重复, 但是不能保证数据不丢失. ","date":"2022-09-02","objectID":"/kafka_3/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"幂等性 ","date":"2022-09-02","objectID":"/kafka_3/:4:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"幂等性原理 幂等性就是指 Producer 不论向 Broker 发送多少次重复数据, Broker 端都只会持久化一条, 保证了不重复. 所以上面的 Exactly Once = 幂等性 + At Least Once(ack=-1 + partitions\u003e=2 + ISR最小副本数\u003e=2) 重复数据的判断标准: 具有\u003cPID, Partition, SeqNumber\u003e相同逐渐的消息提交时, Borker 只会持久化一条. 其中: PID 是 Kafka 每次重启都会分配一个新的; Partition 表示分区号 Sequence Number 单调自增 综上, 幂等性只能保证的是在单分区单会话内不重复. ","date":"2022-09-02","objectID":"/kafka_3/:4:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"如何使用幂等性 开启参数enable.idempotence. ","date":"2022-09-02","objectID":"/kafka_3/:4:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者事务 开启事务, 必须开启幂等性! Kafka事务原理 ","date":"2022-09-02","objectID":"/kafka_3/:5:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"主题命令行操作 查看操作主题命令参数 $ bin/kafka-topics.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 --create 创建主题 --delete 删除主题 --alter 修改主题 --list 查看所有主题 --describe 查看主题详细描述 --partitions \u003cInteger:# of partitions\u003e 设置分区数 --replication-factor \u003cInteger: replication factor\u003e 设置分区副本 --config \u003cString:name=value\u003e 更新系统默认配置 查看当前服务器中的所有 topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list 创建 first topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first 查看 first 主题详情 $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first 修改分区数 (注意: 分区数只能增加, 不能减少!!!) $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 删除topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first ","date":"2022-09-02","objectID":"/kafka_2/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"生产者命令行操作 查看操作生产者命令参数 $ bin/kafka-console-producer.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 发送消息 $ bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first \u003ehello world \u003ehello buli-home ","date":"2022-09-02","objectID":"/kafka_2/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"消费者命令行操作 查看操作生产者命令参数 $ bin/kafka-console-producer.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 --from-beginning 从头开始消费 --group \u003cString:consumer group id\u003e 指定消费者组名称 消费消息 消费 first 主题中的数据 $ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first 把主题中所有的数据都读取出来 (包括历史数据) $ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first ","date":"2022-09-02","objectID":"/kafka_2/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"定义 Kafka传统定义: Kafka 是一个分布式的基于发布/订阅模式的消息队列(Message Queue) 发布/订阅: 消息的发布者不会讲消息直接发送给特定的订阅者, 而是将发布的消息分为不同的类型, 订阅者只接受感兴趣的消息. Kafka最新定义: Kafka是一个开元的分布式事件流平台(Event Streaming Platform), 被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用. ","date":"2022-07-26","objectID":"/kafka_1/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列 目前比较常见的消息队列产品主要有Kafka、ActiveMQ、RabbitMQ、RocketMQ等. 大数据场景主要采用 Kafka, JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ . ","date":"2022-07-26","objectID":"/kafka_1/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"传统消息队列的应用场景 缓存/消峰: 有助于控制和优化数据流经过系统的速度, 解决生产消息和消费消息的处理速度不一致的情况. 解耦: 允许独立的扩展或修改两边的处理过程, 只要确保它们遵守同样的接口约束. 异步通信: 允许用户把一个消息放入队列, 但并不立即处理它, 然后在需要的时候再去处理它们. ","date":"2022-07-26","objectID":"/kafka_1/:2:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列的两种模式 点对点模式: 消费者主动拉取数据, 消息收到后清除消息 发布/订阅模式: 可以有多个 topic(主题) (浏览、点赞、收藏、评论等) 消费者消费数据之后, 不能删除数据 每个消费者相互独立, 都可以消费到数据 ","date":"2022-07-26","objectID":"/kafka_1/:2:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"Kafka基础框架 Kafka基础框架 Producer: 消息生产者, 向 Kafka broker 发消息的客户端. Consumer: 消息消费者, 向 Kafka broker 取消息的客户端. Consumer Group(CG): 消费者组, 由多个 consumer 组成. 消费者组内每个消费者负责消费不同分区的数据, 一个分区只能由一个组内消费者消费; 消费者组之间互不影响. 所有的消费者都属于某个消费者组, 即消费者组是逻辑上的一个订阅者. Broker: 一台 Kafka 服务器就是一个 broker. 一个集群由多个 broker 组成. 一个 broker 可以容纳多个topic. Topic: 可以理解为一个队列, 生产者和消费者面向的都是一个 topic. Partition: 为了实现扩展性, 一个非常大的 topic 可以分布到多个 broker(即服务器) 上, 一个 topic 可以分为多个 partition, 每个 partition 是一个有序的队列. Replica: 副本. 一个 topic 的每个分区都有若干个副本, 一个 Leader 和若干个 Follower. Leader: 每个分区多个副本的\"主\", 生产者发送数据的对象, 以及消费者消费数据的对象都是Leader. Follower: 每个分区多个副本中的\"从\", 实时从 Leader 中同步数据, 保持和 Leader 数据的同步. Leader 发生故障时, 某个 Follower 会成为新的 Leader. ","date":"2022-07-26","objectID":"/kafka_1/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["canal"],"content":"Canal概述 ","date":"2022-07-26","objectID":"/canal/:1:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的起源 阿里巴巴 B2B 公司, 因为业务的特性, 卖家主要集中在国内, 买家主要集中在国外, 所以衍生出了同步杭州和美国异地机房的需求, 从2010年开始, 阿里系公司开始逐步的尝试基于数据库的日志解析, 获取增量变更进行同步, 由此衍生出了增量 订阅\u0026消费 的业务. Canal 是用 Java 开发的基于数据库增量日志解析, 提供增量数据 订阅\u0026消费 的中间件. 目前, Canal 主要支持了 MySQL 的 Binlog 解析, 解析完成后才利用 Canal Client 来处理获得的相关数据. (数据库同步需要阿里的 Otter 中间件, 基于 Canal ). ","date":"2022-07-26","objectID":"/canal/:1:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的Binlog 什么是Binlog MySQL 的二进制日志可以说是 MySQL 最重要的日志了, 它记录了所有的 DDL 和 DML (除了数据查询语句) 语句, 以事件行驶记录, 还包括语句所执行的消耗的时间. Binlog是事务安全性的. 一般来说, SQL语言分为三类: DML(Data Manipulation Language): 数据操纵语言, 最常用的增删改查就是这类, 操作对象是数据表中的记录 DDL(Data Definition Language): 数据定义语言, 例如建库、建表等 DCL(Data Control Language): 数据控制语言, 如 Grant、Rollback 等, 常见于数据库安全管理 Binlog两个最重要的使用场景: MySQL Replication 在 Master 端开启 Binlog, Master 把它的 Binlog 传递给 Slaves 来达到 Master-Slave 数据一致的目的 数据恢复, 通过使用 MySQL Binlog 工具来使恢复数据 Binlog分类 MySQL Binlog 的格式有三种, 分别是 STATEMENT,MIXED,ROW . 在配置文件中可以选配: binlog_format=statement|mixed|row statement: 语句级, binlog 会记录每次执行写操作的语句. 相对row模式节省空间, 但是可能产生不一致性. 🌰: update table set create_date=now(), 如果用这种模式进行恢复, 由于执行时间的不同产生的数据就可能不同. 优势: 节省空间 劣势: 可能造成数据不一致 row: 行级, binlog 会记录每次操作后每行记录的变化. 优势: 保持数据的绝对一致性 劣势: 占用空间大 mixed: statement的升级版, 一定程度上解决了因为一些情况而造成的 statement模式 不一致的问题. mixed默认还是statement, 在某些情况下(🌰): 当函数中包含UUID()时; 包含AUTO_INCREMENT字段的表被更新等. 这些情况下会按照row的方式处理. 优势: 节省空间, 同事兼顾了一定的一致性 劣势: 还有一些极个别情况依旧会造成不一致, 另外 statement 和 mixed 对于需要对 binlog 的监控情况都不方便 综上, Canl想做监控分析, 选择 row 格式比较合适 ","date":"2022-07-26","objectID":"/canal/:1:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的工作原理 MySQL主从复制过程 Master 主库将改变记录, 写到 binlog 中; Slave 从库向 MySQL Slave 发送 dump 协议, 将 Master 主库的 binlog events 拷贝到它的中继日志(relay log); Slave 从库读取并重做中继日志中的事件, 将改变的数据同步到自己的数据库. MySQL主从复制过程 Canal的工作原理 理解了上面的过程, Canal的原理就很简单, 就是把自己伪装成Slave, 假装从 Master 复制数据 ","date":"2022-07-26","objectID":"/canal/:1:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的准备 MySQL的安装这里就不说了, 网上有很多. 首先, 建数据库: canal_test 然后建表: create table user_info { `id` int, `name` varchar(255), `gender` varchar(255) } 修改配置文件开启binlog [root@hadoop102 ~]# vi /etc/my.cnf # 打开binlog log-bin=mysql-bin # 选择ROW模式 binlog-format=row # 配置MYSQL replaction需要定义, 不要和canal的slaveId重复 server_id=1 binlog-do-db=canal_test binlog-do-db根据实际情况配置, 如果不配置, 则表示所有数据库均开启 binlog. 重启 MySQL sudo systemctl restart mysqld ","date":"2022-07-26","objectID":"/canal/:2:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的下载和安装 ","date":"2022-07-26","objectID":"/canal/:3:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"下载 下载地址: https://github.com/alibaba/canal/releases 下载对应版本: canal.deployer-xxx.tar.gz 解压到对应位置 ","date":"2022-07-26","objectID":"/canal/:3:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 canal.properties [root@hadoop102 conf]# pwd /opt/module/canal/conf [root@hadoop102 conf]# vi canal.properties 这个文件是 canal 的基本通用配置, canal端口号默认是11111. 多实例配置: 一个 canal 服务中可以有多个instance, conf/ 下每一个 example 即是一个实例, 每个实例下面都有独立的配置文件. 需修改canal.destinations=实例1,实例2,实例3 ","date":"2022-07-26","objectID":"/canal/:3:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 instance.properties [root@hadoop102 example]# pwd /opt/module/canal/conf/example [root@hadoop102 example]# vi instance.properties ## mysql serverId , v1.0.26+ will autoGen ## v1.0.26版本后会自动生成slaveId，所以可以不用配置 # canal.instance.mysql.slaveId=0 # 数据库地址 canal.instance.master.address=127.0.0.1:3306 # binlog日志名称 canal.instance.master.journal.name=mysql-bin.000001 # mysql主库链接时起始的binlog偏移量 canal.instance.master.position=154 # mysql主库链接时起始的binlog的时间戳 canal.instance.master.timestamp= canal.instance.master.gtid= # username/password # 在MySQL服务器授权的账号密码 canal.instance.dbUsername=canal canal.instance.dbPassword=canal # 字符集 canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false # table regex .*\\\\..*表示监听所有表 也可以写具体的表名，用，隔开 canal.instance.filter.regex=.*\\\\..* # mysql 数据解析表的黑名单，多个表用，隔开 canal.instance.filter.black.regex= # 解析表字段的黑名单, 多个字段用/隔开, 多个表用,隔开(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) canal.instance.filter.black.field=data_center.canal_test_2:column_2,data_center.canal_test_1:column_1,data_center.canal_test_3:column_2/column_3 ","date":"2022-07-26","objectID":"/canal/:3:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"实时监控测试 ","date":"2022-07-26","objectID":"/canal/:4:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"TCP模式测试 创建一个 maven 项目, 在pom.xml中配置: \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba.otter\u003c/groupId\u003e \u003cartifactId\u003ecanal.client\u003c/artifactId\u003e \u003cversion\u003e1.1.2\u003c/version\u003e \u003c/dependency\u003e 创建类: CanalClient package org.mustard.app; import com.alibaba.fastjson.JSONObject; import com.alibaba.otter.canal.client.CanalConnector; import com.alibaba.otter.canal.client.CanalConnectors; import com.alibaba.otter.canal.protocol.CanalEntry; import com.alibaba.otter.canal.protocol.Message; import com.google.protobuf.ByteString; import com.google.protobuf.InvalidProtocolBufferException; import java.net.InetSocketAddress; import java.util.List; public class CanalClient { public static void main(String[] args) throws InvalidProtocolBufferException { // 获取连接对象 CanalConnector canalConnector = CanalConnectors.newSingleConnector(new InetSocketAddress(\"hadoop102\", 11111), \"example\", \"\", \"\"); // 获取连接 canalConnector.connect(); // 指定要监控的数据库 canalConnector.subscribe(\"canal.*\"); long idx = 0; while (true) { // 获取message Message msg = canalConnector.get(100); List\u003cCanalEntry.Entry\u003e entries = msg.getEntries(); if (entries.size() \u003c= 0) { System.out.println((++idx) + \". 没有数据, 等一会儿\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } else { for (CanalEntry.Entry entry : entries) { // 获取表名 String tableName = entry.getHeader().getTableName(); // Entry类型 CanalEntry.EntryType entryType = entry.getEntryType(); if (CanalEntry.EntryType.ROWDATA.equals(entryType)) { // 序列化数据 ByteString storeValue = entry.getStoreValue(); // 反序列化 CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(storeValue); // 获取事件类型 CanalEntry.EventType eventType = rowChange.getEventType(); // 获取具体数据 List\u003cCanalEntry.RowData\u003e rowDatasList = rowChange.getRowDatasList(); // 遍历打印 for (CanalEntry.RowData rowData : rowDatasList) { List\u003cCanalEntry.Column\u003e beforeColumnsList = rowData.getBeforeColumnsList(); JSONObject beforeData = new JSONObject(); for (CanalEntry.Column column : beforeColumnsList) { beforeData.put(column.getName(), column.getValue()); } List\u003cCanalEntry.Column\u003e afterColumnsList = rowData.getAfterColumnsList(); JSONObject afterData = new JSONObject(); for (CanalEntry.Column column : afterColumnsList) { afterData.put(column.getName(), column.getValue()); } System.out.println(\"TableName: \" + tableName + \", EventType: \" + eventType + \", Before: \" + beforeData + \", After: \" + afterData); } } } } } } } ","date":"2022-07-26","objectID":"/canal/:4:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Kafka模式测试 修改 canal.properties 中 canal 的输出 model ######### common argument ############# canal.id = 1 canal.ip = canal.port = 11111 canal.metrics.pull.po rt = 11112 canal.zkServers = # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = false # tcp, kafka, RocketMQ canal.serverMode = kafka # flush meta cursor/parse position to file 修改 kafka 集群的地址 ######### Kafka ############# kafka.bootstrap.servers = hadoop102:9092 修改 instance.properties 输出到 Kafka 的主题以及分区数 # mq config canal.mq.topic=canal_test 默认还是输出到指定 Kafka 主题的一个分区, 因为多个分区并行可能会打乱 binlog 的顺序, 如果要提高并行度, 首先设置 kafka 的分区数 \u003e 1, 然后设置canal.mq.partitionHash属性. 启动 canal $ ./bin/startup.sh 然后测试: [root@hadoop102 kafka_2.12-2.8.1]# ./bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic canal_test {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589672000,\"id\":2,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589672852,\"type\":\"DELETE\"} {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589697000,\"id\":3,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589697570,\"type\":\"INSERT\"} ","date":"2022-07-26","objectID":"/canal/:4:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["scala"],"content":"分支控制 ","date":"2022-07-15","objectID":"/scala_4/:1:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"if-else 单分支: if{} 双分支: if{} else{} 多分支: if{} else if{} Scala 中的if-else是有返回值的, 具体取决于满足条件的代码块的最后一行内容 Scala 中是没有三元运算符的, 但是可以用if-else代替 ","date":"2022-07-15","objectID":"/scala_4/:1:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"switch Scala 中是没有switch的, 而是使用模式匹配来处理的 ","date":"2022-07-15","objectID":"/scala_4/:1:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"循环控制 ","date":"2022-07-15","objectID":"/scala_4/:2:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"for Scala 中为for提供了很多的特性: 1. 范围数据循环 (to) // 前后闭合: [1,3] for (i \u003c- 1 to 3) { println(i) } 2. 范围数据循环 (until) // 前闭后开: [1, 3) for (i \u003c- 1 until 3) { println(i) } 3. 循环守卫 for (i \u003c- 1 to 3 if i != 2) { println(i) } // 上面的代码就相当于: for (i \u003c- 1 to 3) { if (i != 2) { println(i) } } 4. 循环步长 for (i \u003c- 1 to 10 by 2) { println(i) } 5. 嵌套循环 // 因为没有关键字, 所以一定要加 `;` 来进行分割 for (i \u003c- 1 to 3; j \u003c- 1 to 3) { println(\"i=\" + i + \", j=\" + j) } // 上面的代码相当于: for (i \u003c- 1 to 3) { for (j \u003c- 1 to 3) { println(\"i=\" + i + \", j=\" + j) } } 6. 引入变量 for (i \u003c- 1 to 3; j = 4 - i) { println(\"i=\" + i + \", j=\" + j) } // for 推导式有一个不成文的规定: // 1. 仅包含单一表达式时, 使用圆括号 // 2. 当包含多个表达式时, 一般每一行一个表达式, 并且用花括号 for { i \u003c- 1 to 3 j = 4 - i } { println(\"i=\" + i + \", j=\" + j) } 7. 循环返回值 val res = for (i \u003c- 1 to 5) yield { i * 2 } // 输出 2, 4, 6, 8, 10 println(res) 8. 倒序 for (i \u003c- 1 to 10 reverse) { println(i) } ","date":"2022-07-15","objectID":"/scala_4/:2:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"while和do..while Scala 中的while和do..while和 Java 中的用法一致 while 循环条件是返回一个布尔值的表达式 while先判断再执行 与for不同, while没有返回值, 即整个while语句的结果是Unit类型 do..while 循环条件是返回一个布尔值的表达式 do..while先执行再判断 ","date":"2022-07-15","objectID":"/scala_4/:2:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"循环中断 Scala 内置控制结构特地去掉了break和continue. 是因为更好的适应函数式编程, 推荐使用函数式的风格解决break和continue, 而不是一个关键字. Scala 中使用breakable控制结构来实现break和continue功能. ","date":"2022-07-15","objectID":"/scala_4/:3:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"异常的方式退出 def main(args: Array[String]): Unit = { try { for (elem \u003c- 1 to 10) { println(elem) if (elem == 5) throw new RuntimeException } } catch { case e =\u003e } println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"Scala 自带函数退出 import scala.util.control.Breaks def main(args: Array[String]): Unit = { Breaks.breakable( for (ele \u003c- 1 to 10) { println(ele) if (ele == 5) Breaks.break() } ) println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"对break进行省略 import scala.util.control.Breaks._ def main(args: Array[String]): Unit = { breakable( for (ele \u003c- 1 to 10) { println(ele) if (ele == 5) break } ) println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:3","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"continue import scala.util.control.Breaks._ def main(args: Array[String]): Unit = { for (ele \u003c- 1 to 10) { breakable( if (ele % 2 == 1) break else println(ele) ) } println(\"结束循环\") } 这里的breakable和上面的区别是将其放入到了循环内部, 这样可以实现结束本次执行而不是整个循环结束, 从而实现continue的功能. ","date":"2022-07-15","objectID":"/scala_4/:3:4","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":" Scala 运算符的使用和 Java 的基本相同, 只有个别细节上不同 ","date":"2022-07-14","objectID":"/scala_3/:0:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"算数运算符 运算符 运算 🌰 结果 + 正号 +3 3 - 负号 b=4;-b -4 + 加 5+5 10 - 减 6-4 2 * 乘 3*4 12 / 除 5/5 1 % 取模(取余) 7%5 2 + 字符串相加 \"Must\"+\"ard\" “Mustard” ","date":"2022-07-14","objectID":"/scala_3/:1:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"关系运算符(比较运算符) 运算符 运算 🌰 结果 == 相等于 4==3 false != 不等于 4!=3 true \u003c 小于 4\u003c3 false \u003e 大于 4\u003e3 true \u003c= 小于等于 4\u003c=3 false \u003e= 大于等于 4\u003e=3 true Java 和 Scala 中关于==的区别 Java: ==比较两个变量本身的值, 即两个对象在内存中的首地址 equals比较字符串中包含的内容是否相同 Scala: ==更类似于 Java 中的equals def main(args: Array[String]): Unit = { val s1 = \"abc\" val s2 = new String(\"abc\") // true println(s1 == s2) // false println(s1.eq(s2)) } ","date":"2022-07-14","objectID":"/scala_3/:2:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"逻辑运算符 运算符 描述 实例 (A=true, B=false) \u0026\u0026 逻辑与 A \u0026\u0026 B = false ` ` ! 逻辑非 !(A \u0026\u0026 B) = true ","date":"2022-07-14","objectID":"/scala_3/:3:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"位运算符 运算符 描述 实例 (a=60, b=13) =\u003e (a: 0011 1100, b: 0000 1101) \u0026 按位与 a \u0026 b = 12 =\u003e 0000 1100 ` ` 按位或 ^ 按位异或 a ^ b = 49 =\u003e 0011 0001 ~ 按位取反 ~a = -61 =\u003e 1100 0011 \u003c\u003c 左移 a \u003c\u003c 2 = 240 =\u003e 0011 0000 \u003e\u003e 右移 a \u003e\u003e 2 = 15 =\u003e 0000 1111 \u003e\u003e\u003e 无符号右移 a \u003e\u003e\u003e 2 = 15 =\u003e 0000 1111 ","date":"2022-07-14","objectID":"/scala_3/:4:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"赋值运算符 Scala中的赋值运算符就是 算数运算符/位运算符+= Scala 中没有++, --操作, 这点Swift和这里是相同的, 都是因为觉得这两个运算符并不符合面向对象的思想. 需要通过+=和-=来实现. ","date":"2022-07-14","objectID":"/scala_3/:5:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"Scala 运算符的本质 看源码可以知道, Scala 中其实是没有运算符的, 所有的运算符都是方法 object TestOpt { def main(args: Array[String]): Unit = { // 标准的加法运算 val i: Int = 1.+(1) // 1. 当调用对象的方法时, `.`可以省略 val j: Int = 1 + (1) // 2. 如果函数参数只有一个, 或者没有参数, `()`可以省略 val k: Int = 1 + 1 println(1.toString()) println(1 toString()) println(1 toString) } } ","date":"2022-07-14","objectID":"/scala_3/:6:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["big data"],"content":" 官方文档: 传送门 本文档针对 SeaTunnel v2.1.2 编写 ","date":"2022-07-12","objectID":"/seatunnel_use/:0:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel是什么 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"概述 先来看一下官方文档上是怎么说的: SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data. It can synchronize tens of billions of data stably and efficiently every day, and has been used in the production of nearly 100 companies. 翻译一下: SeaTunnel 是一个使用起来非常简单, 性能非常高效的分布式数据集成平台. 它支持海量数据的实时同步. 它可以每天稳定高效的同步数百亿的数据, 并且已经用于近百个公司的生产中. 可以从上面提炼出几个关键词: very easy-to-use: 使用非常简单. 其实 SeaTunnel 并不是对 Flink 或是 Spark 或是以后支持的其他技术的二次开发, 而是在其之上封装了一层, 使得这些技术使用起来会更简便. ultra-high-performance: 超高性能. real-time synchronization of massive data: 海量数据的实时同步. 以下是从官方文档摘抄: ","date":"2022-07-12","objectID":"/seatunnel_use/:1:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"为什么我们需要 SeaTunnel SeaTunnel 尽所能为您解决海量数据同步中可能遇到的问题： 数据丢失与重复 任务堆积与延迟 吞吐量低 应用到生产环境周期长 缺少应用运行状态监控 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel 使用场景 海量数据同步 海量数据集成 海量数据的 ETL 海量数据聚合 多源数据处理 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境依赖 组件 版本 备注 Java \u003e=8 Flink 1.13 因为当前版本SeaTunnel还未适配Flink1.14, 所以使用Flink1.13版本进行文档编写 Spark 2.x 如果是集群方式的话, Yarn/Standalone 都是支持的 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境配置 config目录下seatunnel-env.sh中可以配置Spark和Flink的环境 # Home directory of spark distribution. SPARK_HOME=${SPARK_HOME:-/opt/spark} # Home directory of flink distribution. FLINK_HOME=${FLINK_HOME:-/opt/flink} :-代表: 若未找到之前的地址, 则用之后的地址 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"官方示例 首先来进行一个官方示例简单的运行一下. 在config目录下创建文件example.conf: # 配置 Spark 或 Flink 的参数 env { # You can set flink configuration here execution.parallelism = 1 #execution.checkpoint.interval = 10000 #execution.checkpoint.data-uri = \"hdfs://hadoop102:9092/checkpoint\" } # 在 source 所属的块中配置数据源 # 默认端口: 9999 source { SocketStream{ host = hadoop102 result_table_name = \"fake\" field_name = \"info\" } } # 在 transform 的块中声明转换插件 # 这里需要说明的是: Split是不会立即生效的, 只有当sql插件中的sql语句中调用了split函数才会真正的作用在数据上 transform { Split{ separator = \"#\" fields = [\"name\",\"age\"] } sql { sql = \"select info, split(info) as info_row from fake\" } } # 在 sink 块中声明要输出到哪 sink { ConsoleSink {} } 然后cd到seatunnel目录在shell中执行: ./bin/start-seatunnel-flink.sh --config config/example.conf 用nc -lk 9999模拟一下socket连接 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"sql执行顺序 在source块中, 利用SocketStream插件读取出数据, 命名为fake表, 字段名为info 拿到info字段, 利用Split插件进行切分 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"启动命令 ./bin/start-seatunnel-flink.sh -h Usage: start-seatunnel-flink.sh [options] Options: -t, --check check config (default: false) * -c, --config Config file -h, --help Show the usage message -r, --run-mode job run mode, run or run-application (default: RUN) (values: [RUN, APPLICATION_RUN]) -i, --variable variable substitution, such as -i city=beijing, or -i date=20190318 (default: []) 其中, --config是必填参数 -i 当其中上面的sql改为: sql = \"select * from (select info, split(info) as info_row from fake) where age \u003e \"${age}\"\" 启动命令改为: ./bin/start-seatunnel-flink.sh --config config/example02.conf -i age=18 -r 执行 flink 自带的命令参数, 可以cd到 flink 下面 -\u003e ./bin/flink run -h查看 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"应用配置的4个基本组件 一个完整的SeaTunnel配置文件应包含四个配置组件: env{}` `source{}` --\u003e `transform{}` --\u003e `sink{} ","date":"2022-07-12","objectID":"/seatunnel_use/:4:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"env块 env块中可以直接写 spark 或 flink 支持的配置项. 比如并行度, 检查点时间, 检查点 hdfs 路径等. 以 Flink 为例, 在 SeaTunnel 源码的ConfigKeyName类中声明了所有可用的key: package org.apache.seatunnel.flink.util; public class ConfigKeyName { private ConfigKeyName() { throw new IllegalStateException(\"Utility class\"); } public static final String TIME_CHARACTERISTIC = \"execution.time-characteristic\"; public static final String BUFFER_TIMEOUT_MILLIS = \"execution.buffer.timeout\"; public static final String PARALLELISM = \"execution.parallelism\"; public static final String MAX_PARALLELISM = \"execution.max-parallelism\"; public static final String CHECKPOINT_INTERVAL = \"execution.checkpoint.interval\"; public static final String CHECKPOINT_MODE = \"execution.checkpoint.mode\"; public static final String CHECKPOINT_TIMEOUT = \"execution.checkpoint.timeout\"; public static final String CHECKPOINT_DATA_URI = \"execution.checkpoint.data-uri\"; public static final String MAX_CONCURRENT_CHECKPOINTS = \"execution.max-concurrent-checkpoints\"; public static final String CHECKPOINT_CLEANUP_MODE = \"execution.checkpoint.cleanup-mode\"; public static final String MIN_PAUSE_BETWEEN_CHECKPOINTS = \"execution.checkpoint.min-pause\"; public static final String FAIL_ON_CHECKPOINTING_ERRORS = \"execution.checkpoint.fail-on-error\"; public static final String RESTART_STRATEGY = \"execution.restart.strategy\"; public static final String RESTART_ATTEMPTS = \"execution.restart.attempts\"; public static final String RESTART_DELAY_BETWEEN_ATTEMPTS = \"execution.restart.delayBetweenAttempts\"; public static final String RESTART_FAILURE_INTERVAL = \"execution.restart.failureInterval\"; public static final String RESTART_FAILURE_RATE = \"execution.restart.failureRate\"; public static final String RESTART_DELAY_INTERVAL = \"execution.restart.delayInterval\"; public static final String MAX_STATE_RETENTION_TIME = \"execution.query.state.max-retention\"; public static final String MIN_STATE_RETENTION_TIME = \"execution.query.state.min-retention\"; public static final String STATE_BACKEND = \"execution.state.backend\"; public static final String PLANNER = \"execution.planner\"; } ","date":"2022-07-12","objectID":"/seatunnel_use/:4:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Row 在说明source块, transform块和sink块之前, 需要先了解一下 SeaTunnel 中的核心数据结构: Row Row 是 SeaTunnel 中数据传递的核心数据结构. 对来 Flink 说, source 插件需要给下游的转换插件返回一个 DataStream\u003cRow\u003e, 转换插件接到上游的 DataStream\u003cRow\u003e进行处理后需要再给下游返回一个 DataStream\u003cRow\u003e. 最后 Sink 插件将转换插件处理好的DataStream\u003cRow\u003e输出到外部的数据系统. 因为 DataStream可以很方便地和 Table 进行互转, 所以将 Row 当作核心数据结构可以让转换插件同时具有使用代码 (命令式) 和 sql (声明式) 处理数据的能力. 可以看一下上面示例中, 读取数据的源码: package org.apache.seatunnel.flink.socket.source; import ... @AutoService(BaseFlinkSource.class) public class SocketStream implements FlinkStreamSource { ... @Override public DataStream\u003cRow\u003e getData(FlinkEnvironment env) { final StreamExecutionEnvironment environment = env.getStreamExecutionEnvironment(); return environment.socketTextStream(host, port) .map((MapFunction\u003cString, Row\u003e) value -\u003e { Row row = new Row(1); row.setField(0, value); return row; }).returns(new RowTypeInfo(Types.STRING())); } } 感兴趣的话, 也可以看到源码中的 Split, sql, sink 都是用DataStream\u003cRow\u003e进行数据传递的 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"source块 source{}是可以配置多个 source 插件的 # 伪代码 env { ... } source { hdfs { ... } jdbc { ... } elasticsearch { ... } } transform { sql { sql = \"\"\" select ... from hdfs_table join es_table on hdfs_table.uid = es_table.uid where ...\"\"\" } } sink { elasticsearch { ... } } 需要注意的是: 所有的 source 插件中都可以声明result_table_name. 如果声明了result_table_name. SeaTunnel 会将 source 插件输出的DataStream\u003cRow\u003e转换为 Table 并注册在 Table 环境中. 当指定了result_table_name那么还可以指定field_name, 在注册时, 给 Table重设字段名. 因为每个 source 所需要的配置是不一致的 (result_table_name和field_name为共有非必填参数), 所以配置的时候查找官方文档会好一点 当前支持的source source 支持Spark 支持Flink 备注 文档地址 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ❌ 传送门 Fake ✔️ ✔️ 改类型主要是用于方便生成指定的数据, 用作 SeaTunnel 的功能验证, 测试和性能测试. 传送门 Feishu Sheet ✔️ ❌ 传送门 File ✔️ ✔️ 从本地或者 hdfs 中读取. 传送门 HBase ✔️ ❌ 传送门 Hive ✔️ ❌ 传送门 Http ✔️ ✔️ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDb ❌ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ Kafka 版本 \u003e= 0.10.0, 目前源码中发现 Schema 的解析有问题(原因为社区把 fastjson 换成 Jackon 引起的). 传送门 Kudu ✔️ ❌ 兼容 Kerberos 认证 传送门 MongoDb ✔️ ❌ 传送门 Neo4j ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 Socket ✔️ ✔️ 传送门 Tidb ✔️ ❌ 传送门 WebhookStream ✔️ ❌ 提供 http 接口推送数据, 仅支持 POST 请求 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"transform块 目前社区对 transform 插件做了很多规划, 但截至 v2.1.2 版本, 可用的插件有3个: Split, Sql和Json. 其中Json只适配 Spark 可用. transform{}中可以声明多个转换插件. 所有的转换插件都可以使用source_table_name, 和result_table_name. 同样, 如果声明了result_table_name, 那么就能声明field_name. Split插件 这里着重说一下 Split 插件: @Override public DataSet\u003cRow\u003e processBatch(FlinkEnvironment env, DataSet\u003cRow\u003e data) { return data; } @Override public DataStream\u003cRow\u003e processStream(FlinkEnvironment env, DataStream\u003cRow\u003e dataStream) { return dataStream; } @Override public void registerFunction(FlinkEnvironment flinkEnvironment) { if (flinkEnvironment.isStreaming()) { flinkEnvironment .getStreamTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } else { flinkEnvironment .getBatchTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } } 从源码中可以发现 Split 插件并没有对数据流进行任何的处理, 而是将它直接return了. 反之, 它向表环境中注册了一个名为 split 的 UDF(用户自定义函数). 而且, 函数名是写死的. 这意味着, 如果声明了多个 Split 后面的 UDF 还会把前面的覆盖. 从 Split 插件中就能看出了, 这个插件其实是通过注册方法的方式来调用的. 但是, transform 接口其实是预留了直接操作数据的能力的 (比如 Sql 插件中的处理方式), 也就是processStream方法. 那么, 一个 transform 插件其实同时履行了 process 和 UDF 的职责, 这是违背单一职责原则的. 所以要判断一个 transform 插件在做什么就只能从源码和文档的方面来加以区分了. Sql 插件 sql插件中需要特别说明的是, 指定source_table_name对于 sql 插件的意义不大, 因为可以通过from子句来决定从哪个表里抽取数据. ","date":"2022-07-12","objectID":"/seatunnel_use/:4:4","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Sink块 sink块里可以声明多个 sink 插件, 每个 sink 插件都可以指定source_table_name. 当前支持的sink source 支持Spark 支持Flink 备注 文档地址 Clickhouse ✔️ ✔️ 使用 Clickhouse-jdbc 根据字段名称对应数据源, 并将其写入. 使用前需创建对应的数据表. 传送门 ClickhouseFile ✔️ ✔️ 通过 clickhouse-local 程序生成 Clickhouse 数据文件, 然后将其发送到 Clickhouse 服务器, 也称为是 bulk load (批量加载). 传送门 Console ✔️ ✔️ 将数据输出到标准终端或 Flink 的 TaskManager. 通常用于调试和易于观察的数据. 传送门 Doris ✔️ ✔️ 传送门 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ✔️ Spark 支持的 Elasticsearch \u003e= 2.0 并且 \u003c 7.0.0; Flink 支持的 Elasticsearch = 7.x, 如果要用 Elasticsearch 6.x, 需用源码执行命令 mvn clean package -Delasticsearch=6重新打包. 传送门 Email ✔️ ❌ 支持通过 email 附件输出数据. 传送门 File ✔️ ✔️ 传送门 Hbase ✔️ ✔️ 使用 hbase-connectors 将数据输出到 Hbase(\u003e=2.1.0) 和 Spark(\u003e=2.0.0) 版本兼容性取决于 hbase-connectors. 传送门 Hive ✔️ ❌ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDB ❌️ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ 传送门 Kudu ✔️ ❌ 传送门 MongoDB ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 TiDb ✔️ ❌️ 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:5","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["Scala"],"content":"注释 Scala 的注释和 Java 的完全一样: // 1. 单行注释 /* 2. 多行注释 */ /** * 3. 文档注释 */ ","date":"2022-07-07","objectID":"/scala_2/:1:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"变量和常量 ","date":"2022-07-07","objectID":"/scala_2/:2:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 // var 变量名 [: 变量类型] = 初始值 var i: Int = 10 // val 常量名 [: 常量类型] = 初始值 val j: Int = 20 能用常量的地方就不要用变量 声明变(常)量时, 类型可以省略, 编译器自动推导, 即类型推导 类型确定后, 就不能更改, 因为 Scala 是强数据类型语音 变量声明时, 必须有初始值 ","date":"2022-07-07","objectID":"/scala_2/:2:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"标识符的命名规范 ","date":"2022-07-07","objectID":"/scala_2/:3:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"命名规范 Scala 中的标识符声明, 基本和Java是一致的, 但是细节上会有变化: 以字母或下划线开头, 后接字母、数字、下划线 以操作符开头, 且只包含操作符(+-*/#!等) 用反引号`…`包括的任意字符串, 即使是 Scala 关键字(39个)也可以 package, import, class, object, trait, extends, with, type, forSom private, protected, abstract, sealed, final, implicit, lazy, override try, catch, finally, throw if, else, match, case, do, while, for, return, yield def, var, val this, super new true, false, null 看几个特殊一点的🌰 object Hello { def main(args: Array[String]): Unit = { // ok 因为在 Scala 中 Int 是预定义的字符, 不是关键字, 但是不推荐 var Int: String = \"\" // ok 单独一个下划线不可作为标识符, 因为 _ 被认为是一个方法 var _: String = \"str\" // 会报错 println(_) // ok var -+*/#! : String = \"\" // error 以操作符开否, 必都是操作符 var -_*/#!1 : String = \"\" // error var if: String = \"\" // ok var `if`: String = \"\" } } ","date":"2022-07-07","objectID":"/scala_2/:3:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"字符串输出 ","date":"2022-07-07","objectID":"/scala_2/:4:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 字符串, 通过+号连接 printf用法: 字符串，通过%传值。 字符串模板(插值字符串): 通过$获取变量值 来看看🌰: object Hello { def main(args: Array[String]): Unit = { val name = \"mustard\" val age = 18 printf(\"name=%s, age=%d\", name, age) /** * 多行字符串， 在 Scala 中，利用三个双引号包围多行字符串就可以实现。 * 输入的内容，带有空格、 \\t 之类，导致每一行的开始位置不能整洁对齐。 * 应用 scala 的 stripMargin 方法，在 scala 中 stripMargin 默认是 \"|\" 作为连接符， * 在多行换行的行头前面加一个 \"|\" 符号即可。 */ val sql1 = \"\"\" |select | name | age |from user |where name = \"mustard\" \"\"\".stripMargin println(sql1) // 如果需要对变量进行运算，那么可以加 ${} val sql2 = s\"\"\" |select | name | age |from user |where name = \"$name\" and age = ${age + 2} \"\"\".stripMargin println(sql2) val s = s\"name=$name\" println(s) } } ","date":"2022-07-07","objectID":"/scala_2/:4:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"数据类型 ","date":"2022-07-07","objectID":"/scala_2/:5:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Java数据类型 先来回顾一下 Java 的数据类型: 基本数据类型(8种): char, byte, short, int, long, float, double, bollean 引用类型: (对象类型) 由于 Java 有基本类型, 并且基本类型并不是真正意义的对象, 所以 Java 语言并不是真正意义的面向对象. 注意哈: Java 中基本类型和引用类型没有共同的祖先 ","date":"2022-07-07","objectID":"/scala_2/:5:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Scala数据类型 Scala 中一切都是对象, 都是Any的子类 Scala 中数据类型分为两大类: 数值类型 (AnyVal), 引用类型 (AnyRef), 但是都是对象 Scala 数据类型仍然遵守隐式转换 (低精度的值向高精度的值自动转换) Scala 中的StringOps是对 Java 中的String增强 Unit: 对应 Java 中的void, 用于方法返回值的位置, 表示方法没有返回值. Unit是一个数据类型, 只有一个对象就是(). Void不是数据类型, 只是一个关键字. Null是一个类型, 只有一个对象就是null. 它是所有引用类型(AnyRef)的子类. Nothing: 是所有数据类型的子类, 主要用在一个函数没有明确返回值时使用, 因为这样可以把抛出的返回值, 返回给任何的变量或者函数. 整数类型 数据类型 描述 Byte[1] 8位有符号补码整数. 数值区间: -128 到 127 Short[2] 16位有符号补码整数. 数值区间: -32768 到 32767 Int[4] 32位有符号补码整数. 数值区间: -2147483648 到 2147483647 Long[8] 64位有符号补码整数. 数值区间: -2^64 到 2^64-1 Scala 的整型，默认为Int型，声明Long型，须后加l或L 浮点类型 数据类型 描述 FLoat[4] 32位 Double[8] 64位 object TestDataType { def main(args: Array[String]): Unit = { // 这是个 Float var n1 = 1.23456789f // 这是个 Double var n2 = 1.23456789 } } 字符类型 (Char) 布尔类型 (Boolean) 占 1 个字节 Unit 类型、Null 类型和 Nothing 类型 数据类型 描述 Unit 表示无值, 和其他语言中void等同. 用作不返回任何结果的方法的结果类型. Unit只有一个实例: (), 且没有实际意义 Null null, Null类型只有一个实例: null. Null可以赋值给任意引用类型(AnyRef), 但是不能赋值给值类型(AnyVal) Nothing Nothing类型在 Scala 的类层级最低端; 它是任何其他类型的子类型. 当一个函数, 确定没有正常的返回值时, 可以用Nothing来指定返回类型. Nothing的这种机制有一个好处: 可以把返回的值(异常)赋给其它的函数或者变量(兼容性) object TestDataType { def main(args: Array[String]): Unit = { var cat = new Cat() // 正确 cat = null // 错误 var n1: Int = null def test(): Nothing = { throw new Exception() } test } } 类型转换 数值类型自动转换: 精度小的类型自动转换为精度大的数值类型. (隐式转换) Byte \u003c Short \u003c Int \u003c Long \u003c Float \u003c Double 1. 自动提升原则: 有多种类型的数据混合运算时, 首先自动将所有数据转换成精度大的数据类型, 然后再计算 2. 把精度大的数值类型赋值给精度小的数值类型时, 会报错 3. (byte, short)和char之间不会相互自动转换 4. byte,short,char三者可以计算, 但会先转换为int类型 强制类型转换 可能造成精度降低或溢出, 所以需要特别注意 var num: Int = 2.7.toInt 数值类型和String类型间的转换 数值 -\u003e String: + \"\"就行, 和 Java 一样 String -\u003e 数值: s1.toInt, s1.toFloat, s1.toDouble, s1.toByte, s1.toLong, s1.toShort ","date":"2022-07-07","objectID":"/scala_2/:5:2","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["scala"],"content":" Scala这门语言是怎么发展过来的, 网上有很多资料, 这里就不赘述了. ","date":"2022-07-06","objectID":"/scala_1/:0:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"Scala和Java的关系 一般来说, 学习 Scala 之前都会或多或少的接触过 Java, 而 Scala 是基于 Java 的, 因此在学习 Scala 之前, 要先弄清楚 Java, Scala 和 JVM 的关系是很有用的. Java, Scala和JVM的关系 ","date":"2022-07-06","objectID":"/scala_1/:1:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置Scala环境 以 Windows 为例 首先确保 JDK1.8 安装成功 下载对应的 Scala 安装文件: 传送门 (我下载的是zip) 解压 zip 配置 Scala 的环境变量 配置 SCALA_HOME 在 Path 中添加 ","date":"2022-07-06","objectID":"/scala_1/:2:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置IDEA 安装插件 Scala 创建项目之后, 右键项目目录 -\u003e Add Framework Support -\u003e 选择 Scala ","date":"2022-07-06","objectID":"/scala_1/:3:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["big data"],"content":"数据仓库与数据湖的区别 说到湖仓一体, 就要先了解一下数据仓库和数据湖的区别是什么. 下面这个表格就是AWS(传送门)上的对比: 特性 数据仓库 数据湖 数据 来自事务系统、运营数据和业务线应用程序的关系数据 来自IoT设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据 Schema 设计数据仓库实施之前(写入型Schema) 写入在分析时(读取型Schema) 性价比 更快查询结果会带来较高存储成本 更快查询结果只需较低存储成本 数据质量 可作为重要事实依据的高度监管数据 任何可以或无法进行监管的数据(例如原始数据) 用户 业务分析师 数据科学家、数据开发人员和业务分析师(使用监管数据) 分析 批处理报告、BI和可视化 机器学习、预测分析、数据发现和分析 从上面这个表就能看出来数据仓库和数据湖的差别还是很明显的. 在企业中, 两者的作用是互补的, 所以不应该认为数据湖的出现是为了取代数据仓库, 毕竟两者的作用是截然不同的. ","date":"2022-06-16","objectID":"/data_lakehouse/:1:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体是怎么诞生的 对于数据而言, 数据仓库就像是一个大型图书馆, 里面的数据需要按照规范放好, 可以按照类别找到想要的信息. 而数据湖就像是一个大型仓库, 可以存储任何形式(结构化, 半结构化和非结构化)和任何格式(文本, 图像, 音频, 视频)的原始数据. 在产品的角度上来说, 数据仓库一般是独立标准化产品, 数据湖更像是一种架构指导, 需要配合着系列周边工具来实现业务需要. 也就是说, 数据湖的灵活性对于前期开发和前期部署是友好的; 数据仓库的规范性对于大数据后期的运行和长期发展是友好的. 那有没有一种新架构能兼具数据仓库和数据湖的优点? 然后, 湖仓一体就诞生了. 依据DataBricks公司对Lakehouse 的定义, 湖仓一体是一种结合了数据湖和数据仓库优势的新范式, 在用于数据湖的低成本存储上, 实现与数据仓库中类似的数据结构和数据管理功能. 湖仓一体是一种更开放的新型架构, 有人把它做了一个比喻, 就类似于在湖边搭建了很多小房子, 有的负责数据分析, 有的运转机器学习, 有的来检索音视频等, 至于那些数据源流, 都可以从数据湖里轻松获取. 需要注意的是: 数据湖 + 数据仓库 ≠ 湖仓一体 湖仓一体诞生的目的, 总结起来就是: 打通数据的存储与计算 灵活性与成长性兼得 灵活性与成长性 ","date":"2022-06-16","objectID":"/data_lakehouse/:2:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体的好处 数据重复性：如果一个组织同时维护了一个数据湖和多个数据仓库，这无疑会带来数据冗余。在最好的情况下，这仅仅只会带来数据处理的不高效，但是在最差的情况下，它会导致数据不一致的情况出现。湖仓一体的结合，能够去除数据的重复性，真正做到了唯一。 高存储成本：数据仓库和数据湖都是为了降低数据存储的成本。数据仓库往往是通过降低冗余，以及整合异构的数据源来做到降低成本。而数据湖则往往使用大数据文件系统和Spark在廉价的硬件上存储计算数据。湖仓一体架构的目标就是结合这些技术来最大力度降低成本。 报表和分析应用之间的差异：数据科学倾向于与数据湖打交道，使用各种分析技术来处理未经加工的数据。而报表分析师们则倾向于使用整合后的数据，比如数据仓库或是数据集市。而在一个组织内，往往这两个团队之间没有太多的交集，但实际上他们之间的工作又有一定的重复和矛盾。而当使用湖仓一体架构后，两个团队可以在同一数据架构上进行工作，避免不必要的重复。 数据停滞：在数据湖中，数据停滞是一个最为严重的问题，如果数据一直无人治理，那将很快变为数据沼泽。我们往往轻易的将数据丢入湖中，但缺乏有效的治理，长此以往，数据的时效性变得越来越难追溯。湖仓一体的引入，对于海量数据进行治理，能够更有效地帮助提升分析数据的时效性。 潜在不兼容性带来的风险：数据分析仍是一门兴起的技术，新的工具和技术每年仍在不停地出现中。一些技术可能只和数据湖兼容，而另一些则又可能只和数据仓库兼容。湖仓一体的架构意味着为两方面做准备。 ","date":"2022-06-16","objectID":"/data_lakehouse/:3:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"什么是数据湖 看了网上很多的资料, 关于数据湖的定义有很多, 我们先来看看AWS(传送门)的定义: 数据湖是一个集中式存储库，允许您以任意规模存储所有结构化和非结构化数据。您可以按原样存储数据（无需先对数据进行结构化处理），并运行不同类型的分析 – 从控制面板和可视化到大数据处理、实时分析和机器学习，以指导做出更好的决策。 再来看一下Wikipedia(传送门)的定义: 指使用大型二进制对象或文件这样的自然格式储存数据的系统 。它通常把所有的企业数据统一存储，既包括源系统中的原始副本，也包括转换后的数据，比如那些用于报表, 可视化, 数据分析和机器学习的数据。数据湖可以包括关系数据库的结构化数据(行与列)、半结构化的数据(CSV，日志，XML, JSON)，非结构化数据 (电子邮件、文件、PDF)和 二进制数据(图像、音频、视频)。 比较统一的一点是数据湖存储的是未经加工的原始数据, 包含结构化(如关系型数据库中的表)、半结构化(如CSV、日志、XML、JSON)和非结构化(如电子邮件、文档、PDF)的各类数据. 因为是原始数据, 所以也就保持着数据在业务系统中原来的样子. 这就是使得数据湖一定要具备完善的管理能力, 也就是要有完善的元数据, 可以管理各类数据相关的要素，包括数据源、数据格式、连接信息、数据schema、权限管理等. 因为数据湖是为了分析数据而演化来的, 也就要存储各类分析处理的中间结果, 并且还要记录完整的分析过程. 数据沼泽是一个劣化的数据湖, 用户无法访问, 或是没什么价值 ","date":"2022-06-15","objectID":"/date_lake/:1:0","tags":["big data","data lake"],"title":"数据湖","uri":"/date_lake/"},{"categories":["big data"],"content":"数据仓库的基本概念 ","date":"2022-06-14","objectID":"/data_warehouse/:1:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"概念 英文名为Data Warehouse, 简写为DW或DWH. 出现的目的是构建面向分析的集成化数据环境, 为企业提供决策支持(Decision Support). 因为分析性报告和决策支持目的而出现的技术. 之所以叫做\"仓库“而不是”工厂“就是因为DW本身是不生产或消费任何数据, 数据来源于外部, 并且开放给外部应用. ","date":"2022-06-14","objectID":"/data_warehouse/:1:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"基本特征 DW是面向主题的, 集成的, 非易失的和时变的数据集合, 用以支持决策. 面向主题(Subject Oriented) 在传统数据库中, 最大的特点就是面向应用进行数据的组织, 各个系统可能是相互分离的, 而数据仓库则是面向主题的. 抽象上来说, 主题就是在较高层次上讲企业信息系统中数据进行综合、归类分析利用. 每一个主题基本对应一个宏观分析领域. 逻辑上来说, 主题是对应企业中某一个宏观分析领域所涉及的分析对象. 集成的(Integrate) 通过对分散、独立、异构的数据库数据进行抽取、清理、转换和汇总便得到了数据仓库的数据， 这样能保证整个企业的数据的一致性, 避免了产生了信息孤岛. 数据仓库中的综合数据不能从原有的数据库系统直接得到. 所以在数据进入到DW之前, 必然要经过统一与综合(抽取和清洗), 这就是DW建设中, 最关键、最复杂的一步. 非易失性(Non-Volatile) 非易失性也可称为稳定性或不可更新性. 数据仓库的数据反映的是相当一段时间内的历史数据的内容, 是不同时点的数据库的快照的集合, 以及基于这些快照进行统计、综合和重组的导出数据. 基于这个特点, DW一般有大量的查询操作, 但修改和删除操作很少. 通常只需要定期的加载和更新. 时变(Time Variant) 数据仓库包含各种粒度的历史数据. 虽然说DW的用户不能修改数据, 但并不是说数据仓库的数据就是永远不变的. 分析的结果只能反映过去的情况, 当业务变化后, 挖掘出的模式会失去时效性. 所以说, DW中的数据需要更新, 以适应决策的需要. DW的数据时限一般要远远长于操作型数据的数据时限. 操作型系统存储的是当前的数据, 而数据仓库中的数据是历史数据. 数据仓库中的数据是按照时间顺序追加的, 它们都带有时间属性. ","date":"2022-06-14","objectID":"/data_warehouse/:1:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库与数据库的区别 这两者的区别其实就是OLAP与OLTP的区别. 联机事务处理OLTP (On-Line Transaction Processing), 也可以叫做面向交易的处理系统. 是针对具体业务在数据库联机的日常操作, 通常对少数记录进行查询、修改. 用户较为关心的是响应时间、数据的安全性、完整性和并发支持的用户数等问题. 例如MySQL, Oracle等关系型数据库一般属于OLTP 联机分析处理OLAP(On-Line Analytical Processing)一般针对某些主题的历史数据进行分析, 支持管理决策. 通过这两个的对比就能发现, 数据仓库的出现并不是为了替代数据库的. 数据库设计是尽量避免冗余, 一般是针对某一业务进行设计的. 而数据仓库在设计时有意引入冗余, 依照分析需求、分析维度、分析指标进行设计. 总的来说数据库是为了捕获数据而设计, 数据存库是为了分析数据而设计. ","date":"2022-06-14","objectID":"/data_warehouse/:2:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层架构 按照数据流入流出的过程, DW架构可分为: 数据运营层、数据仓库层、数据服务层. ","date":"2022-06-14","objectID":"/data_warehouse/:3:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据运营层 (ODS) Operation Data Store 数据准备区, 也叫做贴源层, 源数据. 数据仓库源头系统的数据表通常会原封不动的存储一份, 是后续数据仓库加工数据的来源. 来源方式: 业务库 经常会使用SQOOP来进行抽取, 🌰: 每天定时抽取一次 实时方面, 可以考虑用canal/FlinkCDC监听MySQL的binlog 埋点日志 日志一般是以文件的形式保存, 可以用flume定时同步 可以用spark streaming或Flink实时接入 kafka也可以 消息队列 来自ActiveMQ、Kafka的数据. ","date":"2022-06-14","objectID":"/data_warehouse/:3:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库层 (DW) DW数据分层, 由下到上为DWD, DWB, DWS. DWD:data warehouse details 细节数据层, 是业务层与数据仓库的隔离层. 主要对ODS数据层做一些数据清洗(去除空值、脏数据、超过极限范围的)和规范化的操作. DWB: data warehouse base 数据基础层, 存储的是客观数据, 一般用作中间层, 可以认为是大量指标的数据层. DWS: data warehouse service 数据服务层, 基于DWB上的基础数据, 整合汇总成分析某一个主题域的服务数据层, 一般是宽表(字段多的表). 用于提供后续的业务查询, OLAP, 数据分发等. 用户行为, 轻度聚合 主要对ODS/DWD层数据做一些轻度的汇总 ","date":"2022-06-14","objectID":"/data_warehouse/:3:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据服务层/应用层 (ADS) application data service 应用数据服务, 该层主要是提供数据产品和数据分析使用的数据, 一般会存储在ES、mysql等系统中供线上系统使用 一般会将大宽表, 比如报表数据放在这里 ","date":"2022-06-14","objectID":"/data_warehouse/:3:3","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"ETL 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL, 换句话说就是描述数据从源端经过**抽取(Extra)、转换(Transfer)、加载(Load)**到目的端的过程. ETL就是数据仓库的流水线, 而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定. ","date":"2022-06-14","objectID":"/data_warehouse/:3:4","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层的目的 用空间换时间: 通过大量的预处理来提升应用系统的用户体验(效率), 因此数据仓库会存在大量冗余的数据. 如果不分层的话, 如果源业务系统的业务规则发生变化将会影响整个数据清洗过程, 工作量巨大. 简化数据清洗的过程: 把原来一步的工作分成多个步骤来完成. 每一层的处理逻辑都相对简单和容易理解了, 这样比较容易保证每个步骤的正确性. ","date":"2022-06-14","objectID":"/data_warehouse/:3:5","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库元数据的管理 元数据(Meta data), 一句话就能总结: 关于数据的数据. 数据仓库中的元数据**主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态. 并且是整个数据仓库系统运行的基础, 元数据把数据仓库系统中各个松散的组件联系起来, 组成一个有机的整体. ** ","date":"2022-06-14","objectID":"/data_warehouse/:4:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["Operating System"],"content":"地址空间 之前已经说过了, 地址空间分为两种: 物理地址空间: 硬件支持的地址空间 这部分地址空间的管理和控制是由硬件来完成的. 逻辑地址空间: 一个运行的程序所拥有的内存范围 相对于物理地址空间而言, 程序所能\"看到的\"逻辑地址空间更简单一点, 就是一个一维的线性的地址空间. 但是最终, 逻辑地址和物理地址都是需要对应上的, 这部分就是由OS来完成的. ","date":"2022-03-17","objectID":"/os_address/:1:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"地址生成 用一个C语言的函数来举个🌰 逻辑地址生成 从最开始的**‘符号逻辑地址’到最终的‘具体逻辑地址’**, 经过了上面的这些转换过程, 而这些过程是基本上不需要操作系统来帮助的, 而是通过应用程序、编译器或者Loader就可以完成. 但是当把这个地址放入到内存中之后也是逻辑地址而不是物理地址. 再把之前的操作系统架构中的图片拿出来: 物理地址生成 CPU方面: 运算器需要在逻辑地址的内存内容 内存管理单元(MMU)寻找在逻辑地址和物理地址之间的映射 控制器从总线发送在物理地址的内存内容的请求 内存方面: 内存发送物理地址内存的内容给CPU 操作系统方面: 建立逻辑地址和物理地址之间的映射 **操作系统的一个重要的作用就是: 确保放在内存中的程序相互之间不能相互干扰, 每个程序去访问地址空间是合法的. 换而言之, 确保每个程序访问地址空间是在一个范围之内的. ** ","date":"2022-03-17","objectID":"/os_address/:2:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"计算机基本硬件结构 CPU主要是完成对程序的控制 内存主要放置程序的代码和处理的数据 设备 计算机基本硬件结构 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:1:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"内存分层体系 放上这张图(这张图应该很多人都看到过) 内存延时🌰 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统需要完成的四个目标 抽象: 逻辑地址空间 我们希望应用程序在内存中运行的时候, 不用考虑很多细节(物理内存地址在什么地方, 外设在什么地方), 只需要访问一个连续的地址空间就可以了, 即逻辑地址空间. 保护: 独立地址空间 因为在内存中运行着多个不同的应用程序. 运行的过程中, 有可能会访问其他应用程序的地址空间并造成破坏, 所以就需要将不同应用程序的运行空间进行隔离. 共享: 访问相同内存 不同的程序之间除了隔离之外, 还会有交互. 所以操作系统提供共享的内存空间来完成. 虚拟化: 更多的地址空间 当内存中的程序过多的时候, 有可能会出现内存不够的情况. 这样, 就会把急需要地址空间的程序放在内存中, 暂时不需要地址空间的程序先放在硬盘上去. ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:1","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统中管理内存的不同方法 程序重定位 分段 分页 虚拟内存 按需分页虚拟内存 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:2","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"实现高度依赖于硬件 必须知道内存架构 MMU(内存管理单元): 硬件组件负责处理CPU的内存访问请求 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:3","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"系统调用, 异常, 中断的特点 ","date":"2022-03-16","objectID":"/os_interface/:1:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"来源 系统调用: 应用程序 异常: 应用程序 中断: 外设 ","date":"2022-03-16","objectID":"/os_interface/:1:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"处理时间 系统调用: 异步或同步 异常: 同步 中断: 异步 ","date":"2022-03-16","objectID":"/os_interface/:1:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"响应 系统调用: 等待和持续 异常: 杀死或重新执行意想不到的应用程序指令 中断: 持续, 对用户应用程序是透明的 ","date":"2022-03-16","objectID":"/os_interface/:1:3","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断和异常的处理机制 中断陈外设的事件 异常是内部CPU的事件 中断和异常迫使CPU访问一些被中断和异常服务访问的功能 ","date":"2022-03-16","objectID":"/os_interface/:2:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断 硬件 设置中断标记**[CPU初始化]** 将内部、外部事件设置中断标记 中断时间的ID 软件 保存当前处理状态: 为了确保之后能从打断的地方能够继续执行 中断服务程序处理 清除中断标记 恢复之前保存的处理状态 ","date":"2022-03-16","objectID":"/os_interface/:2:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"异常 保存现场 异常处理 杀死产生异常的程序 重新执行异常指令 (这种情况下, 对于用户来说, 异常就是透明的) 恢复现场 ","date":"2022-03-16","objectID":"/os_interface/:2:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"系统调用 系统调用和前面的中断和异常从名字上看就不一样(认真脸). 应用程序需要系统提供一些服务, 而这些服务不能由应用程序直接执行, 需要由操作系统来执行, 这个过程就需要接口, 这个接口就称为系统调用接口. 举个🌰, C语言中的printf()就会触发系统调用write(). 程序访问主要是通过高层次的API接口, 而不是直接进行系统调用. Win32 API 用于 Windows POSIX API 用于 POSIX-based systems (包括UNIX, Linux, Mac OS 的所有版本) Java API 用于 JVM 通常, 每个系统调用相关的序号. 系统调用接口根据这些需要来维护表的索引. 系统调用接口调用内核态中预期的系统调用, 并返回系统调用的状态和其他任何返回值 用户不需要知道系统调用是如何实现的, 只需要获取API和了解操作系统将什么作为返回结果. 操作系统接口的细节大部分都隐藏在API中, 通过运行程序支持的库来管理(包括编译器的库来创建函数集). 系统调用和传统的函数调用是有区别的: 当应用程序发出函数调用的时候, 是在一个栈空间完成了参数的传递和参数的返回. 系统调用的执行过程中, 应用程序和OS是有各自的堆栈. 当应用程序发出系统调用的时候, 当切换到内核中执行之后也需要切换堆栈. 同时, 也需要完成特权级的转换(从用户态转为内核态). 也就是说系统调用的开销是比函数调用的开销大得多, 但是这样的做法也提高了安全性. ","date":"2022-03-16","objectID":"/os_interface/:3:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"跨越操作系统边界的开销 通过上面可以了解到, 其实系统调用, 异常和中断就是操作系统和应用程序, 以及操作系统和外设之间跨越了边界. 在执行时间上的开销超过程序调用 开销: 建立中断/异常/系统调用号与对应服务例程映射关系的初始化开销 建立内核堆栈 验证参数, 因为操作系统是不信任应用程序的 内核态映射到用户态的地址空间, 更新页面映射权限. 这是一个拷贝的过程, 因为不能将内核态中的数据简单的用传递指针的方式传递给用户态. 内核态独立地址空间, TLB ","date":"2022-03-16","objectID":"/os_interface/:3:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"首先要知道, OS并没有放在内存当中, 而是放在了DISK中的. 当开机的时候, 首先是由BIOS(Basic Input Output System)检测各种各样的外设, 通过了之后才会去加载OS(由BootLoader完成). BIOS在内存中是有一个固定的地址的, 以x86为例, BIOS是存放在CS:IP = 0xf000:fff0这个地址中的(CS: 段寄存器; IP: 指令寄存器) . BootLoader启动过程 ","date":"2022-03-15","objectID":"/os_bootloader/:0:0","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"操作系统与设备和程序交互 操作系统的Interface包含三个: 系统调用(system call): 来源于应用程序主动向操作系统发出服务请求 异常(exception): 来源于不良的操作程序, 非法指令或者其他坏的处理状态(🌰 内存出错) 中断(interrupt): 来源于外设. 来自不同的硬件设备的计时器和网络的中断 ","date":"2022-03-15","objectID":"/os_bootloader/:0:1","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"那为什么应用程序不能直接去访问外设呢, 而是通过操作系统? 首先从安全的角度, OS是一个特殊的应用软件, 和其他的应用程序最大的不同就是, OS对整个计算机有控制权, 它是可信任的. 如果应用程序直接访问外设的话, 很容易造成整个系统的崩溃. 另一个方面, 希望通过操作系统, 给上层的应用提供简单, 一致的接口, 使得应用程序不用关注底层设备的复杂性和差异性. ","date":"2022-03-15","objectID":"/os_bootloader/:0:2","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Data Structure"],"content":"首先, 看一个🌰. 现在有一个五子棋程序, 其中有一个存盘退出和续上盘的功能. 二维数组记录棋盘 从上面这张图就能看到, 很多值就是默认值(0), 也就是说记录了很多没有意义的值, 所以就引出了稀疏数组. ","date":"2022-03-07","objectID":"/sparsearray/:0:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"基本介绍 当一个数组中大部分元素都是同一个值的数组时, 可以使用稀疏数组来保存该数组. 稀疏数组的处理方法是: 记录数组一共几行几列, 有多少个不同的值 把有不同值的元素的行列及值记录在一个小规模的数组中, 从而所辖程序的规模 // 一个正常的二维数组, 一共有7行8列, 其中大部分都是0 int[][] array = { {0,1,0,0,0,0,0,0}, {0,0,0,2,0,0,0,0}, {0,0,0,0,0,0,3,0}, {0,-4,0,0,0,-5,0,0}, {0,0,0,0,0,0,0,0}, {0,0,0,0,-6,0,0,0}, {0,0,7,0,0,0,0,0}, } 行(row) 列(column) 值(value) 7 8 7 0 1 1 1 3 2 2 6 3 3 1 -4 3 5 -5 5 4 -6 6 2 7 表中第一行记录了一共几行几列以及多少个非零值 从上面这个🌰就能看出来, 将原本需要7*8=56个空间变为了3*8=24个空间. ","date":"2022-03-07","objectID":"/sparsearray/:1:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"实现思路 二维数组 转 稀疏数组 遍历原始的二维数组, 得到有效数据的个数sum 根据sum就可以创建稀疏数组int[sum+1][3] 将二维数组的有效数据存入到稀疏数组 稀疏数组 转 二维数组 先读取稀疏数组第一行, 根据第一行的数组创建原始二维数组 在读取稀疏数组后面的数据, 并赋值给二维数组 ","date":"2022-03-07","objectID":"/sparsearray/:2:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"线性结构 作为最常用的数据结构, 特点是数据元素之间存在一对一的线性关系 有两种不同的存储结构: 顺序存储结构和练市存储结构. 顺序存储的线性表称为顺序表, 顺序表中的存储元素是连续的. 链式存储的线性表称为链表, 链表中的存储元素不一定是连续的, 元素节点中存放数据元素以及相邻元素的地址信息. 线性结构常见的有: 数据, 队列, 链表和栈. ","date":"2022-03-07","objectID":"/linear_nonlinear/:1:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Data Structure"],"content":"非线性结构 已经不是一对一的关系了, 其结构包括: 二维数组, 多维数组, 广义表, 树结构, 图结构. ","date":"2022-03-07","objectID":"/linear_nonlinear/:2:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Syntactic"],"content":"什么是fail-fast 百度百科上是这么写的: fail-fast是Java集合(Collection)中的一种错误机制. 我觉得不太全面. 下面来看一下维基百科上这怎么写的: In systems design, a fail-fast system is one which immediately reports at its interface any condition that is likely to indicate a failure. Fail-fast systems are usually designed to stop normal operation rather than attempt to continue a possibly flawed process. Such designs often check the system’s state at several points in an operation, so any failures can be detected early. The responsibility of a fail-fast module is detecting errors, then letting the next-highest level of the system handle them. 从上面这段话就能看出来, fail-fast是在系统设计当中的一种错误检测机制, 一旦检测到可能发生错误, 就立即抛出异常, 程序不再继续运行. ","date":"2022-03-06","objectID":"/failfast/:1:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"集合中的fail-fast 首先, 来复现这个错误: List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 运行上面的代码就会抛出这样的异常: Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) 上面的代码是在for-each中要删除集合中的元素而抛出的异常. 同样的, 增加(add())元素也会抛出这个异常. ","date":"2022-03-06","objectID":"/failfast/:2:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"异常产生的原因 来看一下源码: public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // ... } public boolean remove(Object o) { if (o == null) { for (int index = 0; index \u003c size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u003c size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } private void fastRemove(int index) { modCount++; // ... } 从这段代码可以发现, 对ArrayList的add(), remove(), clear()方法, 只要涉及到改变集合中的元素的个数的方法都会导致modCount的改变, 但是没有对expectedModCount进行改变. final void checkForComodification() { if (expectedModCount != modCount) throw new ConcurrentModificationException(); } 当expectModCount和modCount不同的时候, 就会抛出一开始出现的异常. ","date":"2022-03-06","objectID":"/failfast/:3:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"避免产生异常 ","date":"2022-03-06","objectID":"/failfast/:4:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"1. 使用普通for循环进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (int i = 0; i \u003c nameList.size(); i++) { String name = nameList.get(i); if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 这样虽然不会报错, 但是可能会产生漏删的情况出现. ","date":"2022-03-06","objectID":"/failfast/:4:1","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"2. 使用Iterator进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; Iterator\u003cString\u003e iterator = nameList.iterator(); while (iterator.hasNext()) { if (iterator.next().equals(\"丑八怪\")) { iterator.remove(); } } ","date":"2022-03-06","objectID":"/failfast/:4:2","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"3. 其实使用for-each也可以 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); break; } } 当集合中只删除一个元素的时候, 在删除操作之后立即break, 使循环不进入下一次遍历就可以了. ","date":"2022-03-06","objectID":"/failfast/:4:3","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"4. 使用fail-safe的集合类 ","date":"2022-03-06","objectID":"/failfast/:4:4","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"5. 使用stream中的filter","date":"2022-03-06","objectID":"/failfast/:4:5","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Data Structure"],"content":" 前两天在LeetCode做题的时候, 做到了二分查找(Binary Search). 现在来做一下梳理. 地址: LeetCode-Binary Search 二分查找作为程序员的一个基本技能, 也是面试的时候有可能面试官会问到的一种算法. 可以达到O(log n)的时间复杂度. 一般来说, 当出现这几个条件的时候, 就应该用到二分查找: 待查找的数组是有序的或者是部分有序的 要求时间复杂度低于O(n), 或者直接说明时间复杂度是O(log n) 而且二分查找也有很多的变体. 在使用的时候, 要注意好查找条件, 判断条件以及左右边界的条件变更方式. 这三个地方没有注意好, 很容易就会出现死循环或是遗漏. 今天来梳理一下这几种: 标准的二分查找 二分查找左边界 二分查找右边界 二分查找的左右边界 二分查找极值点 ","date":"2022-03-01","objectID":"/binarysearch/:0:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"标准的二分查找 上面这个题目就用标准的二分查找来实现, 我们先来看标准的二分查找的模板: public int search(int[] array, int target) { int left = 0, right = array.length-1; // 1. 因为循环中包含了 left == right 的条件, 所以每次循环的时候, left或right都要有变化 while (left \u003c= right) { // 这句话其实等同于 (right+left)/2, 但是这样的写法可以避免溢出 int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] == target) { return mid; } else if (array[mid] \u003c target) { // 左边界更新 left = mid + 1; } else { // 右边界更新 right = mid - 1; } } // 未查找到目标值 return -1; } ","date":"2022-03-01","objectID":"/binarysearch/:1:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界 使用这种变体的时候通常有这几种特性: 数组有序, 包含重复元素 数组部分有序, 包含重复元素 数组部分有序, 不包含重复元素 ","date":"2022-03-01","objectID":"/binarysearch/:2:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界① 这种分类包含了上面的1,3两种情况. 既然是查找左边界, 就要从右侧开始, 然后不断左移. 也就是说, 即使找到了array[mid] == target, 这个mid值也不见得就是要查找的左边界. 所以还是要继续收缩: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else { right = mid; } } return array[left] == target ? left : -1; } 可以很明显的看出来和标准的有何不同: 查找条件变为了left \u003c right 因为在最后left与right相邻的时候, mid和left处在同一位置. 所以下一步, left, mid, right都会在同一位置. 也就是说, 如果判断条件还是left \u003c= right的话, 可能最后就会进入死循环. 右边界更新为right = mid 因为需要在查找到目标值之后继续向左移动. ","date":"2022-03-01","objectID":"/binarysearch/:2:1","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界② 这种分类包含了上面的2情况, 也就是数组部分有序, 包含重复元素. 这种条件下, 右边界在向左移动的时候, 不能简单的令right = mid. 因为有重复的元素, 这样就可能会造成遗漏: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else if (array[mid] \u003e target) { right = mid; } else { --right; } } return array[left] == target ? left : -1; } ","date":"2022-03-01","objectID":"/binarysearch/:2:2","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Java"],"content":" 今天进行了一场面试, 面试官在问我关于HashMap的时候, 感觉自己回答的不是很好, 所以现在索性就梳理一下Java关于集合的这部分知识. 主要是问了这么几个问题: HashMap是线程安全的么 那线程安全的map是哪种? 在定义HashMap的时候会有定义长度的习惯么? HashMap的底层是怎么实现的? HashMap是如何存储的? HashMap最大长度是多少? 或者说是达到多大的长度就需要扩容了? (这个没答上来…😭) 说到Java的Collection就一定会放出这张神图 根据这张图能发现, 这一切的一切都起始于Iterable接口. ","date":"2022-02-24","objectID":"/java_collection/:0:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Iterable 从源码里能看到, 这个接口允许对象成为for-each的循环目标, 也就是增强型for循环, 是Java中的一种语法糖. List\u003cObject\u003e list = new ArrayList(); // 补充: 数组也可`for-each`遍历 // Object[] list = new Object[5]; for (Object obj: list) {} 其他遍历方式 JDK 1.8 之前, Iterable只有一个方法: Iterator\u003cT\u003e iterator(); 这个接口能够创建一个轻量级的迭代器, 用于安全的遍历元素, 移除元素, 添加元素. 其中涉及了一个概念就是fail-fast. 总结起来就是: 能创建迭代器进行元素添加和删除的话, 就尽量使用迭代器进行添加和删除操作. for (Iterator it = list.iterator(); it.hasNext(); ) { System.out.println(it.next()); } ","date":"2022-02-24","objectID":"/java_collection/:1:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"顶层接口 Collection是一个顶层接口, 主要是用来定义集合的约定. List接口也是一个顶层接口, 继承了Collection接口, 同时也是ArrayList, LinkedList等集合元素的父类. Set接口位于与List接口同级的层次上, 它同时也继承了Collection接口. Set接口提供了额外的规定. 对add(), equals(), hashCode()方法提供了额外的标准. Queue是和List, Set接口并列的Collection的三大接口之一. Queue的设计用来在处理之前保持元素的访问次序. 除了Collection基础的操作外, 对立提供了额外的插入, 读取, 检查操作. SortSet接口直接继承与Set接口, 使用Comparable对元素进行自然排序或者使用Comparator在创建时对元素提供定制的排序规则. set的迭代器将按升序元素顺序遍历集合. Map是一个支持key-value存储的对象, Map不能包含重复的key, 每个键最多映射一个值. 这个接口替代了Dictionary类, Dictionary是一个抽象类而不是接口. ","date":"2022-02-24","objectID":"/java_collection/:2:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"ArrayList ArrayList是实现List接口的可扩容数组(动态数组), 它的内部是基于数组实现的, 具体的定义: public class ArrayList\u003cE\u003e extends AbstractList\u003cE\u003e implements List\u003cE\u003e, RandomAccess, Cloneable, java.io.Serializable {...} ArrayList可以实现所有可选择的列表操作, 允许所有元素 (包括 null). ArrayList还提供了内部存储list的方法, 它能够完全替代Vector, 只有一点例外, ArrayList不是线程安全的容器. 下面会说到Vector ArrayList有一个容量的概念, 这个数组的容量就是List用来存储元素的容量. 在不声明容量的时候, 默认的是10. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.5, 即: 旧容量 * 1.5 ==\u003e 10-\u003e15-\u003e22-\u003e33... ArrayList的上限为Integer.MAX_VALUE - 8(232 - 8). ArrayList不是线程安全的容器, 所以可以使用线程安全的List: List list = Collections.synchronizedList(new ArrayList\u003c\u003e()); ArrayList具有fail-fast快速失败机制, 当在迭代集合的过程中, 该集合发成了改变的时候, 就可能会发生fail-fast, 抛出ConcurrentModificationException异常. ","date":"2022-02-24","objectID":"/java_collection/:3:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Vector Java中的Vector类是允许不同类型共存的变长数组, Java.util.Vector提供了向量(Vector)类以实现类似动态数组的功能. 在相对于ArrayList来说, Vector线程是安全的, 也就是说是同步的. 因为Vector对内部的每个方法都是简单粗暴的上锁, 所以访问元素的效率远远低于ArrayList. 还有一点在扩容上, ArrayList扩容后的数组长度会增加50%, 而Vector的扩容长度后数组是翻倍. ","date":"2022-02-24","objectID":"/java_collection/:4:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedList LinkedList是一个双向链表, 允许所有元素 (包括 null): LinkedList所有的操作都可以表现为双向性, 索引到链表的操作将遍历从头到尾, 看那个距离短为遍历顺序. LinkedList不是线程安全的容器, 所以可以使用线程安全的Set: List list = Collections.synchronizedList(new LinkedList\u003c\u003e()); 因为LinkedList是一个双向链表, 所以没有初始化大小, 没有扩容机制. ","date":"2022-02-24","objectID":"/java_collection/:5:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Stack 堆栈(Stack)就是常说的后入先出的容器. 它继承了Vector类, 提供了常用的pop, push和peek操作, 以及判断stack是否为空的empty方法和寻找与栈顶距离的search方法. ","date":"2022-02-24","objectID":"/java_collection/:6:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashSet HashSet是Set接口的实现类, 有哈希表支持 (实际上HashSet是HashMap的一个实例), 不能保证集合的迭代顺序. 允许所有元素 (包括 null). HashSet不是线程安全的容器, 所以可以使用线程安全的Set: Set set = Collections.synchronizedSet(new HashSet\u003c\u003e()); 支持fail-fast机制. 因为HashSet的底层实际使用HashMap实现的, 所以和HashMap的容量和扩容机制是一致的: 在不声明容量的时候, 默认的是16. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.75, 即: 旧容量 * 1.75 ==\u003e 16-\u003e28-\u003e49-\u003e85... ","date":"2022-02-24","objectID":"/java_collection/:7:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeSet TreeSet是一个基于TreeMap的NavigableSet实现. 这些元素使用他们的自然排序或者在创建时提供的Comparator进行排序, 具体取决于使用的构造函数. 此实现为基本操作add, remove, contains提供了log(n)的时间成本. HashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:8:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashSet LinkedHashSet继承体系 LinkedHashSet是Set接口的Hash表和LinkedList的实现. 但是这个实现不同于HashSet的是, 它维护者一个贯穿所有条目的双向列表. 此链表定义了元素插入集合的顺序. 注意: 如果元素重新插入, 则插入顺序不会受到影响. LinkedHashSet有两个影响其构成的参数: 初始容量和负载因子. 它们的定义与HashSet完全相同. 但是对于LinkedHashSet, 选择过高的初始容量值的开销要比HashSet小, 因为LinkedHashSet的迭代次数不收容量影响. LinkedHashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:9:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"PriorityQueue PriorityQueue(优先级队列)是AbstractQueue的实现类, 其中的元素根据自然排序(最小元素最先出)或者通过构造函数时期提供的Comparator来排序, 具体根据构造器判断. 注意: PriorityQueue不允许null元素. 队列的头在某种意义上是指定顺序的最后一个元素. 队列查找操作poll, remove, peek和element访问队列头部元素. PriorityQueue是无界队列(无限制的), 但是有内部capacity, 用户控制用于在队列中存储元素的数组大小. 该类以及迭代器实现了Collection, Iterator接口的所有可选方法. 这个迭代器提供了iterator()方法不能保证以任何特定顺序遍历PriorityQueue. 如果需要有序遍历的话, 可以考虑使用Arrays.sort(pq.toArray()). PriorityQueue必须存储的可比较的对象, 如果不是的话, 则必须指定比较器. PriorityQueue不是线程安全的容器, 而线程安全的类是PriorityBlockingQueue. ","date":"2022-02-24","objectID":"/java_collection/:10:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashMap HashMap是一个利用哈希表原理来存储元素的集合, 允许空的key-value键值对. HashMap的默认初始用量和负载因子和HashSet一致. HashMap不是线程安全的容器. ","date":"2022-02-24","objectID":"/java_collection/:11:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeMap 一个基于NavigableMap实现的红黑树. 这个map根据key自认排序存储, 或者通过Comparator进行定制排序. TreeMap为containsKey,get,put和remove方法提供了log(n)的时间开销. TreeMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:12:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashMap LinkedHashMap是Map接口的哈希表和链表的实现. 这个实现与HashMap的不同之处在于它维护了一个贯穿其所有条目的双向链表. 这个链表中定义的顺序, 通常是插入的顺序. 提供了一个特殊的构造器: LinkedHashMap(int,float,boolean), 其遍历的顺序是其最后一次访问的顺序. 可以重写removeEldestEntry(Map.Entry)方法, 以便在将新映射添加到map时强制删除过期映射的策略. 这个类提供了所有可选择的map操作, 并且允许null元素. 由于维护链表的额外开销, 性能可能会低于HashMap, 有一条除外: 遍历LinkedHashMap中的collection-views需要与map.size成正比, 无论其容量如何. HashMap的迭代看起来开销更大, 因为还要求时间与其容量成正比. LinkedHashMap有两个因素影响了它的构成: 初始容量和负载因子. LinkedHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:13:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashTable 与HashMap不同的是, HashTable是线程安全的. 任何非空对象都可以用作键或值. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:14:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"IdentityHashMap IdentityHashMap 是一个比较小众的Map实现类. IdentityHashMap不是一个通用的Map实现, 虽然这个类实现了Map接口, 但是它故意违反了Map的约定, 该约定要求在比较对象时使用equals方法, 此类仅适用于需要引用相等语义的极少数情况. IdentityHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:15:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"WeakHashMap WeakHashMap类基于哈希表的Map基础实现, 带有弱键. WeakHashMap中的entry当不再使用时还会自动移除. 也就是说, WeakHashMap中的entry不会增加其引用计数. 基于map接口, 是一种弱键相连, WeakHashMap里面的键会自动回收. 支持null键和null值. 支持fail-fast机制 WeakHashMap经常用作缓存. ","date":"2022-02-24","objectID":"/java_collection/:16:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"集合实现类特征图 下面这个表格汇总了部分集合框架的主要实现类的特征 集合 排序 随机访问 key-value存储 重复元素 空元素 线程安全 ArrayList ✅ ✅ ❌ ✅ ✅ ❌ LinkedList ✅ ❌ ❌ ✅ ✅ ❌ HashSet ❌ ❌ ❌ ❌ ✅ ❌ TreeSet ✅ ❌ ❌ ❌ ❌ ❌ HashMap ❌ ✅ ✅ ❌ ✅ ❌ TreeMap ✅ ✅ ✅ ❌ ❌ ❌ Vector ✅ ✅ ❌ ✅ ✅ ✅ HashTable ❌ ✅ ✅ ❌ ❌ ✅ ConcurrentHashMap ❌ ✅ ✅ ❌ ❌ ✅ Stack ✅ ❌ ❌ ✅ ✅ ✅ CopyOnWriteArrayList ✅ ✅ ❌ ✅ ✅ ✅ ","date":"2022-02-24","objectID":"/java_collection/:17:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Security"],"content":"什么是SQL注入 SQL注入攻击是通过将恶意的SQL语句插入到应用的输入参数中, 再在后台SQL服务器上解析执行的攻击. 是目前对数据库进行攻击的最常用手段之一. 主要原因是程序对用户输入数据的合法性没有判断和处理. ","date":"2022-02-24","objectID":"/sqlinjection/:0:1","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"原理 恶意拼接查询 都知道SQL语句是用;进行分隔两个语句的: SELECT * FROM users WHERE user_id = $user_id; 其中, user_id是传入参数, 但如果传入的参数变为1;DELETE FROM users;, 那么语句最终就会变为: SELECT * FROM users WHERE user_id = 1;;DELETE FROM users; 如果执行了上面的语句, 那么user表中所有数据都被删除了. 利用注释执行非法命令 SQL语句中可以添加注释: SELECT * FROM users WHERE user_gender='男' AND user_age=$age 如果user_age中包含了恶意的字符串20 OR 25 AND SLEEP(500)--, 那么语句最终会变为: SELECT * FROM users WHERE user_gender='男' AND user_age=20 OR 25 AND SLEEP(500)-- 上面这条语句只是想耗尽系统资源, SLEEP(500)会一直执行, 但是如果其中添加了修改, 删除数据的语句, 将会造成更大的破坏. 传入非法参数 SQL语句中的字符串是用单引号包裹的, 但是如果其本身包含单引号而没有处理, 那么就可能篡改SQL语句的作用: SELECT * FROM users WHERE user_name=$user_name 如果user_name传入参数值M'ustard, 那么语句最终会变为: SELECT * FROM users WHERE user_name='M'ustard' 一般情况下, 执行上面语句就会报错, 但是这种方式可能会产生恶意的SQL语句. 添加额外条件 在SQL语句中添加一些额外添加, 来改变执行行为. 条件一般为真值表达式: UPDATE users SET user_password=$user_password where user_id=$user_id 如果user_id传入的是1 OR TRUE, 那么语句最终会变为: UPDATE users SET user_password='123456' where user_id=1 OR TRUE 如果执行了上面的语句, 那么所有用户的密码都被更改了. ","date":"2022-02-24","objectID":"/sqlinjection/:0:2","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"防御手段 过滤输入内容, 校验字符串 过滤掉用户输入中的不合法字符剔除掉, 可以使用编程语言提供的处理函数或自己封装的函数来过滤, 也可以使用正则表达式来进行匹配. 也要验证参数的类型, 比如字符串或者整型. 参数化查询 参数化查询是目前被视作预防SQL注入攻击最有效的方法. 指的设计数据库连接并访问数据时, 在需要填入数据的地方, 使用参数(Parameter)来给值. MySQL的参数格式是以?加上参数名称而成: UPDATE table_1 SET row_1=?row_1, row_2=row_2 WHERE row_3=?row_3 在使用参数化查询下, 数据库不会将参数的内容视为SQL语句的一部分来处理, 而是在数据库完成SQL语句的编译之后, 才套用参数执行. 因此就算参数中含有破坏性的指令, 也不会被数据库所运行. 安全测试, 安全审计 避免使用动态SQL 不要将敏感数据保留在纯文本中 限制数据库权限和特权 避免直接向用户显示数据库错误 ","date":"2022-02-24","objectID":"/sqlinjection/:0:3","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"什么是CSRF攻击 CSRF攻击全称跨站请求伪造(Cross-site request forgery), 也被称为one-click attack或session riding, 通常缩写为CSRF或者XSRF. 尽管CSRF和XSS攻击很像, 但是两者的方式截然不同, 甚至可以说是相左的. XSS利用的是用户对指定网站的信任, CSRF利用的是网站对用户网页浏览器的信任. 我理解的是XSS是攻击者对浏览器下手, CSRF是对用户下手. 可以这么理解: 攻击者盗用受信任用户的身份, 以该用户的名已发送恶意请求. ","date":"2022-02-23","objectID":"/csrf/:0:1","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"CSRF攻击的原理 用户1访问受信任的网站A, 输入用户名和密码登录网站A. 登录成功之后, 网站A产生Cookie信息并返回给浏览器. 用户1在未退出登录网站A的情况下, 在同一浏览器, 访问网站B. 网站B接收到请求后, 携带网站A的Cookie信息, 发出一个请求去访问网站A 网站A在接收到网站B的请求之后, 会以用户1的权限处理该请求, 从而导致用户1的隐私泄漏和财产安全受到威胁. ","date":"2022-02-23","objectID":"/csrf/:0:2","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"常见的几种类型 GET类型的CSRF: 这种类型的CSRF利用非常简单, 只需要一个HTTP请求: \u003cimg src=http://xxx.com/csrf?user=xxx /\u003e 在访问这个img的页面后, 成功发出了一次HTTP请求. POST类型的CSRF: 这种类型的CSRF没有GET型的大, 利用起来通常使用的是一个自动提交的表单: \u003cform action=http://xxx.com/csrf.php method=POST\u003e \u003cinput type=\"text\" name=\"user\" value=\"xxx\" /\u003e \u003c/form\u003e \u003cscript\u003edocument.forms[0].submit();\u003c/script\u003e 其他类型CSRF: \u003cimg src=http://admin:admin@192.168.1.1 /\u003e 在访问这个img的页面后, 路由器会给用户一个合法的SESSION, 就可以进行下一步操作了. ","date":"2022-02-23","objectID":"/csrf/:0:3","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"防御手段 目前防御CSRF攻击主要有三种策略: 检查HTTP Referer字段 根据HTTP协议, 在HTTP头中有一个字段叫Referer, 它记录了该HTTP请求的来源地址. 优势: 简单易行, 网站的普通开发人员不需要操心CSRF的漏洞, 只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查Referer的值就可以了. 特别是对于已有的系统来说, 不需要更改当前系统的任何代码和逻辑, 没有风险. 劣势: 这种方式是把安全性都依赖于浏览器来保障, 对于一些低版本的浏览器来说, 是可以篡改Referer值的. 对于某些最新的浏览器, 虽然不能篡改Referer的值, 但是用户有些时候会认为Referer值会记录下用户的访问来源, 所以用户自己可以设置浏览器在发送请求的时候不再提供Referer. 这种情况下, 此种方式就会认为是CSRF攻击, 从而拒绝合法用户的访问. 添加校验token CSRF之所以能够成功是因为攻击者获取到了Cookie信息. 如果能够不只是依靠Cookie中的信息来抵御CSRF攻击, 那么就可以防御了. 所以这种方式就是在HTTP请求中以参数的形式加入一个随机产生的token, 并且在服务器端建立一个拦截器来验证这个token. 优势: 这种方法比检查Referer更安全一些, token可以在用户登录之后产生并放与SESSION中, 然后每次请求时把token从SESSION中取出来进行比对. 劣势: 难以保证token本身的安全. 疑问: 可以做到每次验证token成功之后, 产生一个新的token是否可以避免此种方法可能产生的漏洞. 在HTTP头中自定义属性并验证 这种方法也是使用token进行验证, 但是不是放在参数中, 而是放在HTTP请求头的自定义属性中. 通过XMLHttpRequest这个类, 可以一次性给所有该类请求加上csrftoken这个HTTP头属性, 并把token放进去. 优势: 不会记录到浏览器的地址栏, 统一管理token输入输出, 可以保证token的安全性. 劣势: 无法在非异步的请求上实施. ","date":"2022-02-23","objectID":"/csrf/:0:4","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":" 昨天的时候, 部门老大提到session和OnceToken的时候, 说到了xss攻击和csrf攻击. 今天记录一下学习的内容. ","date":"2022-02-23","objectID":"/xss/:0:0","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"什么是XSS攻击 XSS攻击全称跨站脚本攻击(Cross Site Scripting), 是一种在Web应用中的计算机安全漏洞, 它允许恶意Web用户将代码植入到提供给其他用户使用的正常页面中. 之所以缩写是XSS, 是因为如果是CSS会和层叠样式表混淆(Cascading Style Sheets). 举个简单的🌰, 恶意服务器嵌套了正常服务器中某页面的某form表单. 当用户登录了正常服务器之后, 在恶意服务器上也可以提交这个表单, 甚至拿到更高的权限. XSS是最普遍的Web应用安全漏洞. 可以做到劫持用户会话, 插入恶意内容, 重定向用户, 使用恶意软件劫持用户浏览器, 繁殖XSS蠕虫, 甚至是破坏网站, 修改路由器配置信息等. 所以XSS攻击的危害还是很严重的. ","date":"2022-02-23","objectID":"/xss/:0:1","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"XSS原理 首先看看XSS可以插入哪里: script标签内容 HTML注释内容 HTML标签的属性名 HTML标签的属性值 HTML标签的名字 直接插入到CSS里 看到XSS可以插入到这些地方之后, 就更能理解它的原理. XSS通过一些被特殊对待的文本和标记(🌰, 小于符号\u003c 被看做是HTML标签的开始), 使得用户浏览器将这些误认为是插入了正常的内容, 所以就会在用户浏览器中被执行. 也就是说, 当这些特殊字符不能被动态页面检查或检查出现失误时, 就将会产生XSS漏洞. ","date":"2022-02-23","objectID":"/xss/:0:2","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"分类 存储型XSS攻击(持久型): 最直接的危害类型. 将XSS代码提交存储到服务器端(数据库, 内存, 文件系统等). 这样下次请求目标页面时就不用再提交XSS代码, 会从服务器获取. 一般出现在留言, 评论, 博客日志等交互. 存储型XSS攻击 反射性XSS攻击(非持久型): 最普遍的类型. 通过特定手法(🌰email), 诱使用户访问一个包含恶意代码的地址, 当用户点击这些链接的时候, 恶意代码会在用户的浏览器执行. 一般出现在搜索栏, 用户登录口, 常用来窃取客户端Cookies或进行钓鱼欺骗. 反射型XSS攻击 DOM-based 型XSS攻击: 通过/xss修改页面的DOM(Document Object Model)结构, 是纯粹发生在客户端的攻击. 在整个攻击过程中, 服务器响应的页面没有发生变化, 取出和执行恶意代码都由浏览器端完成, 属于前端自身的安全漏洞. ","date":"2022-02-23","objectID":"/xss/:0:3","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"防御手段 总体思路: 对用户的输入(和URL参数)进行过滤, 对输出进行编码. 也就是说, 对用户提交的所有内容进行过滤, 对URL中的参数进行过滤, 过滤掉会导致脚本执行的相关内容; 然后对动态输出到页面的内容进行html编码, 使脚本无法在浏览器中执行. 还可以服务端设置会话Cookie的HTTP Only属性, 这样客户端的JS脚本就不能获取Cookie信息了. ","date":"2022-02-23","objectID":"/xss/:0:4","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":null,"content":"Java Overview (Java Platform SE 8 ) (oracle.com) Java 8 中文版 Overview (Java SE 11 \u0026 JDK 11 ) (oracle.com) Java 11 中文版 Nacos Maven各版本地址 ","date":"2022-02-22","objectID":"/documents/:1:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Big Data Apache Kafka The Scala Programming Language (scala-lang.org) Apache Flink: Stateful Computations over Data Streams Apache SeaTunnel ","date":"2022-02-22","objectID":"/documents/:2:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"iOS Swift - Resources - Apple Developer Vapor Xcode Releases ","date":"2022-02-22","objectID":"/documents/:3:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Other 力扣（LeetCode）中国官网 Markdown 入门基础 | Markdown 官方教程 飞桨PaddlePaddle-源于产业实践的开源深度学习平台 Elasticsearch Guide 8.0 ","date":"2022-02-22","objectID":"/documents/:4:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":["MySQL"],"content":"有了数据库之后, 还需要先进行压测 拿到一个数据库之后, 首先得先对这个数据库进行一个较为基本的基准压测. 也就是说, 你得基于一些工具模拟一个系统每秒发出1000个请求到数据库上去, 观察一下他的CPU负载、磁盘IO负载、网络 IO负载、内存复杂, 然后数据库能否每秒处理掉这1000个请求, 还是每秒只能处理500个请求? 这个过程, 就是压测. 那为什么不等到Java系统都开发完之后, 直接让Java系统连接上MySQL数据库, 然后直接对Java系统进行压测呢? 因为数据库的压测和它上面的Java系统的压测, 其实是两回事, 首先得知道数据库最大能抗多大压力, 然后再去看Java系统能抗多大压力. 因为有一种可能是, 数据库每秒可以抗下2000个请求, 但是Java系统每秒只能抗下500个请求. 所以不能光是对Java系统去进行压测, 在那之前也得先对数据库进行压测, 做到心里有个数. ","date":"2022-01-10","objectID":"/mysql_4/:1:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"QPS和TPS到底有什么区别 既然是要压测, 那么肯定得先明白一点, 每秒能抗下多少个请求, 其实是有专业术语的, 分别是QPS和TPS. QPS: Query Per Second. 也就是说数据库每秒可以处理多少个请求, 大致可以理解为一次请求就是一条SQL语句, 也就是说数据库可以每秒处理多少个SQL语句. Java系统或者中间件系统在进行压测的时候, 也可以使用这个指标. TPS: Transaction Per Second. 指的是每秒可以处理的事务量. 就是说数据库可以每秒处理多少次事务提交或者回滚. ","date":"2022-01-10","objectID":"/mysql_4/:2:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"IO相关的压测性能指标 IOPS: 指的是机器的随机IO并发处理能力. 举个🌰: 机器可以达到200 IOPS, 意思就是说每秒可以执行200个随机IO读写请求. 这个指标很关键, 因为在之前说过, 在内存中更新的脏数据库, 最后都由后台IO线程在不确定的时间, 刷回到磁盘中去, 这就是随机IO的过程. 也就是说, 如果IOPS指标太低, 那么就会导致内存里的脏数据库刷回磁盘的效率不高. 吞吐量: 指的是机器的磁盘存储每秒可以读写多少字节的数据量. 这个指标也很关键, 之前也说过, 在执行各种SQL语句的时候, 提交事务的时候, 其实都是大量的会写redo log之类的日志的, 这些日志都会直接写磁盘文件. 所以一台机器的存储每秒可以读写多少字节的数据量, 就决定了他每秒可以把多少redo log之类的日志写入到磁盘里去. 一般来说, 我们写redo log之类的日志, 都是对磁盘文件进行顺序写入的, 也就是一行接着一行的写, 不会说进行随机的读写, 那么一般普通磁盘的顺序写入的吞吐量每秒都可以达到200MB左右. 所以通常而言, 机器的磁盘吞吐量都是足够承载高并发请求的. latency: 指的是往磁盘里写入一条数据的延迟. 这个指标同样重要, 因为执行SQL语句的和提交事务的时候, 都需要顺序写redo log磁盘文件, 所以此时写一条日志到磁盘文件里去, 到底是延迟1ms, 还是延迟100us, 这就对数据库的SQL语句执行性能是有影响的. ","date":"2022-01-10","objectID":"/mysql_4/:3:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"压测的时候要关注的其他性能指标 CPU负载: 这个也是一个很重要的指标. 因为假设数据库压测到了3000 QPS, 可能其他指标都还正常, 但是此时CPU负载特别高, 那么也说明你的数据库不能继续往下压测更高的QPS了, 否则CPU是吃不消的. 网络负载: 这个就是看机器带宽情况下, 在压测到一定的QPS和TPS的时候, 每秒钟机器的网卡会输入多少MB数据, 会输出多少MB数据. 因为有可能网络带宽最多每秒传输100MB的数据, 那么可能QPS到1000的时候, 网卡就打满了, 已经每秒传输100MB的数据了, 此时即使其他指标还正常, 也不能继续压测下去了. 内存负载: 这个就是看压测到一定情况下的时候, 机器内存损耗了多少, 如果说机器内存损耗过高了, 说明也不能继续压测下去了. ","date":"2022-01-10","objectID":"/mysql_4/:4:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"推荐压测工具 sysbench, 这个工具可以自动帮你在数据库里构建出来大量的数据. 然后也可以模拟几千个线程并发的访问数据库, 模拟各种sql语句, 包括各种书屋提交到数据库里, 甚至可以模拟出几十万的TPS对数据库进行压测. ","date":"2022-01-10","objectID":"/mysql_4/:5:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"生产数据库一般用什么配置的机器 首先要明确的一点, 如果系统是一个没什么并发访问量, 用户就几十个人或者几百个人的系统, 那么其实选择什么样的机器去部署数据库, 影响不大. 哪怕是个人笔记本电脑去部署一个MySQL数据库, 其实也能支撑地并发系统的运行. 因为这种系统可能每个几分钟才会有一波请求发到数据库上, 而且数据库里一张表也许就几百条, 几千条或者是几万条. 数据量很小, 并发量很小, 操作频率很低, 用户量很小, 并发量很小, 只不过可能系统的业务逻辑很复杂而已. 对于这类系统的数据库机器选型, 什么样都可以. ","date":"2021-12-27","objectID":"/mysql_3/:1:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"普通的Java应用系统部署在机器上能抗多少并发 通常来说, Java应用系统部署的时候常选用的机器配置大致是2核4G和4核8G的较多一些, 数据库部署的时候常选用的机器配置最低在8核16G以上, 正常在16核32G. 那么以大量的高并发线上系统的生产经验观察下来而言, 一般Java应用系统部署在4核8G的机器上, 每秒钟抗下500左右的并发访问量, 差不多是比较合适的. 当然这个也不是绝对的, 假设每个请求花费1s可以处理完, 那么一台机器每秒也许只可以处理100个请求, 但是如果每个请求只要花费100ms就可以处理完, 那么一台机器每秒也许就可以处理几百个请求. 所以一台机器能抗下每秒多少请求, 往往是跟每个请求处理耗费多长时间关联的. 但是大体上来说, 根据大量的经验观察而言, 4核8G的机器部署普通的Java应用系统, 每秒大致能抗下几百的并发访问, 从每秒一两百请求到每秒七八百请求, 都是有可能的, 关键是每个请求耗费多长时间. ","date":"2021-12-27","objectID":"/mysql_3/:2:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"高并发场景下, 数据库应该用什么样的机器 对于数据库而言, 上文也说了, 通常推荐的数据至少是选用8核16G以上的机器更加合适. 因为要考虑一个问题, 对于我们的Java应用系统, 主要耗费时间的是Java系统和数据库之间的网络通信. 对Java系统自己而言, 如果仅仅只是系统内部运行一些普通的业务逻辑, 纯粹在自己内存中完成一些业务逻辑, 这个性能是极高极高的. 对于Java系统受到的每个请求, 耗时最多的还是发送网络请求到数据库上去, 等待数据库执行一些SQL语句, 返回结果给Java系统. 所以其实常说的Java系统压力很大, 负载很高. 其实主要的压力和复杂都是集中在依赖的那个MySQL数据库上! 因为执行大量的增删改查的SQL语句的时候, MySQL数据库需要对内存和磁盘文件进行大量的IO操作, 所以数据库往往是负载最高的! 通过经验而言, 一般8核16G的机器部署的MySQL数据库, 每秒抗个一两千并发请求是没问题的, 但是如果并发量再高一些, 假设每秒有几千并发请求, 那么可能数据库就会有危险了, 因为数据库的CPU、磁盘、IO、内存的负载都会很高, 弄不好数据库压力过大就会宕机. 对于16核32G的机器部署的MySQL数据库而言, 每秒两三千, 甚至三四千的并发也都是可以的, 但是如果达到每秒上万请求, 也是会有宕机的危险. 如果可以的话, 数据库机器周最好用SSD的硬盘而不是机械硬盘. ","date":"2021-12-27","objectID":"/mysql_3/:3:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"申请机器机器之后做好心中有数, 交给专业的DBA部署 数据库机器申请下来之后, 作为架构师要对机器做到心中有数. 比如申请的是8核16G的机器, 心里大致就该知道这个数据库每秒抗个一两千请求是可以的, 如果申请的是16核32G的机器, 那心里知道妥妥可以抗个每秒两三千, 甚至三四千的请求. 其次, 申请一台机器下来之后, 接着这台机器在有一定规模的公司里, 一定是交给公司专业的DBA去安装、部署和启动MySQL的. DBA这个时候会按照他国王的经验, 用自己的MySQL生产调优参数模板, 直接放到MySQL里去, 然后用一个参数模板去启动这个MySQL, 往往这里很多参数都是调优过的. 而且DBA还可能对linux机器一些OS内核参数进行一定的调整, 比如说最大文件句柄之类的参数, 这些参数往往也都是需要调整的. ","date":"2021-12-27","objectID":"/mysql_3/:4:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"什么是InnoDB? InnoDB是第一个提供外键约束的存储引擎, 而且它对事务的处理能力是其它存储引擎无法与之比拟的. MySQL在5.5版本之后, 默认存储引擎由MyISAM修改为InnoDB. 目前, InnoDB是最重要的, 也是使用最广泛的存储引擎. 1. InnoDB优势: 支持事务安装 灾难恢复性好 使用行级锁 实现了缓冲处理 支持外键 适合需要大型数据库的网站 2. 物理存储 数据文件(表数据和索引数据): 共享表空间 独立表空间 日志文件 ","date":"2021-12-23","objectID":"/mysql_2/:1:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新语句在MySQL中是如何执行的? 首先假设有一条语句是这样的: UPDATE users SET name='xxx' WHERE id=10 这条语句是如何执行的呢? 首先肯定是系统通过一个数据库连接发送到了MySQL上, 然后经过SQL接口、解析器、优化器、执行器几个环节, 解析SQL语句, 生成执行计划, 接着由执行器负责这个计划的执行, 调用InnoDB存储引擎的接口去执行. ","date":"2021-12-23","objectID":"/mysql_2/:2:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"InnoDB的重要内存结构: 缓冲池 前面提到了InnoDB的一个优势就是\"实现了缓冲处理\", 就是通过InnoDB存储引擎中的一个非常重要的放在内存里的组件实现的, 就是缓冲池(Buffer Pool). 这个里面会存很多数据, 便于以后的查询, 要是缓冲池里有数据, 就不会去磁盘查询. 所以当执行上面那条更新语句的时候, 就会现将id=10这一行数据看看是否在缓冲池里, 如果不在的话, 那么会直接从磁盘里加载到缓冲池里来, 而且还会对这行记录加锁. ","date":"2021-12-23","objectID":"/mysql_2/:3:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"undo日志文件: 如何让你更新的数据可以回滚 接着下一步, 假设id=10这行数据的name原来是\"zhangsan\", 现在我们更新为\"xxx\", 那么此时得现将要更新的原来的值\"zhangsan\"和id=10这些信息, 写入到undo日志文件中去. 其实大家都知道, 如果执行一个更新语句是在一个事务里的话, 那么事务提交之前我们都是可以对数据进行**回滚(rollback)**的. ","date":"2021-12-23","objectID":"/mysql_2/:4:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新buffer pool中的缓存数据 当我们把要更新的那行记录从磁盘文件加载到缓冲池, 也对其进行加锁之后, 并且还把更新前的旧值写入undo日志文件之后, 就可以正式开始更新这行记录了. 更新的时候, 先是会更新缓冲池中的记录, 此时这个数据就是脏数据了. 为什么是脏数据: 因为此时的磁盘中id=10这行数据的name还是\"zhangsan\", 还不是\"xxx\". ","date":"2021-12-23","objectID":"/mysql_2/:5:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"Redo Log Buffer: 万一系统宕机, 如何避免数据丢失 如果按照上面的操作进行更新, 现在已经把内存里的数据进行了修改, 但是磁盘上的数据还没有修改. 就在这个时候, 系统宕机了, 该怎么办? 这个时候就必须要把对内存所做的修改写入到一个Redo Log Buffer里去, 这也是一个内存的缓冲区, 用来存放redo日志的. redo日志用来记录对什么记录进行了修改, 比如对id=10这行记录修改了name为\"xxx\", 这就是一个日志. 这个redo日志其实就是用来在MySQL突然宕机的时候, 用来恢复更新过的数据的. ","date":"2021-12-23","objectID":"/mysql_2/:6:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"如果还没提交事务, MySQL宕机了怎么办 如果还没有提交事务, 那么此时如果MySQL崩溃, 必然导致内存里Buffer Pool中的修改过的数据都丢失, 同时写入Redo Log Buffer中的redo日志也会消失. 其实此时数据丢失是不要紧的, 因为一个更新语句, 没提交事务, 就代表还没有执行成功, 此时MySQL宕机虽然导致内存里的数据都丢失了, 但是会发现, 磁盘上的数据怡然居还停留在原样子. 换句话说, id=10那行数据的name字段的值还是旧值\"zhangsan\", 所以此时这个事务就是执行失败了, 没能成功完成更新, 会收到一个数据库的异常. 然后当MySQL重启之后, 数据并没有任何变化. 所以, 如果还没提交事务时, MySQL宕机了, 不会有任何问题. ","date":"2021-12-23","objectID":"/mysql_2/:7:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"提交事务的时候将redo日志写入磁盘中 接着俩要提交一个事务了, 此时就会根据一定的策略把redo日志从redo log buffer里刷入到磁盘文件里去. 这个策略是通过innodb_flush_log_at_trx_commit来配置的, 它有几个选项. 当这个参数的值为0的时候, 那么当提交事务的时候, 不会把redo log buffer里的数据刷入磁盘文件, 此时可能都提交事务了, 结果MySQL宕机了, 然后内存里的数据全部丢失了. 这就相当于提交事务成功了, 但是由于MySQL宕机, 导致内存中的数据和redo日志都丢失了. 当这个参数的值为1的时候, 那么当提交事务的时候, 就必须把redo log buffer从内存刷入到磁盘文件里去, 只有事务提交成功, 那么redo log就必然在磁盘里了. 所以只有提交事务成功之后, redo日志一定在磁盘文件里. 也就是说, 哪怕此时buffer pool中更新过的数据还没刷新到磁盘里去, 此时内存里的数据已经是更新过name=\"xxx\", 然后磁盘上的数据还是没更新过的name=\"zhangsan\". 当MySQL重启之后, 可以根据redo日志去恢复之前做过的修改. 当这个参数的值是2的时候, 那么当提交事务的时候, 把redo日志写入磁盘文件对应的os cache里去, 而不是直接进入磁盘文件, 可能1秒之后才会吧os cache里的数据写入到磁盘文件里去. 这种模式下, 提交事务之后, redo log可能仅仅停留在os cache内存缓存里, 没实际进入磁盘文件, 玩意此时要是机器宕机了, 那么os cache里的redo log就会丢失, 同样会感觉提交事务了, 但是结果数据丢了. ","date":"2021-12-23","objectID":"/mysql_2/:8:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"什么是CRUD? CRUD是指在做计算处理时的增加(Create), 读取查询(Retrieve), 更新(Update)和删除(Delete). 主要是被用在描述软件系统中DataBase或者持久层的基本操作. ","date":"2021-12-13","objectID":"/mysql_1/:1:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"什么是数据库驱动? 想要访问数据库, 就需要和数据库建立一个网络连接. 那么, 建立这个网络连接的就是数据库驱动. 所以对于MySQL来说, 对应每种常见的编程语言(e.g. Java, PHP, .NET, Python, Ruby等), MySQL都会提供对应语言的MySQL驱动. 其实数据库驱动就是中间件. ","date":"2021-12-13","objectID":"/mysql_1/:2:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"数据库连接池是用来干嘛的? 首先要知道的是, 一个系统和数据库建立的连接往往都不止一个. 但是每次访问数据库的时候都建立一个连接, 然后执行SQL语句, 然后再销毁这个连接, 这种方式显然是不合适的. 因为每次建立一个数据库连接都很耗时, 效率会很低下. 所以, 就出现了数据库连接池这个东西. 一个数据库连接池里会维持多个连接, 让多个线程使用里面的不同的数据库连接去执行SQL语句, 执行完语句之后, 不是销毁这个连接, 而是把它放回池子里, 后面还能继续用. 常见的数据库连接池有DBCP, C3P0, Druid ","date":"2021-12-13","objectID":"/mysql_1/:3:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"MySQL是如何执行一条SQL语句的? ","date":"2021-12-13","objectID":"/mysql_1/:4:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第一步 线程: 接收SQL语句 首先, 假设数据库服务器的连接池中的某个连接收到了网络请求, 假设就是一条SQL语句, 这个工作一定是一个线程来进行处理的, 来监听请求以及读取请求数据. 当MySQL的工作线程接收到SQL语句之后, 会转交给SQL接口去执行. SQL接口(SQL Interface)就是MySQL内部提供的一个组件, 是一套执行SQL语句的接口. ","date":"2021-12-13","objectID":"/mysql_1/:4:1","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第二步 SQL接口: 解析SQL语句 那么, SQL接口又是如何执行SQL语句的呢? 比如要执行下面这条语句: SELECT id, name, age FROM users WHERE id = 1; MySQL本身也是一个系统, 是一个数据库管理系统(DBMS), 是没法直接理解这些语句的, 所以就需要**查询解析器(Parser)**出场了! 这个查询解析器是负责对SQL语句进行解析的, 比如上面的语句拆解一下, 可以拆解为一下几个部分: 我们现在要从users表里查询数据 查询id字段的值等于1的那行数据 对查出来的哪行数据要提取里面的id, name, age三个字段 所谓的SQL解析, 就是按照既定的SQL语法, 对我们按照SQL语法规则编写的SQL语句进行解析, 然后理解这个SQL语句要干什么事情. ","date":"2021-12-13","objectID":"/mysql_1/:4:2","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第三步 查询优化器: 选择最优查询路径 当通过解析器理解了SQL语句要干什么之后, 接着就会找查询优化器(Optimizer)来选择一个最优的查询路径. ","date":"2021-12-13","objectID":"/mysql_1/:4:3","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第四步 存储引擎: 调用存储引擎接口, 真正执行SQL语句 最后一步, 就是把查询优化器选择的最有查询路径交给底层的存储引擎去真正的执行. 存储引擎是MySQL架构设计中很有特色的一个环节. 在真正执行SQL语句的时候, 要不是更新数据, 要不是查询数据, 但是具体的数据是存放在内存里还是在磁盘里呢? 这个时候就需要存储引擎了, 存储引擎其实就是执行SQL语句的, 它是按照一定的步骤去查询内存缓存数据, 更新磁盘数据, 查询磁盘数据等等, 执行诸如此类的一系列的操作. MySQL的架构设计中, SQL接口, SQL解析器, 查询优化器其实都是通用的, 就是一套组件而已. 但是是支持各种各样的存储引擎的, 比如常见的InnoDB, MyISAM, Memory等等, 我们是可以选择使用哪种存储引擎来负责具体的SQL语句执行的. ","date":"2021-12-13","objectID":"/mysql_1/:4:4","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第五步 执行器: 根据执行计划调用存储引擎的接口 现在回过头来看一个问题, 存储引擎可以访问内存和磁盘上的数据, 那么是谁来调用存储引擎的接口呢? 其实还漏了一个执行器的概念, 执行器会根据优化器选择的执行方案, 去调用存储引擎的接口按照一定的顺序和步骤, 就把SQL语句的逻辑给执行了. 举个🌰: 比如执行器可能会先调用存储引擎的一个接口, 去获取users表中的第一行数据, 然后判断一下这个数据的id字段的值是否等于我们期望的值, 如果不是的话, 就继续调用存储引擎的接口, 去获取users表的下一行数据. 基于上述的思路, 执行器就会去根据优化器生成的一套执行计划, 然后不停的调用存储引擎的各种接口去完成SQL语句的执行计划, 大致就是不停的更新或者提取一些数据出来. ","date":"2021-12-13","objectID":"/mysql_1/:4:5","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":null,"content":"初衷 Since: 2021-11-20 08:27:00 ","date":"2021-11-20","objectID":"/about/:1:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"期许 不卑不亢，不矜不伐，戒骄戒躁 不嗔不怒，不争不弃，独善其身 ","date":"2021-11-20","objectID":"/about/:2:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"About me 在职: iOS开发程序猿, Java开发程序猿, 大数据开发小学徒 用我所学, 学我所用. 不盲目堆叠技术栈, 保持谦逊, 保持探索欲, 砥砺前行. ","date":"2021-11-20","objectID":"/about/:3:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"Other Annual Summary /years/ ","date":"2021-11-20","objectID":"/about/:4:0","tags":null,"title":"关于 Buli Home","uri":"/about/"}]