[{"categories":["Kafka"],"content":"定义 Kafka传统定义: Kafka 是一个分布式的基于发布/订阅模式的消息队列(Message Queue) 发布/订阅: 消息的发布者不会讲消息直接发送给特定的订阅者, 而是将发布的消息分为不同的类型, 订阅者只接受感兴趣的消息. Kafka最新定义: Kafka是一个开元的分布式事件流平台(Event Streaming Platform), 被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用. ","date":"2022-07-26","objectID":"/kafka_1/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列 目前比较常见的消息队列产品主要有Kafka、ActiveMQ、RabbitMQ、RocketMQ等. 大数据场景主要采用 Kafka, JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ . ","date":"2022-07-26","objectID":"/kafka_1/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"传统消息队列的应用场景 缓存/消峰: 有助于控制和优化数据流经过系统的速度, 解决生产消息和消费消息的处理速度不一致的情况. 解耦: 允许独立的扩展或修改两边的处理过程, 只要确保它们遵守同样的接口约束. 异步通信: 允许用户把一个消息放入队列, 但并不立即处理它, 然后在需要的时候再去处理它们. ","date":"2022-07-26","objectID":"/kafka_1/:2:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列的两种模式 点对点模式: 消费者主动拉取数据, 消息收到后清除消息 发布/订阅模式: 可以有多个 topic(主题) (浏览、点赞、收藏、评论等) 消费者消费数据之后, 不能删除数据 每个消费者相互独立, 都可以消费到数据 ","date":"2022-07-26","objectID":"/kafka_1/:2:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"Kafka基础框架 Kafka基础框架 Producer: 消息生产者, 向 Kafka broker 发消息的客户端. Consumer: 消息消费者, 向 Kafka broker 取消息的客户端. Consumer Group(CG): 消费者组, 由多个 consumer 组成. 消费者组内每个消费者负责消费不同分区的数据, 一个分区只能由一个组内消费者消费; 消费者组之间互不影响. 所有的消费者都属于某个消费者组, 即消费者组是逻辑上的一个订阅者. Broker: 一台 Kafka 服务器就是一个 broker. 一个集群由多个 broker 组成. 一个 broker 可以容纳多个topic. Topic: 可以理解为一个队列, 生产者和消费者面向的都是一个 topic. Partition: 为了实现扩展性, 一个非常大的 topic 可以分布到多个 broker(即服务器) 上, 一个 topic 可以分为多个 partition, 每个 partition 是一个有序的队列. Replica: 副本. 一个 topic 的每个分区都有若干个副本, 一个 Leader 和若干个 Follower. Leader: 每个分区多个副本的\"主\", 生产者发送数据的对象, 以及消费者消费数据的对象都是Leader. Follower: 每个分区多个副本中的\"从\", 实时从 Leader 中同步数据, 保持和 Leader 数据的同步. Leader 发生故障时, 某个 Follower 会成为新的 Leader. ","date":"2022-07-26","objectID":"/kafka_1/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["canal"],"content":"Canal概述 ","date":"2022-07-26","objectID":"/canal/:1:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的起源 阿里巴巴 B2B 公司, 因为业务的特性, 卖家主要集中在国内, 买家主要集中在国外, 所以衍生出了同步杭州和美国异地机房的需求, 从2010年开始, 阿里系公司开始逐步的尝试基于数据库的日志解析, 获取增量变更进行同步, 由此衍生出了增量 订阅\u0026消费 的业务. Canal 是用 Java 开发的基于数据库增量日志解析, 提供增量数据 订阅\u0026消费 的中间件. 目前, Canal 主要支持了 MySQL 的 Binlog 解析, 解析完成后才利用 Canal Client 来处理获得的相关数据. (数据库同步需要阿里的 Otter 中间件, 基于 Canal ). ","date":"2022-07-26","objectID":"/canal/:1:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的Binlog 什么是Binlog MySQL 的二进制日志可以说是 MySQL 最重要的日志了, 它记录了所有的 DDL 和 DML (除了数据查询语句) 语句, 以事件行驶记录, 还包括语句所执行的消耗的时间. Binlog是事务安全性的. 一般来说, SQL语言分为三类: DML(Data Manipulation Language): 数据操纵语言, 最常用的增删改查就是这类, 操作对象是数据表中的记录 DDL(Data Definition Language): 数据定义语言, 例如建库、建表等 DCL(Data Control Language): 数据控制语言, 如 Grant、Rollback 等, 常见于数据库安全管理 Binlog两个最重要的使用场景: MySQL Replication 在 Master 端开启 Binlog, Master 把它的 Binlog 传递给 Slaves 来达到 Master-Slave 数据一致的目的 数据恢复, 通过使用 MySQL Binlog 工具来使恢复数据 Binlog分类 MySQL Binlog 的格式有三种, 分别是 STATEMENT,MIXED,ROW . 在配置文件中可以选配: binlog_format=statement|mixed|row statement: 语句级, binlog 会记录每次执行写操作的语句. 相对row模式节省空间, 但是可能产生不一致性. 🌰: update table set create_date=now(), 如果用这种模式进行恢复, 由于执行时间的不同产生的数据就可能不同. 优势: 节省空间 劣势: 可能造成数据不一致 row: 行级, binlog 会记录每次操作后每行记录的变化. 优势: 保持数据的绝对一致性 劣势: 占用空间大 mixed: statement的升级版, 一定程度上解决了因为一些情况而造成的 statement模式 不一致的问题. mixed默认还是statement, 在某些情况下(🌰): 当函数中包含UUID()时; 包含AUTO_INCREMENT字段的表被更新等. 这些情况下会按照row的方式处理. 优势: 节省空间, 同事兼顾了一定的一致性 劣势: 还有一些极个别情况依旧会造成不一致, 另外 statement 和 mixed 对于需要对 binlog 的监控情况都不方便 综上, Canl想做监控分析, 选择 row 格式比较合适 ","date":"2022-07-26","objectID":"/canal/:1:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的工作原理 MySQL主从复制过程 Master 主库将改变记录, 写到 binlog 中; Slave 从库向 MySQL Slave 发送 dump 协议, 将 Master 主库的 binlog events 拷贝到它的中继日志(relay log); Slave 从库读取并重做中继日志中的事件, 将改变的数据同步到自己的数据库. MySQL主从复制过程 Canal的工作原理 理解了上面的过程, Canal的原理就很简单, 就是把自己伪装成Slave, 假装从 Master 复制数据 ","date":"2022-07-26","objectID":"/canal/:1:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的准备 MySQL的安装这里就不说了, 网上有很多. 首先, 建数据库: canal_test 然后建表: create table user_info { `id` int, `name` varchar(255), `gender` varchar(255) } 修改配置文件开启binlog [root@hadoop102 ~]# vi /etc/my.cnf # 打开binlog log-bin=mysql-bin # 选择ROW模式 binlog-format=row # 配置MYSQL replaction需要定义, 不要和canal的slaveId重复 server_id=1 binlog-do-db=canal_test binlog-do-db根据实际情况配置, 如果不配置, 则表示所有数据库均开启 binlog. 重启 MySQL sudo systemctl restart mysqld ","date":"2022-07-26","objectID":"/canal/:2:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的下载和安装 ","date":"2022-07-26","objectID":"/canal/:3:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"下载 下载地址: https://github.com/alibaba/canal/releases 下载对应版本: canal.deployer-xxx.tar.gz 解压到对应位置 ","date":"2022-07-26","objectID":"/canal/:3:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 canal.properties [root@hadoop102 conf]# pwd /opt/module/canal/conf [root@hadoop102 conf]# vi canal.properties 这个文件是 canal 的基本通用配置, canal端口号默认是11111. 多实例配置: 一个 canal 服务中可以有多个instance, conf/ 下每一个 example 即是一个实例, 每个实例下面都有独立的配置文件. 需修改canal.destinations=实例1,实例2,实例3 ","date":"2022-07-26","objectID":"/canal/:3:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 instance.properties [root@hadoop102 example]# pwd /opt/module/canal/conf/example [root@hadoop102 example]# vi instance.properties ## mysql serverId , v1.0.26+ will autoGen ## v1.0.26版本后会自动生成slaveId，所以可以不用配置 # canal.instance.mysql.slaveId=0 # 数据库地址 canal.instance.master.address=127.0.0.1:3306 # binlog日志名称 canal.instance.master.journal.name=mysql-bin.000001 # mysql主库链接时起始的binlog偏移量 canal.instance.master.position=154 # mysql主库链接时起始的binlog的时间戳 canal.instance.master.timestamp= canal.instance.master.gtid= # username/password # 在MySQL服务器授权的账号密码 canal.instance.dbUsername=canal canal.instance.dbPassword=canal # 字符集 canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false # table regex .*\\\\..*表示监听所有表 也可以写具体的表名，用，隔开 canal.instance.filter.regex=.*\\\\..* # mysql 数据解析表的黑名单，多个表用，隔开 canal.instance.filter.black.regex= ","date":"2022-07-26","objectID":"/canal/:3:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"实时监控测试 ","date":"2022-07-26","objectID":"/canal/:4:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"TCP模式测试 创建一个 maven 项目, 在pom.xml中配置: \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba.otter\u003c/groupId\u003e \u003cartifactId\u003ecanal.client\u003c/artifactId\u003e \u003cversion\u003e1.1.2\u003c/version\u003e \u003c/dependency\u003e `` 创建类: CanalClient package org.mustard.app; import com.alibaba.fastjson.JSONObject; import com.alibaba.otter.canal.client.CanalConnector; import com.alibaba.otter.canal.client.CanalConnectors; import com.alibaba.otter.canal.protocol.CanalEntry; import com.alibaba.otter.canal.protocol.Message; import com.google.protobuf.ByteString; import com.google.protobuf.InvalidProtocolBufferException; import java.net.InetSocketAddress; import java.util.List; public class CanalClient { public static void main(String[] args) throws InvalidProtocolBufferException { // 获取连接对象 CanalConnector canalConnector = CanalConnectors.newSingleConnector(new InetSocketAddress(\"hadoop102\", 11111), \"example\", \"\", \"\"); // 获取连接 canalConnector.connect(); // 指定要监控的数据库 canalConnector.subscribe(\"canal.*\"); long idx = 0; while (true) { // 获取message Message msg = canalConnector.get(100); List\u003cCanalEntry.Entry\u003e entries = msg.getEntries(); if (entries.size() \u003c= 0) { System.out.println((++idx) + \". 没有数据, 等一会儿\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } else { for (CanalEntry.Entry entry : entries) { // 获取表名 String tableName = entry.getHeader().getTableName(); // Entry类型 CanalEntry.EntryType entryType = entry.getEntryType(); if (CanalEntry.EntryType.ROWDATA.equals(entryType)) { // 序列化数据 ByteString storeValue = entry.getStoreValue(); // 反序列化 CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(storeValue); // 获取事件类型 CanalEntry.EventType eventType = rowChange.getEventType(); // 获取具体数据 List\u003cCanalEntry.RowData\u003e rowDatasList = rowChange.getRowDatasList(); // 遍历打印 for (CanalEntry.RowData rowData : rowDatasList) { List\u003cCanalEntry.Column\u003e beforeColumnsList = rowData.getBeforeColumnsList(); JSONObject beforeData = new JSONObject(); for (CanalEntry.Column column : beforeColumnsList) { beforeData.put(column.getName(), column.getValue()); } List\u003cCanalEntry.Column\u003e afterColumnsList = rowData.getAfterColumnsList(); JSONObject afterData = new JSONObject(); for (CanalEntry.Column column : afterColumnsList) { afterData.put(column.getName(), column.getValue()); } System.out.println(\"TableName: \" + tableName + \", EventType: \" + eventType + \", Before: \" + beforeData + \", After: \" + afterData); } } } } } } } ","date":"2022-07-26","objectID":"/canal/:4:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Kafka模式测试 修改 canal.properties 中 canal 的输出 model ######### common argument ############# canal.id = 1 canal.ip = canal.port = 11111 canal.metrics.pull.po rt = 11112 canal.zkServers = # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = false # tcp, kafka, RocketMQ canal.serverMode = kafka # flush meta cursor/parse position to file 修改 kafka 集群的地址 ######### Kafka ############# kafka.bootstrap.servers = hadoop102:9092 修改 instance.properties 输出到 Kafka 的主题以及分区数 # mq config canal.mq.topic=canal_test 默认还是输出到指定 Kafka 主题的一个分区, 因为多个分区并行可能会打乱 binlog 的顺序, 如果要提高并行度, 首先设置 kafka 的分区数 \u003e 1, 然后设置canal.mq.partitionHash属性. 启动 canal 然后测试: [root@hadoop102 kafka_2.12-2.8.1]# ./bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic canal_test {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589672000,\"id\":2,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589672852,\"type\":\"DELETE\"} {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589697000,\"id\":3,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589697570,\"type\":\"INSERT\"} ","date":"2022-07-26","objectID":"/canal/:4:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["scala"],"content":" Scala 运算符的使用和 Java 的基本相同, 只有个别细节上不同 ","date":"2022-07-14","objectID":"/scala_3/:0:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"算数运算符 运算符 运算 🌰 结果 + 正号 +3 3 - 负号 b=4;-b -4 + 加 5+5 10 - 减 6-4 2 * 乘 3*4 12 / 除 5/5 1 % 取模(取余) 7%5 2 + 字符串相加 \"Must\"+\"ard\" “Mustard” ","date":"2022-07-14","objectID":"/scala_3/:1:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"关系运算符(比较运算符) 运算符 运算 🌰 结果 == 相等于 4==3 false != 不等于 4!=3 true \u003c 小于 4\u003c3 false \u003e 大于 4\u003e3 true \u003c= 小于等于 4\u003c=3 false \u003e= 大于等于 4\u003e=3 true Java 和 Scala 中关于==的区别 Java: ==比较两个变量本身的值, 即两个对象在内存中的首地址 equals比较字符串中包含的内容是否相同 Scala: ==更类似于 Java 中的equals def main(args: Array[String]): Unit = { val s1 = \"abc\" val s2 = new String(\"abc\") // true println(s1 == s2) // false println(s1.eq(s2)) } ","date":"2022-07-14","objectID":"/scala_3/:2:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"逻辑运算符 运算符 描述 实例 (A=true, B=false) \u0026\u0026 逻辑与 A \u0026\u0026 B = false ` ` ! 逻辑非 !(A \u0026\u0026 B) = true ","date":"2022-07-14","objectID":"/scala_3/:3:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"位运算符 运算符 描述 实例 (a=60, b=13) =\u003e (a: 0011 1100, b: 0000 1101) \u0026 按位与 a \u0026 b = 12 =\u003e 0000 1100 ` ` 按位或 ^ 按位异或 a ^ b = 49 =\u003e 0011 0001 ~ 按位取反 ~a = -61 =\u003e 1100 0011 \u003c\u003c 左移 a \u003c\u003c 2 = 240 =\u003e 0011 0000 \u003e\u003e 右移 a \u003e\u003e 2 = 15 =\u003e 0000 1111 \u003e\u003e\u003e 无符号右移 a \u003e\u003e\u003e 2 = 15 =\u003e 0000 1111 ","date":"2022-07-14","objectID":"/scala_3/:4:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"赋值运算符 Scala中的赋值运算符就是 算数运算符/位运算符+= Scala 中没有++, --操作, 这点Swift和这里是相同的, 都是因为觉得这两个运算符并不符合面向对象的思想. 需要通过+=和-=来实现. ","date":"2022-07-14","objectID":"/scala_3/:5:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"Scala 运算符的本质 看源码可以知道, Scala 中其实是没有运算符的, 所有的运算符都是方法 object TestOpt { def main(args: Array[String]): Unit = { // 标准的加法运算 val i: Int = 1.+(1) // 1. 当调用对象的方法时, `.`可以省略 val j: Int = 1 + (1) // 2. 如果函数参数只有一个, 或者没有参数, `()`可以省略 val k: Int = 1 + 1 println(1.toString()) println(1 toString()) println(1 toString) } } ","date":"2022-07-14","objectID":"/scala_3/:6:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["big data"],"content":" 官方文档: 传送门 本文档针对 SeaTunnel v2.1.2 编写 ","date":"2022-07-12","objectID":"/seatunnel_use/:0:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel是什么 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"概述 先来看一下官方文档上是怎么说的: SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data. It can synchronize tens of billions of data stably and efficiently every day, and has been used in the production of nearly 100 companies. 翻译一下: SeaTunnel 是一个使用起来非常简单, 性能非常高效的分布式数据集成平台. 它支持海量数据的实时同步. 它可以每天稳定高效的同步数百亿的数据, 并且已经用于近百个公司的生产中. 可以从上面提炼出几个关键词: very easy-to-use: 使用非常简单. 其实 SeaTunnel 并不是对 Flink 或是 Spark 或是以后支持的其他技术的二次开发, 而是在其之上封装了一层, 使得这些技术使用起来会更简便. ultra-high-performance: 超高性能. real-time synchronization of massive data: 海量数据的实时同步. 以下是从官方文档摘抄: ","date":"2022-07-12","objectID":"/seatunnel_use/:1:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"为什么我们需要 SeaTunnel SeaTunnel 尽所能为您解决海量数据同步中可能遇到的问题： 数据丢失与重复 任务堆积与延迟 吞吐量低 应用到生产环境周期长 缺少应用运行状态监控 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel 使用场景 海量数据同步 海量数据集成 海量数据的 ETL 海量数据聚合 多源数据处理 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境依赖 组件 版本 备注 Java \u003e=8 Flink 1.13 因为当前版本SeaTunnel还未适配Flink1.14, 所以使用Flink1.13版本进行文档编写 Spark 2.x 如果是集群方式的话, Yarn/Standalone 都是支持的 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境配置 config目录下seatunnel-env.sh中可以配置Spark和Flink的环境 # Home directory of spark distribution. SPARK_HOME=${SPARK_HOME:-/opt/spark} # Home directory of flink distribution. FLINK_HOME=${FLINK_HOME:-/opt/flink} :-代表: 若未找到之前的地址, 则用之后的地址 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"官方示例 首先来进行一个官方示例简单的运行一下. 在config目录下创建文件example.conf: # 配置 Spark 或 Flink 的参数 env { # You can set flink configuration here execution.parallelism = 1 #execution.checkpoint.interval = 10000 #execution.checkpoint.data-uri = \"hdfs://hadoop102:9092/checkpoint\" } # 在 source 所属的块中配置数据源 # 默认端口: 9999 source { SocketStream{ host = hadoop102 result_table_name = \"fake\" field_name = \"info\" } } # 在 transform 的块中声明转换插件 # 这里需要说明的是: Split是不会立即生效的, 只有当sql插件中的sql语句中调用了split函数才会真正的作用在数据上 transform { Split{ separator = \"#\" fields = [\"name\",\"age\"] } sql { sql = \"select info, split(info) as info_row from fake\" } } # 在 sink 块中声明要输出到哪 sink { ConsoleSink {} } 然后cd到seatunnel目录在shell中执行: ./bin/start-seatunnel-flink.sh --config config/example.conf 用nc -lk 9999模拟一下socket连接 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"sql执行顺序 在source块中, 利用SocketStream插件读取出数据, 命名为fake表, 字段名为info 拿到info字段, 利用Split插件进行切分 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"启动命令 ./bin/start-seatunnel-flink.sh -h Usage: start-seatunnel-flink.sh [options] Options: -t, --check check config (default: false) * -c, --config Config file -h, --help Show the usage message -r, --run-mode job run mode, run or run-application (default: RUN) (values: [RUN, APPLICATION_RUN]) -i, --variable variable substitution, such as -i city=beijing, or -i date=20190318 (default: []) 其中, --config是必填参数 -i 当其中上面的sql改为: sql = \"select * from (select info, split(info) as info_row from fake) where age \u003e \"${age}\"\" 启动命令改为: ./bin/start-seatunnel-flink.sh --config config/example02.conf -i age=18 -r 执行 flink 自带的命令参数, 可以cd到 flink 下面 -\u003e ./bin/flink run -h查看 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"应用配置的4个基本组件 一个完整的SeaTunnel配置文件应包含四个配置组件: env{}` `source{}` --\u003e `transform{}` --\u003e `sink{} ","date":"2022-07-12","objectID":"/seatunnel_use/:4:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"env块 env块中可以直接写 spark 或 flink 支持的配置项. 比如并行度, 检查点时间, 检查点 hdfs 路径等. 以 Flink 为例, 在 SeaTunnel 源码的ConfigKeyName类中声明了所有可用的key: package org.apache.seatunnel.flink.util; public class ConfigKeyName { private ConfigKeyName() { throw new IllegalStateException(\"Utility class\"); } public static final String TIME_CHARACTERISTIC = \"execution.time-characteristic\"; public static final String BUFFER_TIMEOUT_MILLIS = \"execution.buffer.timeout\"; public static final String PARALLELISM = \"execution.parallelism\"; public static final String MAX_PARALLELISM = \"execution.max-parallelism\"; public static final String CHECKPOINT_INTERVAL = \"execution.checkpoint.interval\"; public static final String CHECKPOINT_MODE = \"execution.checkpoint.mode\"; public static final String CHECKPOINT_TIMEOUT = \"execution.checkpoint.timeout\"; public static final String CHECKPOINT_DATA_URI = \"execution.checkpoint.data-uri\"; public static final String MAX_CONCURRENT_CHECKPOINTS = \"execution.max-concurrent-checkpoints\"; public static final String CHECKPOINT_CLEANUP_MODE = \"execution.checkpoint.cleanup-mode\"; public static final String MIN_PAUSE_BETWEEN_CHECKPOINTS = \"execution.checkpoint.min-pause\"; public static final String FAIL_ON_CHECKPOINTING_ERRORS = \"execution.checkpoint.fail-on-error\"; public static final String RESTART_STRATEGY = \"execution.restart.strategy\"; public static final String RESTART_ATTEMPTS = \"execution.restart.attempts\"; public static final String RESTART_DELAY_BETWEEN_ATTEMPTS = \"execution.restart.delayBetweenAttempts\"; public static final String RESTART_FAILURE_INTERVAL = \"execution.restart.failureInterval\"; public static final String RESTART_FAILURE_RATE = \"execution.restart.failureRate\"; public static final String RESTART_DELAY_INTERVAL = \"execution.restart.delayInterval\"; public static final String MAX_STATE_RETENTION_TIME = \"execution.query.state.max-retention\"; public static final String MIN_STATE_RETENTION_TIME = \"execution.query.state.min-retention\"; public static final String STATE_BACKEND = \"execution.state.backend\"; public static final String PLANNER = \"execution.planner\"; } ","date":"2022-07-12","objectID":"/seatunnel_use/:4:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Row 在说明source块, transform块和sink块之前, 需要先了解一下 SeaTunnel 中的核心数据结构: Row Row 是 SeaTunnel 中数据传递的核心数据结构. 对来 Flink 说, source 插件需要给下游的转换插件返回一个 DataStream\u003cRow\u003e, 转换插件接到上游的 DataStream\u003cRow\u003e进行处理后需要再给下游返回一个 DataStream\u003cRow\u003e. 最后 Sink 插件将转换插件处理好的DataStream\u003cRow\u003e输出到外部的数据系统. 因为 DataStream可以很方便地和 Table 进行互转, 所以将 Row 当作核心数据结构可以让转换插件同时具有使用代码 (命令式) 和 sql (声明式) 处理数据的能力. 可以看一下上面示例中, 读取数据的源码: package org.apache.seatunnel.flink.socket.source; import ... @AutoService(BaseFlinkSource.class) public class SocketStream implements FlinkStreamSource { ... @Override public DataStream\u003cRow\u003e getData(FlinkEnvironment env) { final StreamExecutionEnvironment environment = env.getStreamExecutionEnvironment(); return environment.socketTextStream(host, port) .map((MapFunction\u003cString, Row\u003e) value -\u003e { Row row = new Row(1); row.setField(0, value); return row; }).returns(new RowTypeInfo(Types.STRING())); } } 感兴趣的话, 也可以看到源码中的 Split, sql, sink 都是用DataStream\u003cRow\u003e进行数据传递的 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"source块 source{}是可以配置多个 source 插件的 # 伪代码 env { ... } source { hdfs { ... } jdbc { ... } elasticsearch { ... } } transform { sql { sql = \"\"\" select ... from hdfs_table join es_table on hdfs_table.uid = es_table.uid where ...\"\"\" } } sink { elasticsearch { ... } } 需要注意的是: 所有的 source 插件中都可以声明result_table_name. 如果声明了result_table_name. SeaTunnel 会将 source 插件输出的DataStream\u003cRow\u003e转换为 Table 并注册在 Table 环境中. 当指定了result_table_name那么还可以指定field_name, 在注册时, 给 Table重设字段名. 因为每个 source 所需要的配置是不一致的 (result_table_name和field_name为共有非必填参数), 所以配置的时候查找官方文档会好一点 当前支持的source source 支持Spark 支持Flink 备注 文档地址 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ❌ 传送门 Fake ✔️ ✔️ 改类型主要是用于方便生成指定的数据, 用作 SeaTunnel 的功能验证, 测试和性能测试. 传送门 Feishu Sheet ✔️ ❌ 传送门 File ✔️ ✔️ 从本地或者 hdfs 中读取. 传送门 HBase ✔️ ❌ 传送门 Hive ✔️ ❌ 传送门 Http ✔️ ✔️ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDb ❌ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ Kafka 版本 \u003e= 0.10.0, 目前源码中发现 Schema 的解析有问题(原因为社区把 fastjson 换成 Jackon 引起的). 传送门 Kudu ✔️ ❌ 兼容 Kerberos 认证 传送门 MongoDb ✔️ ❌ 传送门 Neo4j ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 Socket ✔️ ✔️ 传送门 Tidb ✔️ ❌ 传送门 WebhookStream ✔️ ❌ 提供 http 接口推送数据, 仅支持 POST 请求 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"transform块 目前社区对 transform 插件做了很多规划, 但截至 v2.1.2 版本, 可用的插件有3个: Split, Sql和Json. 其中Json只适配 Spark 可用. transform{}中可以声明多个转换插件. 所有的转换插件都可以使用source_table_name, 和result_table_name. 同样, 如果声明了result_table_name, 那么就能声明field_name. Split插件 这里着重说一下 Split 插件: @Override public DataSet\u003cRow\u003e processBatch(FlinkEnvironment env, DataSet\u003cRow\u003e data) { return data; } @Override public DataStream\u003cRow\u003e processStream(FlinkEnvironment env, DataStream\u003cRow\u003e dataStream) { return dataStream; } @Override public void registerFunction(FlinkEnvironment flinkEnvironment) { if (flinkEnvironment.isStreaming()) { flinkEnvironment .getStreamTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } else { flinkEnvironment .getBatchTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } } 从源码中可以发现 Split 插件并没有对数据流进行任何的处理, 而是将它直接return了. 反之, 它向表环境中注册了一个名为 split 的 UDF(用户自定义函数). 而且, 函数名是写死的. 这意味着, 如果声明了多个 Split 后面的 UDF 还会把前面的覆盖. 从 Split 插件中就能看出了, 这个插件其实是通过注册方法的方式来调用的. 但是, transform 接口其实是预留了直接操作数据的能力的 (比如 Sql 插件中的处理方式), 也就是processStream方法. 那么, 一个 transform 插件其实同时履行了 process 和 UDF 的职责, 这是违背单一职责原则的. 所以要判断一个 transform 插件在做什么就只能从源码和文档的方面来加以区分了. Sql 插件 sql插件中需要特别说明的是, 指定source_table_name对于 sql 插件的意义不大, 因为可以通过from子句来决定从哪个表里抽取数据. ","date":"2022-07-12","objectID":"/seatunnel_use/:4:4","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Sink块 sink块里可以声明多个 sink 插件, 每个 sink 插件都可以指定source_table_name. 当前支持的sink source 支持Spark 支持Flink 备注 文档地址 Clickhouse ✔️ ✔️ 使用 Clickhouse-jdbc 根据字段名称对应数据源, 并将其写入. 使用前需创建对应的数据表. 传送门 ClickhouseFile ✔️ ✔️ 通过 clickhouse-local 程序生成 Clickhouse 数据文件, 然后将其发送到 Clickhouse 服务器, 也称为是 bulk load (批量加载). 传送门 Console ✔️ ✔️ 将数据输出到标准终端或 Flink 的 TaskManager. 通常用于调试和易于观察的数据. 传送门 Doris ✔️ ✔️ 传送门 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ✔️ Spark 支持的 Elasticsearch \u003e= 2.0 并且 \u003c 7.0.0; Flink 支持的 Elasticsearch = 7.x, 如果要用 Elasticsearch 6.x, 需用源码执行命令 mvn clean package -Delasticsearch=6重新打包. 传送门 Email ✔️ ❌ 支持通过 email 附件输出数据. 传送门 File ✔️ ✔️ 传送门 Hbase ✔️ ✔️ 使用 hbase-connectors 将数据输出到 Hbase(\u003e=2.1.0) 和 Spark(\u003e=2.0.0) 版本兼容性取决于 hbase-connectors. 传送门 Hive ✔️ ❌ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDB ❌️ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ 传送门 Kudu ✔️ ❌ 传送门 MongoDB ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 TiDb ✔️ ❌️ 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:5","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["Scala"],"content":"注释 Scala 的注释和 Java 的完全一样: // 1. 单行注释 /* 2. 多行注释 */ /** * 3. 文档注释 */ ","date":"2022-07-07","objectID":"/scala_2/:1:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"变量和常量 ","date":"2022-07-07","objectID":"/scala_2/:2:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 // var 变量名 [: 变量类型] = 初始值 var i: Int = 10 // val 常量名 [: 常量类型] = 初始值 val j: Int = 20 能用常量的地方就不要用变量 声明变(常)量时, 类型可以省略, 编译器自动推导, 即类型推导 类型确定后, 就不能更改, 因为 Scala 是强数据类型语音 变量声明时, 必须有初始值 ","date":"2022-07-07","objectID":"/scala_2/:2:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"标识符的命名规范 ","date":"2022-07-07","objectID":"/scala_2/:3:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"命名规范 Scala 中的标识符声明, 基本和Java是一致的, 但是细节上会有变化: 以字母或下划线开头, 后接字母、数字、下划线 以操作符开头, 且只包含操作符(+-*/#!等) 用反引号`…`包括的任意字符串, 即使是 Scala 关键字(39个)也可以 package, import, class, object, trait, extends, with, type, forSom private, protected, abstract, sealed, final, implicit, lazy, override try, catch, finally, throw if, else, match, case, do, while, for, return, yield def, var, val this, super new true, false, null 看几个特殊一点的🌰 object Hello { def main(args: Array[String]): Unit = { // ok 因为在 Scala 中 Int 是预定义的字符, 不是关键字, 但是不推荐 var Int: String = \"\" // ok 单独一个下划线不可作为标识符, 因为 _ 被认为是一个方法 var _: String = \"str\" // 会报错 println(_) // ok var -+*/#! : String = \"\" // error 以操作符开否, 必都是操作符 var -_*/#!1 : String = \"\" // error var if: String = \"\" // ok var `if`: String = \"\" } } ","date":"2022-07-07","objectID":"/scala_2/:3:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"字符串输出 ","date":"2022-07-07","objectID":"/scala_2/:4:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 字符串, 通过+号连接 printf用法: 字符串，通过%传值。 字符串模板(插值字符串): 通过$获取变量值 来看看🌰: object Hello { def main(args: Array[String]): Unit = { val name = \"mustard\" val age = 18 printf(\"name=%s, age=%d\", name, age) /** * 多行字符串， 在 Scala 中，利用三个双引号包围多行字符串就可以实现。 * 输入的内容，带有空格、 \\t 之类，导致每一行的开始位置不能整洁对齐。 * 应用 scala 的 stripMargin 方法，在 scala 中 stripMargin 默认是 \"|\" 作为连接符， * 在多行换行的行头前面加一个 \"|\" 符号即可。 */ val sql1 = \"\"\" |select | name | age |from user |where name = \"mustard\" \"\"\".stripMargin println(sql1) // 如果需要对变量进行运算，那么可以加 ${} val sql2 = s\"\"\" |select | name | age |from user |where name = \"$name\" and age = ${age + 2} \"\"\".stripMargin println(sql2) val s = s\"name=$name\" println(s) } } ","date":"2022-07-07","objectID":"/scala_2/:4:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"数据类型 ","date":"2022-07-07","objectID":"/scala_2/:5:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Java数据类型 先来回顾一下 Java 的数据类型: 基本数据类型(8种): char, byte, short, int, long, float, double, bollean 引用类型: (对象类型) 由于 Java 有基本类型, 并且基本类型并不是真正意义的对象, 所以 Java 语言并不是真正意义的面向对象. 注意哈: Java 中基本类型和引用类型没有共同的祖先 ","date":"2022-07-07","objectID":"/scala_2/:5:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Scala数据类型 Scala 中一切都是对象, 都是Any的子类 Scala 中数据类型分为两大类: 数值类型 (AnyVal), 引用类型 (AnyRef), 但是都是对象 Scala 数据类型仍然遵守隐式转换 (低精度的值向高精度的值自动转换) Scala 中的StringOps是对 Java 中的String增强 Unit: 对应 Java 中的void, 用于方法返回值的位置, 表示方法没有返回值. Unit是一个数据类型, 只有一个对象就是(). Void不是数据类型, 只是一个关键字. Null是一个类型, 只有一个对象就是null. 它是所有引用类型(AnyRef)的子类. Nothing: 是所有数据类型的子类, 主要用在一个函数没有明确返回值时使用, 因为这样可以把抛出的返回值, 返回给任何的变量或者函数. 整数类型 数据类型 描述 Byte[1] 8位有符号补码整数. 数值区间: -128 到 127 Short[2] 16位有符号补码整数. 数值区间: -32768 到 32767 Int[4] 32位有符号补码整数. 数值区间: -2147483648 到 2147483647 Long[8] 64位有符号补码整数. 数值区间: -2^64 到 2^64-1 Scala 的整型，默认为Int型，声明Long型，须后加l或L 浮点类型 数据类型 描述 FLoat[4] 32位 Double[8] 64位 object TestDataType { def main(args: Array[String]): Unit = { // 这是个 Float var n1 = 1.23456789f // 这是个 Double var n2 = 1.23456789 } } 字符类型 (Char) 布尔类型 (Boolean) 占 1 个字节 Unit 类型、Null 类型和 Nothing 类型 数据类型 描述 Unit 表示无值, 和其他语言中void等同. 用作不返回任何结果的方法的结果类型. Unit只有一个实例: (), 且没有实际意义 Null null, Null类型只有一个实例: null. Null可以赋值给任意引用类型(AnyRef), 但是不能赋值给值类型(AnyVal) Nothing Nothing类型在 Scala 的类层级最低端; 它是任何其他类型的子类型. 当一个函数, 确定没有正常的返回值时, 可以用Nothing来指定返回类型. Nothing的这种机制有一个好处: 可以把返回的值(异常)赋给其它的函数或者变量(兼容性) object TestDataType { def main(args: Array[String]): Unit = { var cat = new Cat() // 正确 cat = null // 错误 var n1: Int = null def test(): Nothing = { throw new Exception() } test } } 类型转换 数值类型自动转换: 精度小的类型自动转换为精度大的数值类型. (隐式转换) Byte \u003c Short \u003c Int \u003c Long \u003c Float \u003c Double 1. 自动提升原则: 有多种类型的数据混合运算时, 首先自动将所有数据转换成精度大的数据类型, 然后再计算 2. 把精度大的数值类型赋值给精度小的数值类型时, 会报错 3. (byte, short)和char之间不会相互自动转换 4. byte,short,char三者可以计算, 但会先转换为int类型 强制类型转换 可能造成精度降低或溢出, 所以需要特别注意 var num: Int = 2.7.toInt 数值类型和String类型间的转换 数值 -\u003e String: + \"\"就行, 和 Java 一样 String -\u003e 数值: s1.toInt, s1.toFloat, s1.toDouble, s1.toByte, s1.toLong, s1.toShort ","date":"2022-07-07","objectID":"/scala_2/:5:2","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["scala"],"content":" Scala这门语言是怎么发展过来的, 网上有很多资料, 这里就不赘述了. ","date":"2022-07-06","objectID":"/scala_1/:0:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"Scala和Java的关系 一般来说, 学习 Scala 之前都会或多或少的接触过 Java, 而 Scala 是基于 Java 的, 因此在学习 Scala 之前, 要先弄清楚 Java, Scala 和 JVM 的关系是很有用的. Java, Scala和JVM的关系 ","date":"2022-07-06","objectID":"/scala_1/:1:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置Scala环境 以 Windows 为例 首先确保 JDK1.8 安装成功 下载对应的 Scala 安装文件: 传送门 (我下载的是zip) 解压 zip 配置 Scala 的环境变量 配置 SCALA_HOME 在 Path 中添加 ","date":"2022-07-06","objectID":"/scala_1/:2:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置IDEA 安装插件 Scala 创建项目之后, 右键项目目录 -\u003e Add Framework Support -\u003e 选择 Scala ","date":"2022-07-06","objectID":"/scala_1/:3:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["big data"],"content":"数据仓库与数据湖的区别 说到湖仓一体, 就要先了解一下数据仓库和数据湖的区别是什么. 下面这个表格就是AWS(传送门)上的对比: 特性 数据仓库 数据湖 数据 来自事务系统、运营数据和业务线应用程序的关系数据 来自IoT设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据 Schema 设计数据仓库实施之前(写入型Schema) 写入在分析时(读取型Schema) 性价比 更快查询结果会带来较高存储成本 更快查询结果只需较低存储成本 数据质量 可作为重要事实依据的高度监管数据 任何可以或无法进行监管的数据(例如原始数据) 用户 业务分析师 数据科学家、数据开发人员和业务分析师(使用监管数据) 分析 批处理报告、BI和可视化 机器学习、预测分析、数据发现和分析 从上面这个表就能看出来数据仓库和数据湖的差别还是很明显的. 在企业中, 两者的作用是互补的, 所以不应该认为数据湖的出现是为了取代数据仓库, 毕竟两者的作用是截然不同的. ","date":"2022-06-16","objectID":"/data_lakehouse/:1:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体是怎么诞生的 对于数据而言, 数据仓库就像是一个大型图书馆, 里面的数据需要按照规范放好, 可以按照类别找到想要的信息. 而数据湖就像是一个大型仓库, 可以存储任何形式(结构化, 半结构化和非结构化)和任何格式(文本, 图像, 音频, 视频)的原始数据. 在产品的角度上来说, 数据仓库一般是独立标准化产品, 数据湖更像是一种架构指导, 需要配合着系列周边工具来实现业务需要. 也就是说, 数据湖的灵活性对于前期开发和前期部署是友好的; 数据仓库的规范性对于大数据后期的运行和长期发展是友好的. 那有没有一种新架构能兼具数据仓库和数据湖的优点? 然后, 湖仓一体就诞生了. 依据DataBricks公司对Lakehouse 的定义, 湖仓一体是一种结合了数据湖和数据仓库优势的新范式, 在用于数据湖的低成本存储上, 实现与数据仓库中类似的数据结构和数据管理功能. 湖仓一体是一种更开放的新型架构, 有人把它做了一个比喻, 就类似于在湖边搭建了很多小房子, 有的负责数据分析, 有的运转机器学习, 有的来检索音视频等, 至于那些数据源流, 都可以从数据湖里轻松获取. 需要注意的是: 数据湖 + 数据仓库 ≠ 湖仓一体 湖仓一体诞生的目的, 总结起来就是: 打通数据的存储与计算 灵活性与成长性兼得 灵活性与成长性 ","date":"2022-06-16","objectID":"/data_lakehouse/:2:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体的好处 数据重复性：如果一个组织同时维护了一个数据湖和多个数据仓库，这无疑会带来数据冗余。在最好的情况下，这仅仅只会带来数据处理的不高效，但是在最差的情况下，它会导致数据不一致的情况出现。湖仓一体的结合，能够去除数据的重复性，真正做到了唯一。 高存储成本：数据仓库和数据湖都是为了降低数据存储的成本。数据仓库往往是通过降低冗余，以及整合异构的数据源来做到降低成本。而数据湖则往往使用大数据文件系统和Spark在廉价的硬件上存储计算数据。湖仓一体架构的目标就是结合这些技术来最大力度降低成本。 报表和分析应用之间的差异：数据科学倾向于与数据湖打交道，使用各种分析技术来处理未经加工的数据。而报表分析师们则倾向于使用整合后的数据，比如数据仓库或是数据集市。而在一个组织内，往往这两个团队之间没有太多的交集，但实际上他们之间的工作又有一定的重复和矛盾。而当使用湖仓一体架构后，两个团队可以在同一数据架构上进行工作，避免不必要的重复。 数据停滞：在数据湖中，数据停滞是一个最为严重的问题，如果数据一直无人治理，那将很快变为数据沼泽。我们往往轻易的将数据丢入湖中，但缺乏有效的治理，长此以往，数据的时效性变得越来越难追溯。湖仓一体的引入，对于海量数据进行治理，能够更有效地帮助提升分析数据的时效性。 潜在不兼容性带来的风险：数据分析仍是一门兴起的技术，新的工具和技术每年仍在不停地出现中。一些技术可能只和数据湖兼容，而另一些则又可能只和数据仓库兼容。湖仓一体的架构意味着为两方面做准备。 ","date":"2022-06-16","objectID":"/data_lakehouse/:3:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"什么是数据湖 看了网上很多的资料, 关于数据湖的定义有很多, 我们先来看看AWS(传送门)的定义: 数据湖是一个集中式存储库，允许您以任意规模存储所有结构化和非结构化数据。您可以按原样存储数据（无需先对数据进行结构化处理），并运行不同类型的分析 – 从控制面板和可视化到大数据处理、实时分析和机器学习，以指导做出更好的决策。 再来看一下Wikipedia(传送门)的定义: 指使用大型二进制对象或文件这样的自然格式储存数据的系统 。它通常把所有的企业数据统一存储，既包括源系统中的原始副本，也包括转换后的数据，比如那些用于报表, 可视化, 数据分析和机器学习的数据。数据湖可以包括关系数据库的结构化数据(行与列)、半结构化的数据(CSV，日志，XML, JSON)，非结构化数据 (电子邮件、文件、PDF)和 二进制数据(图像、音频、视频)。 比较统一的一点是数据湖存储的是未经加工的原始数据, 包含结构化(如关系型数据库中的表)、半结构化(如CSV、日志、XML、JSON)和非结构化(如电子邮件、文档、PDF)的各类数据. 因为是原始数据, 所以也就保持着数据在业务系统中原来的样子. 这就是使得数据湖一定要具备完善的管理能力, 也就是要有完善的元数据, 可以管理各类数据相关的要素，包括数据源、数据格式、连接信息、数据schema、权限管理等. 因为数据湖是为了分析数据而演化来的, 也就要存储各类分析处理的中间结果, 并且还要记录完整的分析过程. 数据沼泽是一个劣化的数据湖, 用户无法访问, 或是没什么价值 ","date":"2022-06-15","objectID":"/date_lake/:1:0","tags":["big data","data lake"],"title":"数据湖","uri":"/date_lake/"},{"categories":["big data"],"content":"数据仓库的基本概念 ","date":"2022-06-14","objectID":"/data_warehouse/:1:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"概念 英文名为Data Warehouse, 简写为DW或DWH. 出现的目的是构建面向分析的集成化数据环境, 为企业提供决策支持(Decision Support). 因为分析性报告和决策支持目的而出现的技术. 之所以叫做\"仓库“而不是”工厂“就是因为DW本身是不生产或消费任何数据, 数据来源于外部, 并且开放给外部应用. ","date":"2022-06-14","objectID":"/data_warehouse/:1:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"基本特征 DW是面向主题的, 集成的, 非易失的和时变的数据集合, 用以支持决策. 面向主题(Subject Oriented) 在传统数据库中, 最大的特点就是面向应用进行数据的组织, 各个系统可能是相互分离的, 而数据仓库则是面向主题的. 抽象上来说, 主题就是在较高层次上讲企业信息系统中数据进行综合、归类分析利用. 每一个主题基本对应一个宏观分析领域. 逻辑上来说, 主题是对应企业中某一个宏观分析领域所涉及的分析对象. 集成的(Integrate) 通过对分散、独立、异构的数据库数据进行抽取、清理、转换和汇总便得到了数据仓库的数据， 这样能保证整个企业的数据的一致性, 避免了产生了信息孤岛. 数据仓库中的综合数据不能从原有的数据库系统直接得到. 所以在数据进入到DW之前, 必然要经过统一与综合(抽取和清洗), 这就是DW建设中, 最关键、最复杂的一步. 非易失性(Non-Volatile) 非易失性也可称为稳定性或不可更新性. 数据仓库的数据反映的是相当一段时间内的历史数据的内容, 是不同时点的数据库的快照的集合, 以及基于这些快照进行统计、综合和重组的导出数据. 基于这个特点, DW一般有大量的查询操作, 但修改和删除操作很少. 通常只需要定期的加载和更新. 时变(Time Variant) 数据仓库包含各种粒度的历史数据. 虽然说DW的用户不能修改数据, 但并不是说数据仓库的数据就是永远不变的. 分析的结果只能反映过去的情况, 当业务变化后, 挖掘出的模式会失去时效性. 所以说, DW中的数据需要更新, 以适应决策的需要. DW的数据时限一般要远远长于操作型数据的数据时限. 操作型系统存储的是当前的数据, 而数据仓库中的数据是历史数据. 数据仓库中的数据是按照时间顺序追加的, 它们都带有时间属性. ","date":"2022-06-14","objectID":"/data_warehouse/:1:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库与数据库的区别 这两者的区别其实就是OLAP与OLTP的区别. 联机事务处理OLTP (On-Line Transaction Processing), 也可以叫做面向交易的处理系统. 是针对具体业务在数据库联机的日常操作, 通常对少数记录进行查询、修改. 用户较为关心的是响应时间、数据的安全性、完整性和并发支持的用户数等问题. 例如MySQL, Oracle等关系型数据库一般属于OLTP 联机分析处理OLAP(On-Line Analytical Processing)一般针对某些主题的历史数据进行分析, 支持管理决策. 通过这两个的对比就能发现, 数据仓库的出现并不是为了替代数据库的. 数据库设计是尽量避免冗余, 一般是针对某一业务进行设计的. 而数据仓库在设计时有意引入冗余, 依照分析需求、分析维度、分析指标进行设计. 总的来说数据库是为了捕获数据而设计, 数据存库是为了分析数据而设计. ","date":"2022-06-14","objectID":"/data_warehouse/:2:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层架构 按照数据流入流出的过程, DW架构可分为: 数据运营层、数据仓库层、数据服务层. ","date":"2022-06-14","objectID":"/data_warehouse/:3:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据运营层 (ODS) Operation Data Store 数据准备区, 也叫做贴源层, 源数据. 数据仓库源头系统的数据表通常会原封不动的存储一份, 是后续数据仓库加工数据的来源. 来源方式: 业务库 经常会使用SQOOP来进行抽取, 🌰: 每天定时抽取一次 实时方面, 可以考虑用canal/FlinkCDC监听MySQL的binlog 埋点日志 日志一般是以文件的形式保存, 可以用flume定时同步 可以用spark streaming或Flink实时接入 kafka也可以 消息队列 来自ActiveMQ、Kafka的数据. ","date":"2022-06-14","objectID":"/data_warehouse/:3:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库层 (DW) DW数据分层, 由下到上为DWD, DWB, DWS. DWD:data warehouse details 细节数据层, 是业务层与数据仓库的隔离层. 主要对ODS数据层做一些数据清洗(去除空值、脏数据、超过极限范围的)和规范化的操作. DWB: data warehouse base 数据基础层, 存储的是客观数据, 一般用作中间层, 可以认为是大量指标的数据层. DWS: data warehouse service 数据服务层, 基于DWB上的基础数据, 整合汇总成分析某一个主题域的服务数据层, 一般是宽表(字段多的表). 用于提供后续的业务查询, OLAP, 数据分发等. 用户行为, 轻度聚合 主要对ODS/DWD层数据做一些轻度的汇总 ","date":"2022-06-14","objectID":"/data_warehouse/:3:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据服务层/应用层 (ADS) application data service 应用数据服务, 该层主要是提供数据产品和数据分析使用的数据, 一般会存储在ES、mysql等系统中供线上系统使用 一般会将大宽表, 比如报表数据放在这里 ","date":"2022-06-14","objectID":"/data_warehouse/:3:3","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"ETL 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL, 换句话说就是描述数据从源端经过**抽取(Extra)、转换(Transfer)、加载(Load)**到目的端的过程. ETL就是数据仓库的流水线, 而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定. ","date":"2022-06-14","objectID":"/data_warehouse/:3:4","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层的目的 用空间换时间: 通过大量的预处理来提升应用系统的用户体验(效率), 因此数据仓库会存在大量冗余的数据. 如果不分层的话, 如果源业务系统的业务规则发生变化将会影响整个数据清洗过程, 工作量巨大. 简化数据清洗的过程: 把原来一步的工作分成多个步骤来完成. 每一层的处理逻辑都相对简单和容易理解了, 这样比较容易保证每个步骤的正确性. ","date":"2022-06-14","objectID":"/data_warehouse/:3:5","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库元数据的管理 元数据(Meta data), 一句话就能总结: 关于数据的数据. 数据仓库中的元数据**主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态. 并且是整个数据仓库系统运行的基础, 元数据把数据仓库系统中各个松散的组件联系起来, 组成一个有机的整体. ** ","date":"2022-06-14","objectID":"/data_warehouse/:4:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["Operating System"],"content":"地址空间 之前已经说过了, 地址空间分为两种: 物理地址空间: 硬件支持的地址空间 这部分地址空间的管理和控制是由硬件来完成的. 逻辑地址空间: 一个运行的程序所拥有的内存范围 相对于物理地址空间而言, 程序所能\"看到的\"逻辑地址空间更简单一点, 就是一个一维的线性的地址空间. 但是最终, 逻辑地址和物理地址都是需要对应上的, 这部分就是由OS来完成的. ","date":"2022-03-17","objectID":"/os_address/:1:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"地址生成 用一个C语言的函数来举个🌰 逻辑地址生成\r从最开始的**‘符号逻辑地址’到最终的‘具体逻辑地址’**, 经过了上面的这些转换过程, 而这些过程是基本上不需要操作系统来帮助的, 而是通过应用程序、编译器或者Loader就可以完成. 但是当把这个地址放入到内存中之后也是逻辑地址而不是物理地址. 再把之前的操作系统架构中的图片拿出来: 物理地址生成\rCPU方面: 运算器需要在逻辑地址的内存内容 内存管理单元(MMU)寻找在逻辑地址和物理地址之间的映射 控制器从总线发送在物理地址的内存内容的请求 内存方面: 内存发送物理地址内存的内容给CPU 操作系统方面: 建立逻辑地址和物理地址之间的映射 **操作系统的一个重要的作用就是: 确保放在内存中的程序相互之间不能相互干扰, 每个程序去访问地址空间是合法的. 换而言之, 确保每个程序访问地址空间是在一个范围之内的. ** ","date":"2022-03-17","objectID":"/os_address/:2:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"计算机基本硬件结构 CPU主要是完成对程序的控制 内存主要放置程序的代码和处理的数据 设备 计算机基本硬件结构\r","date":"2022-03-16","objectID":"/os_computerarchitecture/:1:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"内存分层体系 放上这张图(这张图应该很多人都看到过) 内存延时🌰\r","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统需要完成的四个目标 抽象: 逻辑地址空间 我们希望应用程序在内存中运行的时候, 不用考虑很多细节(物理内存地址在什么地方, 外设在什么地方), 只需要访问一个连续的地址空间就可以了, 即逻辑地址空间. 保护: 独立地址空间 因为在内存中运行着多个不同的应用程序. 运行的过程中, 有可能会访问其他应用程序的地址空间并造成破坏, 所以就需要将不同应用程序的运行空间进行隔离. 共享: 访问相同内存 不同的程序之间除了隔离之外, 还会有交互. 所以操作系统提供共享的内存空间来完成. 虚拟化: 更多的地址空间 当内存中的程序过多的时候, 有可能会出现内存不够的情况. 这样, 就会把急需要地址空间的程序放在内存中, 暂时不需要地址空间的程序先放在硬盘上去. ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:1","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统中管理内存的不同方法 程序重定位 分段 分页 虚拟内存 按需分页虚拟内存 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:2","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"实现高度依赖于硬件 必须知道内存架构 MMU(内存管理单元): 硬件组件负责处理CPU的内存访问请求 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:3","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"系统调用, 异常, 中断的特点 ","date":"2022-03-16","objectID":"/os_interface/:1:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"来源 系统调用: 应用程序 异常: 应用程序 中断: 外设 ","date":"2022-03-16","objectID":"/os_interface/:1:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"处理时间 系统调用: 异步或同步 异常: 同步 中断: 异步 ","date":"2022-03-16","objectID":"/os_interface/:1:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"响应 系统调用: 等待和持续 异常: 杀死或重新执行意想不到的应用程序指令 中断: 持续, 对用户应用程序是透明的 ","date":"2022-03-16","objectID":"/os_interface/:1:3","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断和异常的处理机制 中断陈外设的事件 异常是内部CPU的事件 中断和异常迫使CPU访问一些被中断和异常服务访问的功能 ","date":"2022-03-16","objectID":"/os_interface/:2:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断 硬件 设置中断标记**[CPU初始化]** 将内部、外部事件设置中断标记 中断时间的ID 软件 保存当前处理状态: 为了确保之后能从打断的地方能够继续执行 中断服务程序处理 清除中断标记 恢复之前保存的处理状态 ","date":"2022-03-16","objectID":"/os_interface/:2:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"异常 保存现场 异常处理 杀死产生异常的程序 重新执行异常指令 (这种情况下, 对于用户来说, 异常就是透明的) 恢复现场 ","date":"2022-03-16","objectID":"/os_interface/:2:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"系统调用 系统调用和前面的中断和异常从名字上看就不一样(认真脸). 应用程序需要系统提供一些服务, 而这些服务不能由应用程序直接执行, 需要由操作系统来执行, 这个过程就需要接口, 这个接口就称为系统调用接口. 举个🌰, C语言中的printf()就会触发系统调用write(). 程序访问主要是通过高层次的API接口, 而不是直接进行系统调用. Win32 API 用于 Windows POSIX API 用于 POSIX-based systems (包括UNIX, Linux, Mac OS 的所有版本) Java API 用于 JVM 通常, 每个系统调用相关的序号. 系统调用接口根据这些需要来维护表的索引. 系统调用接口调用内核态中预期的系统调用, 并返回系统调用的状态和其他任何返回值 用户不需要知道系统调用是如何实现的, 只需要获取API和了解操作系统将什么作为返回结果. 操作系统接口的细节大部分都隐藏在API中, 通过运行程序支持的库来管理(包括编译器的库来创建函数集). 系统调用和传统的函数调用是有区别的: 当应用程序发出函数调用的时候, 是在一个栈空间完成了参数的传递和参数的返回. 系统调用的执行过程中, 应用程序和OS是有各自的堆栈. 当应用程序发出系统调用的时候, 当切换到内核中执行之后也需要切换堆栈. 同时, 也需要完成特权级的转换(从用户态转为内核态). 也就是说系统调用的开销是比函数调用的开销大得多, 但是这样的做法也提高了安全性. ","date":"2022-03-16","objectID":"/os_interface/:3:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"跨越操作系统边界的开销 通过上面可以了解到, 其实系统调用, 异常和中断就是操作系统和应用程序, 以及操作系统和外设之间跨越了边界. 在执行时间上的开销超过程序调用 开销: 建立中断/异常/系统调用号与对应服务例程映射关系的初始化开销 建立内核堆栈 验证参数, 因为操作系统是不信任应用程序的 内核态映射到用户态的地址空间, 更新页面映射权限. 这是一个拷贝的过程, 因为不能将内核态中的数据简单的用传递指针的方式传递给用户态. 内核态独立地址空间, TLB ","date":"2022-03-16","objectID":"/os_interface/:3:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"首先要知道, OS并没有放在内存当中, 而是放在了DISK中的. 当开机的时候, 首先是由BIOS(Basic Input Output System)检测各种各样的外设, 通过了之后才会去加载OS(由BootLoader完成). BIOS在内存中是有一个固定的地址的, 以x86为例, BIOS是存放在CS:IP = 0xf000:fff0这个地址中的(CS: 段寄存器; IP: 指令寄存器) . BootLoader启动过程\r","date":"2022-03-15","objectID":"/os_bootloader/:0:0","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"操作系统与设备和程序交互 操作系统的Interface包含三个: 系统调用(system call): 来源于应用程序主动向操作系统发出服务请求 异常(exception): 来源于不良的操作程序, 非法指令或者其他坏的处理状态(🌰 内存出错) 中断(interrupt): 来源于外设. 来自不同的硬件设备的计时器和网络的中断 ","date":"2022-03-15","objectID":"/os_bootloader/:0:1","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"那为什么应用程序不能直接去访问外设呢, 而是通过操作系统? 首先从安全的角度, OS是一个特殊的应用软件, 和其他的应用程序最大的不同就是, OS对整个计算机有控制权, 它是可信任的. 如果应用程序直接访问外设的话, 很容易造成整个系统的崩溃. 另一个方面, 希望通过操作系统, 给上层的应用提供简单, 一致的接口, 使得应用程序不用关注底层设备的复杂性和差异性. ","date":"2022-03-15","objectID":"/os_bootloader/:0:2","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Data Structure"],"content":"首先, 看一个🌰. 现在有一个五子棋程序, 其中有一个存盘退出和续上盘的功能. 二维数组记录棋盘\r从上面这张图就能看到, 很多值就是默认值(0), 也就是说记录了很多没有意义的值, 所以就引出了稀疏数组. ","date":"2022-03-07","objectID":"/sparsearray/:0:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"基本介绍 当一个数组中大部分元素都是同一个值的数组时, 可以使用稀疏数组来保存该数组. 稀疏数组的处理方法是: 记录数组一共几行几列, 有多少个不同的值 把有不同值的元素的行列及值记录在一个小规模的数组中, 从而所辖程序的规模 // 一个正常的二维数组, 一共有7行8列, 其中大部分都是0 int[][] array = { {0,1,0,0,0,0,0,0}, {0,0,0,2,0,0,0,0}, {0,0,0,0,0,0,3,0}, {0,-4,0,0,0,-5,0,0}, {0,0,0,0,0,0,0,0}, {0,0,0,0,-6,0,0,0}, {0,0,7,0,0,0,0,0}, } 行(row) 列(column) 值(value) 7 8 7 0 1 1 1 3 2 2 6 3 3 1 -4 3 5 -5 5 4 -6 6 2 7 表中第一行记录了一共几行几列以及多少个非零值 从上面这个🌰就能看出来, 将原本需要7*8=56个空间变为了3*8=24个空间. ","date":"2022-03-07","objectID":"/sparsearray/:1:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"实现思路 二维数组 转 稀疏数组 遍历原始的二维数组, 得到有效数据的个数sum 根据sum就可以创建稀疏数组int[sum+1][3] 将二维数组的有效数据存入到稀疏数组 稀疏数组 转 二维数组 先读取稀疏数组第一行, 根据第一行的数组创建原始二维数组 在读取稀疏数组后面的数据, 并赋值给二维数组 ","date":"2022-03-07","objectID":"/sparsearray/:2:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"线性结构 作为最常用的数据结构, 特点是数据元素之间存在一对一的线性关系 有两种不同的存储结构: 顺序存储结构和练市存储结构. 顺序存储的线性表称为顺序表, 顺序表中的存储元素是连续的. 链式存储的线性表称为链表, 链表中的存储元素不一定是连续的, 元素节点中存放数据元素以及相邻元素的地址信息. 线性结构常见的有: 数据, 队列, 链表和栈. ","date":"2022-03-07","objectID":"/linear_nonlinear/:1:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Data Structure"],"content":"非线性结构 已经不是一对一的关系了, 其结构包括: 二维数组, 多维数组, 广义表, 树结构, 图结构. ","date":"2022-03-07","objectID":"/linear_nonlinear/:2:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Syntactic"],"content":"什么是fail-fast 百度百科上是这么写的: fail-fast是Java集合(Collection)中的一种错误机制. 我觉得不太全面. 下面来看一下维基百科上这怎么写的: In systems design, a fail-fast system is one which immediately reports at its interface any condition that is likely to indicate a failure. Fail-fast systems are usually designed to stop normal operation rather than attempt to continue a possibly flawed process. Such designs often check the system’s state at several points in an operation, so any failures can be detected early. The responsibility of a fail-fast module is detecting errors, then letting the next-highest level of the system handle them. 从上面这段话就能看出来, fail-fast是在系统设计当中的一种错误检测机制, 一旦检测到可能发生错误, 就立即抛出异常, 程序不再继续运行. ","date":"2022-03-06","objectID":"/failfast/:1:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"集合中的fail-fast 首先, 来复现这个错误: List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 运行上面的代码就会抛出这样的异常: Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) 上面的代码是在for-each中要删除集合中的元素而抛出的异常. 同样的, 增加(add())元素也会抛出这个异常. ","date":"2022-03-06","objectID":"/failfast/:2:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"异常产生的原因 来看一下源码: public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // ... } public boolean remove(Object o) { if (o == null) { for (int index = 0; index \u003c size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u003c size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } private void fastRemove(int index) { modCount++; // ... } 从这段代码可以发现, 对ArrayList的add(), remove(), clear()方法, 只要涉及到改变集合中的元素的个数的方法都会导致modCount的改变, 但是没有对expectedModCount进行改变. final void checkForComodification() { if (expectedModCount != modCount) throw new ConcurrentModificationException(); } 当expectModCount和modCount不同的时候, 就会抛出一开始出现的异常. ","date":"2022-03-06","objectID":"/failfast/:3:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"避免产生异常 ","date":"2022-03-06","objectID":"/failfast/:4:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"1. 使用普通for循环进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (int i = 0; i \u003c nameList.size(); i++) { String name = nameList.get(i); if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 这样虽然不会报错, 但是可能会产生漏删的情况出现. ","date":"2022-03-06","objectID":"/failfast/:4:1","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"2. 使用Iterator进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; Iterator\u003cString\u003e iterator = nameList.iterator(); while (iterator.hasNext()) { if (iterator.next().equals(\"丑八怪\")) { iterator.remove(); } } ","date":"2022-03-06","objectID":"/failfast/:4:2","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"3. 其实使用for-each也可以 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); break; } } 当集合中只删除一个元素的时候, 在删除操作之后立即break, 使循环不进入下一次遍历就可以了. ","date":"2022-03-06","objectID":"/failfast/:4:3","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"4. 使用fail-safe的集合类 ","date":"2022-03-06","objectID":"/failfast/:4:4","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"5. 使用stream中的filter","date":"2022-03-06","objectID":"/failfast/:4:5","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Data Structure"],"content":" 前两天在LeetCode做题的时候, 做到了二分查找(Binary Search). 现在来做一下梳理. 地址: LeetCode-Binary Search 二分查找作为程序员的一个基本技能, 也是面试的时候有可能面试官会问到的一种算法. 可以达到O(log n)的时间复杂度. 一般来说, 当出现这几个条件的时候, 就应该用到二分查找: 待查找的数组是有序的或者是部分有序的 要求时间复杂度低于O(n), 或者直接说明时间复杂度是O(log n) 而且二分查找也有很多的变体. 在使用的时候, 要注意好查找条件, 判断条件以及左右边界的条件变更方式. 这三个地方没有注意好, 很容易就会出现死循环或是遗漏. 今天来梳理一下这几种: 标准的二分查找 二分查找左边界 二分查找右边界 二分查找的左右边界 二分查找极值点 ","date":"2022-03-01","objectID":"/binarysearch/:0:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"标准的二分查找 上面这个题目就用标准的二分查找来实现, 我们先来看标准的二分查找的模板: public int search(int[] array, int target) { int left = 0, right = array.length-1; // 1. 因为循环中包含了 left == right 的条件, 所以每次循环的时候, left或right都要有变化 while (left \u003c= right) { // 这句话其实等同于 (right+left)/2, 但是这样的写法可以避免溢出 int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] == target) { return mid; } else if (array[mid] \u003c target) { // 左边界更新 left = mid + 1; } else { // 右边界更新 right = mid - 1; } } // 未查找到目标值 return -1; } ","date":"2022-03-01","objectID":"/binarysearch/:1:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界 使用这种变体的时候通常有这几种特性: 数组有序, 包含重复元素 数组部分有序, 包含重复元素 数组部分有序, 不包含重复元素 ","date":"2022-03-01","objectID":"/binarysearch/:2:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界① 这种分类包含了上面的1,3两种情况. 既然是查找左边界, 就要从右侧开始, 然后不断左移. 也就是说, 即使找到了array[mid] == target, 这个mid值也不见得就是要查找的左边界. 所以还是要继续收缩: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else { right = mid; } } return array[left] == target ? left : -1; } 可以很明显的看出来和标准的有何不同: 查找条件变为了left \u003c right 因为在最后left与right相邻的时候, mid和left处在同一位置. 所以下一步, left, mid, right都会在同一位置. 也就是说, 如果判断条件还是left \u003c= right的话, 可能最后就会进入死循环. 右边界更新为right = mid 因为需要在查找到目标值之后继续向左移动. ","date":"2022-03-01","objectID":"/binarysearch/:2:1","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界② 这种分类包含了上面的2情况, 也就是数组部分有序, 包含重复元素. 这种条件下, 右边界在向左移动的时候, 不能简单的令right = mid. 因为有重复的元素, 这样就可能会造成遗漏: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else if (array[mid] \u003e target) { right = mid; } else { --right; } } return array[left] == target ? left : -1; } ","date":"2022-03-01","objectID":"/binarysearch/:2:2","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Java"],"content":" 今天进行了一场面试, 面试官在问我关于HashMap的时候, 感觉自己回答的不是很好, 所以现在索性就梳理一下Java关于集合的这部分知识. 主要是问了这么几个问题: HashMap是线程安全的么 那线程安全的map是哪种? 在定义HashMap的时候会有定义长度的习惯么? HashMap的底层是怎么实现的? HashMap是如何存储的? HashMap最大长度是多少? 或者说是达到多大的长度就需要扩容了? (这个没答上来…😭) 说到Java的Collection就一定会放出这张神图\r根据这张图能发现, 这一切的一切都起始于Iterable接口. ","date":"2022-02-24","objectID":"/java_collection/:0:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Iterable 从源码里能看到, 这个接口允许对象成为for-each的循环目标, 也就是增强型for循环, 是Java中的一种语法糖. List\u003cObject\u003e list = new ArrayList(); // 补充: 数组也可`for-each`遍历 // Object[] list = new Object[5]; for (Object obj: list) {} 其他遍历方式 JDK 1.8 之前, Iterable只有一个方法: Iterator\u003cT\u003e iterator(); 这个接口能够创建一个轻量级的迭代器, 用于安全的遍历元素, 移除元素, 添加元素. 其中涉及了一个概念就是fail-fast. 总结起来就是: 能创建迭代器进行元素添加和删除的话, 就尽量使用迭代器进行添加和删除操作. for (Iterator it = list.iterator(); it.hasNext(); ) { System.out.println(it.next()); } ","date":"2022-02-24","objectID":"/java_collection/:1:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"顶层接口 Collection是一个顶层接口, 主要是用来定义集合的约定. List接口也是一个顶层接口, 继承了Collection接口, 同时也是ArrayList, LinkedList等集合元素的父类. Set接口位于与List接口同级的层次上, 它同时也继承了Collection接口. Set接口提供了额外的规定. 对add(), equals(), hashCode()方法提供了额外的标准. Queue是和List, Set接口并列的Collection的三大接口之一. Queue的设计用来在处理之前保持元素的访问次序. 除了Collection基础的操作外, 对立提供了额外的插入, 读取, 检查操作. SortSet接口直接继承与Set接口, 使用Comparable对元素进行自然排序或者使用Comparator在创建时对元素提供定制的排序规则. set的迭代器将按升序元素顺序遍历集合. Map是一个支持key-value存储的对象, Map不能包含重复的key, 每个键最多映射一个值. 这个接口替代了Dictionary类, Dictionary是一个抽象类而不是接口. ","date":"2022-02-24","objectID":"/java_collection/:2:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"ArrayList ArrayList是实现List接口的可扩容数组(动态数组), 它的内部是基于数组实现的, 具体的定义: public class ArrayList\u003cE\u003e extends AbstractList\u003cE\u003e implements List\u003cE\u003e, RandomAccess, Cloneable, java.io.Serializable {...} ArrayList可以实现所有可选择的列表操作, 允许所有元素 (包括 null). ArrayList还提供了内部存储list的方法, 它能够完全替代Vector, 只有一点例外, ArrayList不是线程安全的容器. 下面会说到Vector ArrayList有一个容量的概念, 这个数组的容量就是List用来存储元素的容量. 在不声明容量的时候, 默认的是10. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.5, 即: 旧容量 * 1.5 ==\u003e 10-\u003e15-\u003e22-\u003e33... ArrayList的上限为Integer.MAX_VALUE - 8(232 - 8). ArrayList不是线程安全的容器, 所以可以使用线程安全的List: List list = Collections.synchronizedList(new ArrayList\u003c\u003e()); ArrayList具有fail-fast快速失败机制, 当在迭代集合的过程中, 该集合发成了改变的时候, 就可能会发生fail-fast, 抛出ConcurrentModificationException异常. ","date":"2022-02-24","objectID":"/java_collection/:3:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Vector Java中的Vector类是允许不同类型共存的变长数组, Java.util.Vector提供了向量(Vector)类以实现类似动态数组的功能. 在相对于ArrayList来说, Vector线程是安全的, 也就是说是同步的. 因为Vector对内部的每个方法都是简单粗暴的上锁, 所以访问元素的效率远远低于ArrayList. 还有一点在扩容上, ArrayList扩容后的数组长度会增加50%, 而Vector的扩容长度后数组是翻倍. ","date":"2022-02-24","objectID":"/java_collection/:4:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedList LinkedList是一个双向链表, 允许所有元素 (包括 null): LinkedList所有的操作都可以表现为双向性, 索引到链表的操作将遍历从头到尾, 看那个距离短为遍历顺序. LinkedList不是线程安全的容器, 所以可以使用线程安全的Set: List list = Collections.synchronizedList(new LinkedList\u003c\u003e()); 因为LinkedList是一个双向链表, 所以没有初始化大小, 没有扩容机制. ","date":"2022-02-24","objectID":"/java_collection/:5:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Stack 堆栈(Stack)就是常说的后入先出的容器. 它继承了Vector类, 提供了常用的pop, push和peek操作, 以及判断stack是否为空的empty方法和寻找与栈顶距离的search方法. ","date":"2022-02-24","objectID":"/java_collection/:6:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashSet HashSet是Set接口的实现类, 有哈希表支持 (实际上HashSet是HashMap的一个实例), 不能保证集合的迭代顺序. 允许所有元素 (包括 null). HashSet不是线程安全的容器, 所以可以使用线程安全的Set: Set set = Collections.synchronizedSet(new HashSet\u003c\u003e()); 支持fail-fast机制. 因为HashSet的底层实际使用HashMap实现的, 所以和HashMap的容量和扩容机制是一致的: 在不声明容量的时候, 默认的是16. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.75, 即: 旧容量 * 1.75 ==\u003e 16-\u003e28-\u003e49-\u003e85... ","date":"2022-02-24","objectID":"/java_collection/:7:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeSet TreeSet是一个基于TreeMap的NavigableSet实现. 这些元素使用他们的自然排序或者在创建时提供的Comparator进行排序, 具体取决于使用的构造函数. 此实现为基本操作add, remove, contains提供了log(n)的时间成本. HashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:8:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashSet LinkedHashSet继承体系\rLinkedHashSet是Set接口的Hash表和LinkedList的实现. 但是这个实现不同于HashSet的是, 它维护者一个贯穿所有条目的双向列表. 此链表定义了元素插入集合的顺序. 注意: 如果元素重新插入, 则插入顺序不会受到影响. LinkedHashSet有两个影响其构成的参数: 初始容量和负载因子. 它们的定义与HashSet完全相同. 但是对于LinkedHashSet, 选择过高的初始容量值的开销要比HashSet小, 因为LinkedHashSet的迭代次数不收容量影响. LinkedHashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:9:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"PriorityQueue PriorityQueue(优先级队列)是AbstractQueue的实现类, 其中的元素根据自然排序(最小元素最先出)或者通过构造函数时期提供的Comparator来排序, 具体根据构造器判断. 注意: PriorityQueue不允许null元素. 队列的头在某种意义上是指定顺序的最后一个元素. 队列查找操作poll, remove, peek和element访问队列头部元素. PriorityQueue是无界队列(无限制的), 但是有内部capacity, 用户控制用于在队列中存储元素的数组大小. 该类以及迭代器实现了Collection, Iterator接口的所有可选方法. 这个迭代器提供了iterator()方法不能保证以任何特定顺序遍历PriorityQueue. 如果需要有序遍历的话, 可以考虑使用Arrays.sort(pq.toArray()). PriorityQueue必须存储的可比较的对象, 如果不是的话, 则必须指定比较器. PriorityQueue不是线程安全的容器, 而线程安全的类是PriorityBlockingQueue. ","date":"2022-02-24","objectID":"/java_collection/:10:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashMap HashMap是一个利用哈希表原理来存储元素的集合, 允许空的key-value键值对. HashMap的默认初始用量和负载因子和HashSet一致. HashMap不是线程安全的容器. ","date":"2022-02-24","objectID":"/java_collection/:11:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeMap 一个基于NavigableMap实现的红黑树. 这个map根据key自认排序存储, 或者通过Comparator进行定制排序. TreeMap为containsKey,get,put和remove方法提供了log(n)的时间开销. TreeMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:12:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashMap LinkedHashMap是Map接口的哈希表和链表的实现. 这个实现与HashMap的不同之处在于它维护了一个贯穿其所有条目的双向链表. 这个链表中定义的顺序, 通常是插入的顺序. 提供了一个特殊的构造器: LinkedHashMap(int,float,boolean), 其遍历的顺序是其最后一次访问的顺序. 可以重写removeEldestEntry(Map.Entry)方法, 以便在将新映射添加到map时强制删除过期映射的策略. 这个类提供了所有可选择的map操作, 并且允许null元素. 由于维护链表的额外开销, 性能可能会低于HashMap, 有一条除外: 遍历LinkedHashMap中的collection-views需要与map.size成正比, 无论其容量如何. HashMap的迭代看起来开销更大, 因为还要求时间与其容量成正比. LinkedHashMap有两个因素影响了它的构成: 初始容量和负载因子. LinkedHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:13:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashTable 与HashMap不同的是, HashTable是线程安全的. 任何非空对象都可以用作键或值. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:14:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"IdentityHashMap IdentityHashMap 是一个比较小众的Map实现类. IdentityHashMap不是一个通用的Map实现, 虽然这个类实现了Map接口, 但是它故意违反了Map的约定, 该约定要求在比较对象时使用equals方法, 此类仅适用于需要引用相等语义的极少数情况. IdentityHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:15:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"WeakHashMap WeakHashMap类基于哈希表的Map基础实现, 带有弱键. WeakHashMap中的entry当不再使用时还会自动移除. 也就是说, WeakHashMap中的entry不会增加其引用计数. 基于map接口, 是一种弱键相连, WeakHashMap里面的键会自动回收. 支持null键和null值. 支持fail-fast机制 WeakHashMap经常用作缓存. ","date":"2022-02-24","objectID":"/java_collection/:16:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"集合实现类特征图 下面这个表格汇总了部分集合框架的主要实现类的特征 集合 排序 随机访问 key-value存储 重复元素 空元素 线程安全 ArrayList ✅ ✅ ❌ ✅ ✅ ❌ LinkedList ✅ ❌ ❌ ✅ ✅ ❌ HashSet ❌ ❌ ❌ ❌ ✅ ❌ TreeSet ✅ ❌ ❌ ❌ ❌ ❌ HashMap ❌ ✅ ✅ ❌ ✅ ❌ TreeMap ✅ ✅ ✅ ❌ ❌ ❌ Vector ✅ ✅ ❌ ✅ ✅ ✅ HashTable ❌ ✅ ✅ ❌ ❌ ✅ ConcurrentHashMap ❌ ✅ ✅ ❌ ❌ ✅ Stack ✅ ❌ ❌ ✅ ✅ ✅ CopyOnWriteArrayList ✅ ✅ ❌ ✅ ✅ ✅ ","date":"2022-02-24","objectID":"/java_collection/:17:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Security"],"content":"什么是SQL注入 SQL注入攻击是通过将恶意的SQL语句插入到应用的输入参数中, 再在后台SQL服务器上解析执行的攻击. 是目前对数据库进行攻击的最常用手段之一. 主要原因是程序对用户输入数据的合法性没有判断和处理. ","date":"2022-02-24","objectID":"/sqlinjection/:0:1","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"原理 恶意拼接查询 都知道SQL语句是用;进行分隔两个语句的: SELECT * FROM users WHERE user_id = $user_id; 其中, user_id是传入参数, 但如果传入的参数变为1;DELETE FROM users;, 那么语句最终就会变为: SELECT * FROM users WHERE user_id = 1;;DELETE FROM users; 如果执行了上面的语句, 那么user表中所有数据都被删除了. 利用注释执行非法命令 SQL语句中可以添加注释: SELECT * FROM users WHERE user_gender='男' AND user_age=$age 如果user_age中包含了恶意的字符串20 OR 25 AND SLEEP(500)--, 那么语句最终会变为: SELECT * FROM users WHERE user_gender='男' AND user_age=20 OR 25 AND SLEEP(500)-- 上面这条语句只是想耗尽系统资源, SLEEP(500)会一直执行, 但是如果其中添加了修改, 删除数据的语句, 将会造成更大的破坏. 传入非法参数 SQL语句中的字符串是用单引号包裹的, 但是如果其本身包含单引号而没有处理, 那么就可能篡改SQL语句的作用: SELECT * FROM users WHERE user_name=$user_name 如果user_name传入参数值M'ustard, 那么语句最终会变为: SELECT * FROM users WHERE user_name='M'ustard' 一般情况下, 执行上面语句就会报错, 但是这种方式可能会产生恶意的SQL语句. 添加额外条件 在SQL语句中添加一些额外添加, 来改变执行行为. 条件一般为真值表达式: UPDATE users SET user_password=$user_password where user_id=$user_id 如果user_id传入的是1 OR TRUE, 那么语句最终会变为: UPDATE users SET user_password='123456' where user_id=1 OR TRUE 如果执行了上面的语句, 那么所有用户的密码都被更改了. ","date":"2022-02-24","objectID":"/sqlinjection/:0:2","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"防御手段 过滤输入内容, 校验字符串 过滤掉用户输入中的不合法字符剔除掉, 可以使用编程语言提供的处理函数或自己封装的函数来过滤, 也可以使用正则表达式来进行匹配. 也要验证参数的类型, 比如字符串或者整型. 参数化查询 参数化查询是目前被视作预防SQL注入攻击最有效的方法. 指的设计数据库连接并访问数据时, 在需要填入数据的地方, 使用参数(Parameter)来给值. MySQL的参数格式是以?加上参数名称而成: UPDATE table_1 SET row_1=?row_1, row_2=row_2 WHERE row_3=?row_3 在使用参数化查询下, 数据库不会将参数的内容视为SQL语句的一部分来处理, 而是在数据库完成SQL语句的编译之后, 才套用参数执行. 因此就算参数中含有破坏性的指令, 也不会被数据库所运行. 安全测试, 安全审计 避免使用动态SQL 不要将敏感数据保留在纯文本中 限制数据库权限和特权 避免直接向用户显示数据库错误 ","date":"2022-02-24","objectID":"/sqlinjection/:0:3","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"什么是CSRF攻击 CSRF攻击全称跨站请求伪造(Cross-site request forgery), 也被称为one-click attack或session riding, 通常缩写为CSRF或者XSRF. 尽管CSRF和XSS攻击很像, 但是两者的方式截然不同, 甚至可以说是相左的. XSS利用的是用户对指定网站的信任, CSRF利用的是网站对用户网页浏览器的信任. 我理解的是XSS是攻击者对浏览器下手, CSRF是对用户下手. 可以这么理解: 攻击者盗用受信任用户的身份, 以该用户的名已发送恶意请求. ","date":"2022-02-23","objectID":"/csrf/:0:1","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"CSRF攻击的原理 用户1访问受信任的网站A, 输入用户名和密码登录网站A. 登录成功之后, 网站A产生Cookie信息并返回给浏览器. 用户1在未退出登录网站A的情况下, 在同一浏览器, 访问网站B. 网站B接收到请求后, 携带网站A的Cookie信息, 发出一个请求去访问网站A 网站A在接收到网站B的请求之后, 会以用户1的权限处理该请求, 从而导致用户1的隐私泄漏和财产安全受到威胁. ","date":"2022-02-23","objectID":"/csrf/:0:2","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"常见的几种类型 GET类型的CSRF: 这种类型的CSRF利用非常简单, 只需要一个HTTP请求: \u003cimg src=http://xxx.com/csrf?user=xxx /\u003e 在访问这个img的页面后, 成功发出了一次HTTP请求. POST类型的CSRF: 这种类型的CSRF没有GET型的大, 利用起来通常使用的是一个自动提交的表单: \u003cform action=http://xxx.com/csrf.php method=POST\u003e \u003cinput type=\"text\" name=\"user\" value=\"xxx\" /\u003e \u003c/form\u003e \u003cscript\u003edocument.forms[0].submit();\u003c/script\u003e 其他类型CSRF: \u003cimg src=http://admin:admin@192.168.1.1 /\u003e 在访问这个img的页面后, 路由器会给用户一个合法的SESSION, 就可以进行下一步操作了. ","date":"2022-02-23","objectID":"/csrf/:0:3","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"防御手段 目前防御CSRF攻击主要有三种策略: 检查HTTP Referer字段 根据HTTP协议, 在HTTP头中有一个字段叫Referer, 它记录了该HTTP请求的来源地址. 优势: 简单易行, 网站的普通开发人员不需要操心CSRF的漏洞, 只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查Referer的值就可以了. 特别是对于已有的系统来说, 不需要更改当前系统的任何代码和逻辑, 没有风险. 劣势: 这种方式是把安全性都依赖于浏览器来保障, 对于一些低版本的浏览器来说, 是可以篡改Referer值的. 对于某些最新的浏览器, 虽然不能篡改Referer的值, 但是用户有些时候会认为Referer值会记录下用户的访问来源, 所以用户自己可以设置浏览器在发送请求的时候不再提供Referer. 这种情况下, 此种方式就会认为是CSRF攻击, 从而拒绝合法用户的访问. 添加校验token CSRF之所以能够成功是因为攻击者获取到了Cookie信息. 如果能够不只是依靠Cookie中的信息来抵御CSRF攻击, 那么就可以防御了. 所以这种方式就是在HTTP请求中以参数的形式加入一个随机产生的token, 并且在服务器端建立一个拦截器来验证这个token. 优势: 这种方法比检查Referer更安全一些, token可以在用户登录之后产生并放与SESSION中, 然后每次请求时把token从SESSION中取出来进行比对. 劣势: 难以保证token本身的安全. 疑问: 可以做到每次验证token成功之后, 产生一个新的token是否可以避免此种方法可能产生的漏洞. 在HTTP头中自定义属性并验证 这种方法也是使用token进行验证, 但是不是放在参数中, 而是放在HTTP请求头的自定义属性中. 通过XMLHttpRequest这个类, 可以一次性给所有该类请求加上csrftoken这个HTTP头属性, 并把token放进去. 优势: 不会记录到浏览器的地址栏, 统一管理token输入输出, 可以保证token的安全性. 劣势: 无法在非异步的请求上实施. ","date":"2022-02-23","objectID":"/csrf/:0:4","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":" 昨天的时候, 部门老大提到session和OnceToken的时候, 说到了xss攻击和csrf攻击. 今天记录一下学习的内容. ","date":"2022-02-23","objectID":"/xss/:0:0","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"什么是XSS攻击 XSS攻击全称跨站脚本攻击(Cross Site Scripting), 是一种在Web应用中的计算机安全漏洞, 它允许恶意Web用户将代码植入到提供给其他用户使用的正常页面中. 之所以缩写是XSS, 是因为如果是CSS会和层叠样式表混淆(Cascading Style Sheets). 举个简单的🌰, 恶意服务器嵌套了正常服务器中某页面的某form表单. 当用户登录了正常服务器之后, 在恶意服务器上也可以提交这个表单, 甚至拿到更高的权限. XSS是最普遍的Web应用安全漏洞. 可以做到劫持用户会话, 插入恶意内容, 重定向用户, 使用恶意软件劫持用户浏览器, 繁殖XSS蠕虫, 甚至是破坏网站, 修改路由器配置信息等. 所以XSS攻击的危害还是很严重的. ","date":"2022-02-23","objectID":"/xss/:0:1","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"XSS原理 首先看看XSS可以插入哪里: script标签内容 HTML注释内容 HTML标签的属性名 HTML标签的属性值 HTML标签的名字 直接插入到CSS里 看到XSS可以插入到这些地方之后, 就更能理解它的原理. XSS通过一些被特殊对待的文本和标记(🌰, 小于符号\u003c 被看做是HTML标签的开始), 使得用户浏览器将这些误认为是插入了正常的内容, 所以就会在用户浏览器中被执行. 也就是说, 当这些特殊字符不能被动态页面检查或检查出现失误时, 就将会产生XSS漏洞. ","date":"2022-02-23","objectID":"/xss/:0:2","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"分类 存储型XSS攻击(持久型): 最直接的危害类型. 将XSS代码提交存储到服务器端(数据库, 内存, 文件系统等). 这样下次请求目标页面时就不用再提交XSS代码, 会从服务器获取. 一般出现在留言, 评论, 博客日志等交互. 存储型XSS攻击\r反射性XSS攻击(非持久型): 最普遍的类型. 通过特定手法(🌰email), 诱使用户访问一个包含恶意代码的地址, 当用户点击这些链接的时候, 恶意代码会在用户的浏览器执行. 一般出现在搜索栏, 用户登录口, 常用来窃取客户端Cookies或进行钓鱼欺骗. 反射型XSS攻击\rDOM-based 型XSS攻击: 通过/xss修改页面的DOM(Document Object Model)结构, 是纯粹发生在客户端的攻击. 在整个攻击过程中, 服务器响应的页面没有发生变化, 取出和执行恶意代码都由浏览器端完成, 属于前端自身的安全漏洞. ","date":"2022-02-23","objectID":"/xss/:0:3","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"防御手段 总体思路: 对用户的输入(和URL参数)进行过滤, 对输出进行编码. 也就是说, 对用户提交的所有内容进行过滤, 对URL中的参数进行过滤, 过滤掉会导致脚本执行的相关内容; 然后对动态输出到页面的内容进行html编码, 使脚本无法在浏览器中执行. 还可以服务端设置会话Cookie的HTTP Only属性, 这样客户端的JS脚本就不能获取Cookie信息了. ","date":"2022-02-23","objectID":"/xss/:0:4","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":null,"content":"Java Overview (Java Platform SE 8 ) (oracle.com) Java 8 中文版 Overview (Java SE 11 \u0026 JDK 11 ) (oracle.com) Java 11 中文版 Nacos Maven各版本地址 ","date":"2022-02-22","objectID":"/documents/:1:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Big Data Apache Kafka The Scala Programming Language (scala-lang.org) Apache Flink: Stateful Computations over Data Streams Apache SeaTunnel ","date":"2022-02-22","objectID":"/documents/:2:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"iOS Swift - Resources - Apple Developer Vapor Xcode Releases ","date":"2022-02-22","objectID":"/documents/:3:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Other 力扣（LeetCode）中国官网 Markdown 入门基础 | Markdown 官方教程 飞桨PaddlePaddle-源于产业实践的开源深度学习平台 Elasticsearch Guide 8.0 ","date":"2022-02-22","objectID":"/documents/:4:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":["MySQL"],"content":"有了数据库之后, 还需要先进行压测 拿到一个数据库之后, 首先得先对这个数据库进行一个较为基本的基准压测. 也就是说, 你得基于一些工具模拟一个系统每秒发出1000个请求到数据库上去, 观察一下他的CPU负载、磁盘IO负载、网络 IO负载、内存复杂, 然后数据库能否每秒处理掉这1000个请求, 还是每秒只能处理500个请求? 这个过程, 就是压测. 那为什么不等到Java系统都开发完之后, 直接让Java系统连接上MySQL数据库, 然后直接对Java系统进行压测呢? 因为数据库的压测和它上面的Java系统的压测, 其实是两回事, 首先得知道数据库最大能抗多大压力, 然后再去看Java系统能抗多大压力. 因为有一种可能是, 数据库每秒可以抗下2000个请求, 但是Java系统每秒只能抗下500个请求. 所以不能光是对Java系统去进行压测, 在那之前也得先对数据库进行压测, 做到心里有个数. ","date":"2022-01-10","objectID":"/mysql_4/:1:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"QPS和TPS到底有什么区别 既然是要压测, 那么肯定得先明白一点, 每秒能抗下多少个请求, 其实是有专业术语的, 分别是QPS和TPS. QPS: Query Per Second. 也就是说数据库每秒可以处理多少个请求, 大致可以理解为一次请求就是一条SQL语句, 也就是说数据库可以每秒处理多少个SQL语句. Java系统或者中间件系统在进行压测的时候, 也可以使用这个指标. TPS: Transaction Per Second. 指的是每秒可以处理的事务量. 就是说数据库可以每秒处理多少次事务提交或者回滚. ","date":"2022-01-10","objectID":"/mysql_4/:2:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"IO相关的压测性能指标 IOPS: 指的是机器的随机IO并发处理能力. 举个🌰: 机器可以达到200 IOPS, 意思就是说每秒可以执行200个随机IO读写请求. 这个指标很关键, 因为在之前说过, 在内存中更新的脏数据库, 最后都由后台IO线程在不确定的时间, 刷回到磁盘中去, 这就是随机IO的过程. 也就是说, 如果IOPS指标太低, 那么就会导致内存里的脏数据库刷回磁盘的效率不高. 吞吐量: 指的是机器的磁盘存储每秒可以读写多少字节的数据量. 这个指标也很关键, 之前也说过, 在执行各种SQL语句的时候, 提交事务的时候, 其实都是大量的会写redo log之类的日志的, 这些日志都会直接写磁盘文件. 所以一台机器的存储每秒可以读写多少字节的数据量, 就决定了他每秒可以把多少redo log之类的日志写入到磁盘里去. 一般来说, 我们写redo log之类的日志, 都是对磁盘文件进行顺序写入的, 也就是一行接着一行的写, 不会说进行随机的读写, 那么一般普通磁盘的顺序写入的吞吐量每秒都可以达到200MB左右. 所以通常而言, 机器的磁盘吞吐量都是足够承载高并发请求的. latency: 指的是往磁盘里写入一条数据的延迟. 这个指标同样重要, 因为执行SQL语句的和提交事务的时候, 都需要顺序写redo log磁盘文件, 所以此时写一条日志到磁盘文件里去, 到底是延迟1ms, 还是延迟100us, 这就对数据库的SQL语句执行性能是有影响的. ","date":"2022-01-10","objectID":"/mysql_4/:3:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"压测的时候要关注的其他性能指标 CPU负载: 这个也是一个很重要的指标. 因为假设数据库压测到了3000 QPS, 可能其他指标都还正常, 但是此时CPU负载特别高, 那么也说明你的数据库不能继续往下压测更高的QPS了, 否则CPU是吃不消的. 网络负载: 这个就是看机器带宽情况下, 在压测到一定的QPS和TPS的时候, 每秒钟机器的网卡会输入多少MB数据, 会输出多少MB数据. 因为有可能网络带宽最多每秒传输100MB的数据, 那么可能QPS到1000的时候, 网卡就打满了, 已经每秒传输100MB的数据了, 此时即使其他指标还正常, 也不能继续压测下去了. 内存负载: 这个就是看压测到一定情况下的时候, 机器内存损耗了多少, 如果说机器内存损耗过高了, 说明也不能继续压测下去了. ","date":"2022-01-10","objectID":"/mysql_4/:4:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"推荐压测工具 sysbench, 这个工具可以自动帮你在数据库里构建出来大量的数据. 然后也可以模拟几千个线程并发的访问数据库, 模拟各种sql语句, 包括各种书屋提交到数据库里, 甚至可以模拟出几十万的TPS对数据库进行压测. ","date":"2022-01-10","objectID":"/mysql_4/:5:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"生产数据库一般用什么配置的机器 首先要明确的一点, 如果系统是一个没什么并发访问量, 用户就几十个人或者几百个人的系统, 那么其实选择什么样的机器去部署数据库, 影响不大. 哪怕是个人笔记本电脑去部署一个MySQL数据库, 其实也能支撑地并发系统的运行. 因为这种系统可能每个几分钟才会有一波请求发到数据库上, 而且数据库里一张表也许就几百条, 几千条或者是几万条. 数据量很小, 并发量很小, 操作频率很低, 用户量很小, 并发量很小, 只不过可能系统的业务逻辑很复杂而已. 对于这类系统的数据库机器选型, 什么样都可以. ","date":"2021-12-27","objectID":"/mysql_3/:1:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"普通的Java应用系统部署在机器上能抗多少并发 通常来说, Java应用系统部署的时候常选用的机器配置大致是2核4G和4核8G的较多一些, 数据库部署的时候常选用的机器配置最低在8核16G以上, 正常在16核32G. 那么以大量的高并发线上系统的生产经验观察下来而言, 一般Java应用系统部署在4核8G的机器上, 每秒钟抗下500左右的并发访问量, 差不多是比较合适的. 当然这个也不是绝对的, 假设每个请求花费1s可以处理完, 那么一台机器每秒也许只可以处理100个请求, 但是如果每个请求只要花费100ms就可以处理完, 那么一台机器每秒也许就可以处理几百个请求. 所以一台机器能抗下每秒多少请求, 往往是跟每个请求处理耗费多长时间关联的. 但是大体上来说, 根据大量的经验观察而言, 4核8G的机器部署普通的Java应用系统, 每秒大致能抗下几百的并发访问, 从每秒一两百请求到每秒七八百请求, 都是有可能的, 关键是每个请求耗费多长时间. ","date":"2021-12-27","objectID":"/mysql_3/:2:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"高并发场景下, 数据库应该用什么样的机器 对于数据库而言, 上文也说了, 通常推荐的数据至少是选用8核16G以上的机器更加合适. 因为要考虑一个问题, 对于我们的Java应用系统, 主要耗费时间的是Java系统和数据库之间的网络通信. 对Java系统自己而言, 如果仅仅只是系统内部运行一些普通的业务逻辑, 纯粹在自己内存中完成一些业务逻辑, 这个性能是极高极高的. 对于Java系统受到的每个请求, 耗时最多的还是发送网络请求到数据库上去, 等待数据库执行一些SQL语句, 返回结果给Java系统. 所以其实常说的Java系统压力很大, 负载很高. 其实主要的压力和复杂都是集中在依赖的那个MySQL数据库上! 因为执行大量的增删改查的SQL语句的时候, MySQL数据库需要对内存和磁盘文件进行大量的IO操作, 所以数据库往往是负载最高的! 通过经验而言, 一般8核16G的机器部署的MySQL数据库, 每秒抗个一两千并发请求是没问题的, 但是如果并发量再高一些, 假设每秒有几千并发请求, 那么可能数据库就会有危险了, 因为数据库的CPU、磁盘、IO、内存的负载都会很高, 弄不好数据库压力过大就会宕机. 对于16核32G的机器部署的MySQL数据库而言, 每秒两三千, 甚至三四千的并发也都是可以的, 但是如果达到每秒上万请求, 也是会有宕机的危险. 如果可以的话, 数据库机器周最好用SSD的硬盘而不是机械硬盘. ","date":"2021-12-27","objectID":"/mysql_3/:3:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"申请机器机器之后做好心中有数, 交给专业的DBA部署 数据库机器申请下来之后, 作为架构师要对机器做到心中有数. 比如申请的是8核16G的机器, 心里大致就该知道这个数据库每秒抗个一两千请求是可以的, 如果申请的是16核32G的机器, 那心里知道妥妥可以抗个每秒两三千, 甚至三四千的请求. 其次, 申请一台机器下来之后, 接着这台机器在有一定规模的公司里, 一定是交给公司专业的DBA去安装、部署和启动MySQL的. DBA这个时候会按照他国王的经验, 用自己的MySQL生产调优参数模板, 直接放到MySQL里去, 然后用一个参数模板去启动这个MySQL, 往往这里很多参数都是调优过的. 而且DBA还可能对linux机器一些OS内核参数进行一定的调整, 比如说最大文件句柄之类的参数, 这些参数往往也都是需要调整的. ","date":"2021-12-27","objectID":"/mysql_3/:4:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"什么是InnoDB? InnoDB是第一个提供外键约束的存储引擎, 而且它对事务的处理能力是其它存储引擎无法与之比拟的. MySQL在5.5版本之后, 默认存储引擎由MyISAM修改为InnoDB. 目前, InnoDB是最重要的, 也是使用最广泛的存储引擎. 1. InnoDB优势: 支持事务安装 灾难恢复性好 使用行级锁 实现了缓冲处理 支持外键 适合需要大型数据库的网站 2. 物理存储 数据文件(表数据和索引数据): 共享表空间 独立表空间 日志文件 ","date":"2021-12-23","objectID":"/mysql_2/:1:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新语句在MySQL中是如何执行的? 首先假设有一条语句是这样的: UPDATE users SET name='xxx' WHERE id=10 这条语句是如何执行的呢? 首先肯定是系统通过一个数据库连接发送到了MySQL上, 然后经过SQL接口、解析器、优化器、执行器几个环节, 解析SQL语句, 生成执行计划, 接着由执行器负责这个计划的执行, 调用InnoDB存储引擎的接口去执行. ","date":"2021-12-23","objectID":"/mysql_2/:2:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"InnoDB的重要内存结构: 缓冲池 前面提到了InnoDB的一个优势就是\"实现了缓冲处理\", 就是通过InnoDB存储引擎中的一个非常重要的放在内存里的组件实现的, 就是缓冲池(Buffer Pool). 这个里面会存很多数据, 便于以后的查询, 要是缓冲池里有数据, 就不会去磁盘查询. 所以当执行上面那条更新语句的时候, 就会现将id=10这一行数据看看是否在缓冲池里, 如果不在的话, 那么会直接从磁盘里加载到缓冲池里来, 而且还会对这行记录加锁. ","date":"2021-12-23","objectID":"/mysql_2/:3:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"undo日志文件: 如何让你更新的数据可以回滚 接着下一步, 假设id=10这行数据的name原来是\"zhangsan\", 现在我们更新为\"xxx\", 那么此时得现将要更新的原来的值\"zhangsan\"和id=10这些信息, 写入到undo日志文件中去. 其实大家都知道, 如果执行一个更新语句是在一个事务里的话, 那么事务提交之前我们都是可以对数据进行**回滚(rollback)**的. ","date":"2021-12-23","objectID":"/mysql_2/:4:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新buffer pool中的缓存数据 当我们把要更新的那行记录从磁盘文件加载到缓冲池, 也对其进行加锁之后, 并且还把更新前的旧值写入undo日志文件之后, 就可以正式开始更新这行记录了. 更新的时候, 先是会更新缓冲池中的记录, 此时这个数据就是脏数据了. 为什么是脏数据: 因为此时的磁盘中id=10这行数据的name还是\"zhangsan\", 还不是\"xxx\". ","date":"2021-12-23","objectID":"/mysql_2/:5:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"Redo Log Buffer: 万一系统宕机, 如何避免数据丢失 如果按照上面的操作进行更新, 现在已经把内存里的数据进行了修改, 但是磁盘上的数据还没有修改. 就在这个时候, 系统宕机了, 该怎么办? 这个时候就必须要把对内存所做的修改写入到一个Redo Log Buffer里去, 这也是一个内存的缓冲区, 用来存放redo日志的. redo日志用来记录对什么记录进行了修改, 比如对id=10这行记录修改了name为\"xxx\", 这就是一个日志. 这个redo日志其实就是用来在MySQL突然宕机的时候, 用来恢复更新过的数据的. ","date":"2021-12-23","objectID":"/mysql_2/:6:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"如果还没提交事务, MySQL宕机了怎么办 如果还没有提交事务, 那么此时如果MySQL崩溃, 必然导致内存里Buffer Pool中的修改过的数据都丢失, 同时写入Redo Log Buffer中的redo日志也会消失. 其实此时数据丢失是不要紧的, 因为一个更新语句, 没提交事务, 就代表还没有执行成功, 此时MySQL宕机虽然导致内存里的数据都丢失了, 但是会发现, 磁盘上的数据怡然居还停留在原样子. 换句话说, id=10那行数据的name字段的值还是旧值\"zhangsan\", 所以此时这个事务就是执行失败了, 没能成功完成更新, 会收到一个数据库的异常. 然后当MySQL重启之后, 数据并没有任何变化. 所以, 如果还没提交事务时, MySQL宕机了, 不会有任何问题. ","date":"2021-12-23","objectID":"/mysql_2/:7:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"提交事务的时候将redo日志写入磁盘中 接着俩要提交一个事务了, 此时就会根据一定的策略把redo日志从redo log buffer里刷入到磁盘文件里去. 这个策略是通过innodb_flush_log_at_trx_commit来配置的, 它有几个选项. 当这个参数的值为0的时候, 那么当提交事务的时候, 不会把redo log buffer里的数据刷入磁盘文件, 此时可能都提交事务了, 结果MySQL宕机了, 然后内存里的数据全部丢失了. 这就相当于提交事务成功了, 但是由于MySQL宕机, 导致内存中的数据和redo日志都丢失了. 当这个参数的值为1的时候, 那么当提交事务的时候, 就必须把redo log buffer从内存刷入到磁盘文件里去, 只有事务提交成功, 那么redo log就必然在磁盘里了. 所以只有提交事务成功之后, redo日志一定在磁盘文件里. 也就是说, 哪怕此时buffer pool中更新过的数据还没刷新到磁盘里去, 此时内存里的数据已经是更新过name=\"xxx\", 然后磁盘上的数据还是没更新过的name=\"zhangsan\". 当MySQL重启之后, 可以根据redo日志去恢复之前做过的修改. 当这个参数的值是2的时候, 那么当提交事务的时候, 把redo日志写入磁盘文件对应的os cache里去, 而不是直接进入磁盘文件, 可能1秒之后才会吧os cache里的数据写入到磁盘文件里去. 这种模式下, 提交事务之后, redo log可能仅仅停留在os cache内存缓存里, 没实际进入磁盘文件, 玩意此时要是机器宕机了, 那么os cache里的redo log就会丢失, 同样会感觉提交事务了, 但是结果数据丢了. ","date":"2021-12-23","objectID":"/mysql_2/:8:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"什么是CRUD? CRUD是指在做计算处理时的增加(Create), 读取查询(Retrieve), 更新(Update)和删除(Delete). 主要是被用在描述软件系统中DataBase或者持久层的基本操作. ","date":"2021-12-13","objectID":"/mysql_1/:1:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"什么是数据库驱动? 想要访问数据库, 就需要和数据库建立一个网络连接. 那么, 建立这个网络连接的就是数据库驱动. 所以对于MySQL来说, 对应每种常见的编程语言(e.g. Java, PHP, .NET, Python, Ruby等), MySQL都会提供对应语言的MySQL驱动. 其实数据库驱动就是中间件. ","date":"2021-12-13","objectID":"/mysql_1/:2:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"数据库连接池是用来干嘛的? 首先要知道的是, 一个系统和数据库建立的连接往往都不止一个. 但是每次访问数据库的时候都建立一个连接, 然后执行SQL语句, 然后再销毁这个连接, 这种方式显然是不合适的. 因为每次建立一个数据库连接都很耗时, 效率会很低下. 所以, 就出现了数据库连接池这个东西. 一个数据库连接池里会维持多个连接, 让多个线程使用里面的不同的数据库连接去执行SQL语句, 执行完语句之后, 不是销毁这个连接, 而是把它放回池子里, 后面还能继续用. 常见的数据库连接池有DBCP, C3P0, Druid ","date":"2021-12-13","objectID":"/mysql_1/:3:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"MySQL是如何执行一条SQL语句的? ","date":"2021-12-13","objectID":"/mysql_1/:4:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第一步 线程: 接收SQL语句 首先, 假设数据库服务器的连接池中的某个连接收到了网络请求, 假设就是一条SQL语句, 这个工作一定是一个线程来进行处理的, 来监听请求以及读取请求数据. 当MySQL的工作线程接收到SQL语句之后, 会转交给SQL接口去执行. SQL接口(SQL Interface)就是MySQL内部提供的一个组件, 是一套执行SQL语句的接口. ","date":"2021-12-13","objectID":"/mysql_1/:4:1","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第二步 SQL接口: 解析SQL语句 那么, SQL接口又是如何执行SQL语句的呢? 比如要执行下面这条语句: SELECT id, name, age FROM users WHERE id = 1; MySQL本身也是一个系统, 是一个数据库管理系统(DBMS), 是没法直接理解这些语句的, 所以就需要**查询解析器(Parser)**出场了! 这个查询解析器是负责对SQL语句进行解析的, 比如上面的语句拆解一下, 可以拆解为一下几个部分: 我们现在要从users表里查询数据 查询id字段的值等于1的那行数据 对查出来的哪行数据要提取里面的id, name, age三个字段 所谓的SQL解析, 就是按照既定的SQL语法, 对我们按照SQL语法规则编写的SQL语句进行解析, 然后理解这个SQL语句要干什么事情. ","date":"2021-12-13","objectID":"/mysql_1/:4:2","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第三步 查询优化器: 选择最优查询路径 当通过解析器理解了SQL语句要干什么之后, 接着就会找查询优化器(Optimizer)来选择一个最优的查询路径. ","date":"2021-12-13","objectID":"/mysql_1/:4:3","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第四步 存储引擎: 调用存储引擎接口, 真正执行SQL语句 最后一步, 就是把查询优化器选择的最有查询路径交给底层的存储引擎去真正的执行. 存储引擎是MySQL架构设计中很有特色的一个环节. 在真正执行SQL语句的时候, 要不是更新数据, 要不是查询数据, 但是具体的数据是存放在内存里还是在磁盘里呢? 这个时候就需要存储引擎了, 存储引擎其实就是执行SQL语句的, 它是按照一定的步骤去查询内存缓存数据, 更新磁盘数据, 查询磁盘数据等等, 执行诸如此类的一系列的操作. MySQL的架构设计中, SQL接口, SQL解析器, 查询优化器其实都是通用的, 就是一套组件而已. 但是是支持各种各样的存储引擎的, 比如常见的InnoDB, MyISAM, Memory等等, 我们是可以选择使用哪种存储引擎来负责具体的SQL语句执行的. ","date":"2021-12-13","objectID":"/mysql_1/:4:4","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第五步 执行器: 根据执行计划调用存储引擎的接口 现在回过头来看一个问题, 存储引擎可以访问内存和磁盘上的数据, 那么是谁来调用存储引擎的接口呢? 其实还漏了一个执行器的概念, 执行器会根据优化器选择的执行方案, 去调用存储引擎的接口按照一定的顺序和步骤, 就把SQL语句的逻辑给执行了. 举个🌰: 比如执行器可能会先调用存储引擎的一个接口, 去获取users表中的第一行数据, 然后判断一下这个数据的id字段的值是否等于我们期望的值, 如果不是的话, 就继续调用存储引擎的接口, 去获取users表的下一行数据. 基于上述的思路, 执行器就会去根据优化器生成的一套执行计划, 然后不停的调用存储引擎的各种接口去完成SQL语句的执行计划, 大致就是不停的更新或者提取一些数据出来. ","date":"2021-12-13","objectID":"/mysql_1/:4:5","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":null,"content":"初衷 Since: 2021-11-20 08:27:00 ","date":"2021-11-20","objectID":"/about/:1:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"期许 不卑不亢，不矜不伐，戒骄戒躁 不嗔不怒，不争不弃，独善其身 ","date":"2021-11-20","objectID":"/about/:2:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"About me 在职: iOS开发程序猿, Java开发程序猿, 大数据开发小学徒 用我所学, 学我所用. 不盲目堆叠技术栈, 保持谦逊, 保持探索欲, 砥砺前行. ","date":"2021-11-20","objectID":"/about/:3:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"Other Annual Summary /years/ ","date":"2021-11-20","objectID":"/about/:4:0","tags":null,"title":"关于 Buli Home","uri":"/about/"}]