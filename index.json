[{"categories":["draft"],"content":" 在日常使用 Hive 的过程中, 经常会出现这样一种场景: 明明查询的时候进度条很快, 但是总是会卡在 99% 的地方. 出现这种情况往往就是因为数据倾斜导致的. ","date":"2023-10-10","objectID":"/hive_skewed_data/:0:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"什么是数据倾斜 数据倾斜在 MapReduce 中经常发生的. 简单点说就是, 在整个计算过程中, 大量相同的 key 被分配到了同一个任务上, 造成了大量的数据涌入到一个节点当中. 这也违背了分布式计算的初衷, 使得计算的整体执行效率十分低下. 数据倾斜后的直观表现就是任务进度长时间维持在 99% (或100%), 查看监控之后会发现只有少量的 Reduce 子任务未完成. 因为其处理的数据量和其他的 Reduce 子任务差异过大, 造成最长时长远大于平均时长. ","date":"2023-10-10","objectID":"/hive_skewed_data/:1:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"什么情况下会出现数据倾斜 在日常使用中数据倾斜主要是发生在 Reduce 阶段, 很少会发生在 Map 阶段, 因为 Map 阶段的数据倾斜一般是由于 HDFS 数据存储不均匀造成的(一般存储都是均匀分块存储, 每个文件大小基本固定), 而 Reduce 阶段的数据倾斜几乎都是因为数据没有考虑到某种 key 值数据量偏多的情况而导致的. Reduce 阶段最容易出现数据倾斜的两个场景分别是 Join 和 Count Distinct. ","date":"2023-10-10","objectID":"/hive_skewed_data/:2:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"数据倾斜的原因 key 分布不均匀 业务数据本身的特性 建表时考虑不周 某些 SQL 语句本身就有数据倾斜 ","date":"2023-10-10","objectID":"/hive_skewed_data/:3:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"数据倾斜的解决方案 ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"优先开启负载均衡 -- map 端的 Combiner, 默认为 true set hive.map.aggr=true; -- 开启负载均衡 (默认为 false) set hive.groupby.skewindata=true; 如果发生数据倾斜, 首先需要调整参数, 进行负载均衡处理, 这样 MapReduce 进程则会生成两个额外的 MR Job, 这两个任务的主要操作如下: MR Job 中 Map 输出的结果集首先会随机分配到 Reduce 中, 然后每个 Reduce 做局部聚合操作并输出结果, 这样处理的原因是相同的 Group By Key 有可能被分发到不同的 Reduce Job 中, 从而达到负载均衡的目的. MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中 (这个过程会保证相同的 Group By Key 被分不到同一个 Reduce 中), 最后完成聚合. ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:1","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"表 join 连接时引发的数据倾斜 两个表进行 join 时, 如果表连接的 key 存在倾斜, 那么在 Shuffle 阶段必然会引起数据倾斜 小表 join 大表, 某个 key 过大 通常做法是将倾斜的数据存到分布式缓存中, 分发到各个 Map 任务所在节点. 在 Map 阶段完成 join 操作, 即 MapJoin, 这样就避免了 Shuffle, 从而避免了数据倾斜. MapJoin 是 Hive 的一种优化操作, 其适用于小表 Join 大表的场景, 由于表的 Join 操作是在 Map 端且在内存尽情的, 所以其并不需要启动 Reduce 任务也就不需要经过 Shuffle 阶段, 从而能在一定程度上节省资源提高 Join 效率. 在 Hive v0.11 之前, 如果想在 Map 阶段完成 join 操作, 必须使用 MAPJOIN 来标记显式的启动该优化操作, 由于其需要将小表加载进内存所以要注意小表的大小. -- 常规 join SELECT pis.id_ id, service_name serviceName, service_code serviceCode, pip.code_text serviceType FROM prd_price_increment_service pis LEFT JOIN prd_price_increment_product pip on pip.increment_service_id = pis.id_; -- Hive v0.11 之前开启 MapJoin -- 将小表 prd_price_increment_service 放到 map 端的内存中 -- 如果想将多个表放在 Map 端内存中, 只需在 mapjoin() 中写多个表名称即可, 用逗号分割 SELECT /*+ mapjoin(pis) */ pis.id_ id, service_name serviceName, service_code serviceCode, pip.code_text serviceType FROM prd_price_increment_service pis LEFT JOIN prd_price_increment_product pip on pip.increment_service_id = pis.id_; 在 v0.11 及之后, Hive 默认启动该优化, 不需要显式的使用 MAPJOIN 标记, 可以通过下面两个属性来设置该优化的出发时机: -- 自动开启 MAPJOIN 优化, 默认为 true set hive.auto.convert.join=true; -- 确定使用该优化的表的大小, 如果表的大小小于此值就会被加载到内存中, 默认为 25000000 (25M) set hive.mapjoin.smalltable.filesize=25000000; SELECT pis.id_ id, service_name serviceName, service_code serviceCode, pip.code_text serviceType FROM prd_price_increment_service pis LEFT JOIN prd_price_increment_product pip on pip.increment_service_id = pis.id_; -- 特殊说明 -- 使用默认启动该优化的方式如果出现莫名其妙的 bug (比如 MAPJOIN 不起作用), 就将以下两个属性置为 false -- -- 关闭自动 MAPJOIN 转换操作 set hive.auto.convert.join=false; -- 不忽略 MAPJOIN 标记 set hive.ignore.mapjoin.hint=false; SELECT /*+ mapjoin(pis) */ pis.id_ id, service_name serviceName, service_code serviceCode, pip.code_text serviceType FROM prd_price_increment_service pis LEFT JOIN prd_price_increment_product pip on pip.increment_service_id = pis.id_; 将表放在 Map 端的内存时, 如果节点的内存很大, 但还是出现内存溢出的情况, 可以通过参数 mapreduce.map.memory.mb 调节 Map 端内存的大小. 表中作为关联条件的字段值为 0 或空值的较多 -- 方案一: 给空值添加随机 key 值, 将其发放到不同的 reduce 中处理. 由于 null 值关联不上, 所以对结果无影响. SELECT * FROM log a LEFT JOIN users b ON (CASE WHEN a.user_id IS NULL THEN CONCAT('hive', rand()) ELSE a.user_id END) = b.user_id; -- 方案二: 去重空值 SELECT a.*, b.name FROM ( SELECT * FROM users WHERE LENGTH(user_id) \u003e 1 OR user_id IS NOT NULL ) a JOIN ( SELECT * FROM log WHERE LENGTH(user_id) \u003e 1 OR user_id IS NOT NULL ) b ON a.user_id = b.user_id; 表中作为关联条件的字段重复值过多 SELECT a.*, b.name FROM ( SELECT * FROM users WHERE LENGTH(user_id) \u003e 1 OR user_id IS NOT NULL) a ) a JOIN ( SELECT * FROM (SELECT *, row_number() over(partition by user_id order by create_time desc) rk FROM log WHERE LENGTH(user_id) \u003e 1 OR user_id IS NOT NULL) t where rk = 1 ) b ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:2","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"空值引发的数据倾斜 实际业务中有些大量的 null 值或者一些无意义的数据参与到计算作业中, 表中有大量的 null 值, 如果表之间进行 JOIN 操作, 就会有 Shuffle 产生, 这样所有的 null 值都会被分配到一个 reduce 中, 所以就一定会产生数据倾斜. 这里有一个问题: 如果 A, B 两表进行 JOIN 操作, 如果 A 表中需要 JOIN 的字段为 null, 但是 B 表中需要 JOIN 的字段不为 null, 这两个字段根本就 JOIN 部上, 但是还是会放到一个 reduce 中. 这是因为, 数据放到同一个 reduce 中的原因不是因为字段能不能 JOIN 上, 而是因为 Shuffle 阶段的 hash 操作, 只有 key 的 hash 是一样的, 就会被放到同一个 reduce 中. -- 解决方案 -- 场景: 🌰 日志中, 经常会有信息丢失的问题, 比如日志中的 user_id, 如果取其中的 user_id 和用户表中的 user_id 关联, 会碰到数据倾斜的问题 -- 方案一: 可以直接不让 null 值参与 JOIN 操作, 即不让 null 值有 Shuffle 阶段, 所以 user_id 为空的不参与关联 SELECT * FROM log a JOIN users b ON a.user_id IS NOT NULL AND a.user_id = b.user_id UNION ALL SELECT * FROM log a WHERE a.user_id IS NULL; -- 方案二: 因为 null 值参与 Shuffle 时的 hash 结果是一样的, 那么我们可以给 null 值随机赋值, 这样它们的 hash 结果就不一致, 就会进到不同的 reduce 中 SELECT * FROM log a LEFT JOIN users b ON (CASE WHEN a.user_id IS NULL THEN CONCAT('hvie_', rand()) ELSE a.user_id END) = b.user_id 针对上述方案进行分析, 方案二比方案一的效率更高, 不但 IO 少了, 而且作业数也少了. 方案一中对 log 读取两次, jobs 为 2. 方案二 job 数是 1. 这个优化适合对无效 id (-99, ‘’, null 等) 产生的倾斜问题. 把空值的 key 变成一个字符串加上随机数, 就能把倾斜的数据分到不同的 reduce 上, 从而解决倾斜的问题. ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:3","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"不同数据类型关联产生数据倾斜 对于两个表 JOIN, 表 a 中需要 JOIN 的字段 key 为 int, 表 b 中 key 字段既有 string 类型也有 int 类型. 当按照 key 进行两个表的 JOIN 操作时, 默认的 hash 操作会按 int 类型的 id 来进行分配, 这样所有的 string 类型都将分配成同一个 id, 结果就是所有的 string 类型的字段进入到一个 reduce 中, 从而造成数据倾斜. -- 如果 key 字段既有 string 类型也有 int 类型, 那么就直接都转成 string 类型, hash 时就会按照 string 类型分配了 -- 方案一: 把数字类型转成字符串类型 SELECT * FROM users a LEFT JOIN logs b ON a.user_id = CAST(b.user_id AS string); -- 方案二: 建表时按照规范建设, 统一词根, 同一词根数据类型一致 ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:4","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"COUNT DISTINCT 大量相同特殊值 由于 SQL 中的 DISTINCT 操作本身会有一个全局排序的过程, 一般情况下, 不建议采用 COUNT DISTINCT 方式进行去重计数, 除非表的数量较小. 当 SQL 中不存在分组字段时, COUNT DISTINCT 操作仅生成一个 reduce 任务, 该任务对全部数据进行去重统计; 当 SQL 中存在分组字段时, 可能某些 reduce 任务需要去重统计的数量非常大; -- 可能造成倾斜的 SQL SELECT a, COUNT(DISTINCT b) FROM t GROUP BY a; -- 先去重, 然后分组统计 SELECT a, sum(1) FROM (SELECT a, b FROM t GROUP BY a, b) GROUP BY a; 总结: 如果分组统计的数据存在多个 DISTINCT 结果, 可以先将数值为空的数据占位处理, 分 SQL 统计数据, 然后将两组结果 UNION ALL 进行汇总结算. ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:5","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"数据膨胀引发的数据倾斜 在多维聚合计算时, 如果进行分组聚合的字段过多, 且数据量很大, map 端的聚合不能很好地起到数据压缩的情况下, 会导致 map 产出的数据急速膨胀, 这种情况容易导致作业内存溢出的异常. 如果 log 表含有数据倾斜 key, 会家具 Shuffle 过程的倾斜. -- 造成倾斜或内存溢出的情况 -- SQL 01 SELECT a, b, c, COUNT(1) FROM log GROUP BY a, b, c WITH ROLLUP; -- SQL 02 SELECT a, b, c, COUNT(1) FROM log GROUPING SETS a, b, c; -- 解决方案 -- 可以拆分上面的 SQL, 将 WITH ROLLUP 拆分成几个 SQL SELECT a, b, c sum(1) FROM ( SELECT a, b, c COUNT(1) FROM log GROUP BY a, b, c UNION ALL SELECT a, b, NULL, COUNT(1) FROM log GROUP BY a, b UNION ALL SELECT a, NULL, NULL, COUNT(1) FROM log GROUP BY a UNION ALL SELECT NULL, NULL, NULL COUNT(1) FROM log ) t; -- 但是这种方式不太友好, 因为这是对 3 个字段进行分组聚合, 但如果是更多的字段就很麻烦了. -- 在 Hive 中可以通过参数 hive.new.job.grouping.set.cardinality 配置的方式自动控制作业的拆解, 该参数默认值是 30 -- 该参数主要针对 GROUPING SETS/ROLLUPS/CUBES 这类多维聚合的操作生效, 如果最后拆解的键组合大于该值, 会启用新的任务去处理大于该值之外的组合. 如果在处理数据时, 某个分组聚合的列有较大的倾斜, 可以适当调小该值. set hive.new.job.grouping.set.cardinality=10; SELECT a, b, c, count(1) FROM log GROUP BY a, b, c, WITH ROLLUP; ","date":"2023-10-10","objectID":"/hive_skewed_data/:4:6","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["draft"],"content":"总结 说到底, 解决 reduce 端数据倾斜的必然途径就是让 map 端的输出数据更均匀地分布到 reduce 中. 在此过程中, 掌握四点可以更好的解决数据倾斜问题: 如果任务长时间卡在 99% 则基本可以认为是发生了数据倾斜, 建议调整参数以实现负载均衡: set hive.groupby.skewindata=true 小表关联大表操作, 需要先看能否使用子查询, 再看能否使用 MAPJOIN JOIN 操作注意关联字段不能出现大量的重复值或者空值 COUNT(DISTINCT id) 去重统计用慎用, 尽量通过其它方式替换 ","date":"2023-10-10","objectID":"/hive_skewed_data/:5:0","tags":["draft"],"title":"Hive 的数据倾斜问题","uri":"/hive_skewed_data/"},{"categories":["Network"],"content":" 最近和胖胖讨论的时候发现对于正向代理和反向代理的概念有点模糊. 所以整理做下梳理. ","date":"2023-08-15","objectID":"/proxy/:0:0","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"代理 代理就像是中介, 本来是 A 和 B 之间直接连接的, 但是此时添加了一个 C 在中间, A 和 B 现在需要通过 C 作为中介进行连接. 比如说是房产中介, 房东和租客就是通过中介来进行沟通的. ","date":"2023-08-15","objectID":"/proxy/:1:0","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"正向代理 正向代理是由客户端发起请求, 但是请求并不直接发送给目标服务器, 而是先发送给代理服务器, 由代理服务器代替客户端发送请求, 并将响应返回给客户端. 其实正向代理就是顺着请求的方向进行代理. 举个🌰: 想要访问油管, 但是直接访问是访问不到的. 这个时候就需要一个代理服务器, 我们去访问代理服务器, 然后代理服务器将我们的请求发送个油管, 然后再将油管的响应返回给我们. ","date":"2023-08-15","objectID":"/proxy/:2:0","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"作用 访问原来无法访问的资源 缓存 对客户端访问授权, 进行认证 记录用户访问记录, 对外隐藏用户信息 ","date":"2023-08-15","objectID":"/proxy/:2:1","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"反向代理 反向代理可以隐藏服务器的真实 IP, 同时可以将客户端的请求转发到多个服务器中的一个或多个, 用来提高网站的访问速度和安全性. ","date":"2023-08-15","objectID":"/proxy/:3:0","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"作用 保证内网安全, 组织 web 攻击 负载均衡 ","date":"2023-08-15","objectID":"/proxy/:3:1","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["Network"],"content":"区别与联系 正向代理和反向代理的区别在于代理的对象不同: 正向代理是代理客户端, 代理服务器代替客户端向目标服务器发送请求, 目标服务器不知道真正的请求方是谁. 反向代理是代理服务端, 代理服务器代替目标服务器向客户端发送响应, 客户端不知道真正的响应方是谁. ","date":"2023-08-15","objectID":"/proxy/:4:0","tags":["Network","Proxy"],"title":"正向代理和反向代理","uri":"/proxy/"},{"categories":["big data"],"content":"转载声明 本文大量内容系转载自以下文章，有删改，并参考其他文档资料加入了一些内容： 数据仓库和数据集市的区别 作者：修鹏李 出处：CSDN 大数据：数据仓库和数据库的区别 作者：南宫蓉 出处：简书 第一篇：数据仓库概述 第二篇：数据库关系建模 作者：穆晨 出处：CNBLOS 数据仓库、数据湖、数据集市、和数据中台的故事 作者：Murkey学习之旅 出处：csdn 数据中台和数仓的关系 作者：祝威廉 出处：CSDN 互联网巨头“大中台，小前台”战略 作者：于林富 出处：cnblogs 一文读懂阿里大中台、小前台战略 作者：民国周先生 出处：csdn 一文了解数据湖引擎 作者：WindyQin 出处：知乎 数据湖 | 一文读懂Data Lake的概念、特征、架构与案例 出处：大数据技术架构 ","date":"2023-07-25","objectID":"/data_summary/:1:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"背景 如今，随着诸如互联网以及物联网等技术的不断发展，越来越多的数据被生产出来-据统计，每天大约有超过2.5亿亿字节的各种各样数据产生。这些数据需要被存储起来并且能够被方便的分析和利用。 随着大数据技术的不断更新和迭代，数据管理工具得到了飞速的发展，相关概念如雨后春笋一般应运而生，如从最初决策支持系统(DSS)到商业智能(BI)、数据仓库、数据湖、数据中台等，这些概念特别容易混淆，本文对这些名词术语及内涵进行系统的解析，便于读者对数据平台相关的概念有全面的认识。 1 数据库 ","date":"2023-07-25","objectID":"/data_summary/:2:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"1.1 数据库 关系数据库本质上是一个二元关系，说的简单一些，就是一个二维表格，对普通人来说，最简单的理解就是一个Excel表格。这种数据库类型，具有结构化程度高，独立性强，冗余度低等等优点，一下子就促进了计算机的发展。 ","date":"2023-07-25","objectID":"/data_summary/:3:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"1.2 操作型数据库和分析型数据库 随着关系数据库理论的提出，诞生了一系列经典的RDBMS，如Oracle，MySQL，SQL Server等。这些RDBMS被成功推向市场，并为社会信息化的发展做出的重大贡献。然而随着数据库使用范围的不断扩大，它被逐步划分为两大基本类型： 操作型数据库 主要用于业务支撑。一个公司往往会使用并维护若干个操作型数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等； 分析型数据库 主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析； 那么为什么要\"分家\"？在一起不合适吗？能不能构建一个同样适用于操作和分析的统一数据库？答案是NO。一个显然的原因是它们会\"打架\"…如果操作型任务和分析型任务抢资源怎么办呢？再者，它们有太多不同，以致于早已\"貌合神离\"。接下来看看它们到底有哪些不同吧。 ","date":"2023-07-25","objectID":"/data_summary/:4:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"1.3 操作型数据库 VS 分析型数据库 因为主导功能的不同(面向操作/面向分析)，两类数据库就产生了很多细节上的差异。这就好像同样是人，但一个和尚和一个穆斯林肯定有很多行为/观念上的不同。 接下来本文将详细分析两类数据库的不同点： 数据组成差别 - 数据时间范围差别 一般来讲，操作型数据库只会存放90天以内的数据，而分析型数据库存放的则是数年内的数据。这点也是将操作型数据和分析型数据进行物理分离的主要原因。 数据组成差别 - 数据细节层次差别 操作型数据库存放的主要是细节数据，而分析型数据库中虽然既有细节数据，又有汇总数据，但对于用户来说，重点关注的是汇总数据部分。 操作型数据库中自然也有汇总需求，但汇总数据本身不存储而只存储其生成公式。这是因为操作型数据是动态变化的，因此汇总数据会在每次查询时动态生成。 而对于分析型数据库来说，因为汇总数据比较稳定不会发生改变，而且其计算量也比较大(因为时间跨度大)，因此它的汇总数据可考虑事先计算好，以避免重复计算。 数据组成差别 - 数据时间表示差别 操作型数据通常反映的是现实世界的当前状态；而分析型数据库既有当前状态，还有过去各时刻的快照，分析型数据库的使用者可以综合所有快照对各个历史阶段进行统计分析。 技术差别 - 查询数据总量和查询频度差别 操作型查询的数据量少而频率多，分析型查询则反过来，数据量大而频率少。要想同时实现这两种情况的配置优化是不可能的，这也是将两类数据库物理分隔的原因之一。 技术差别 - 数据更新差别 操作型数据库允许用户进行增，删，改，查；分析型数据库用户则只能进行查询。 技术差别 - 数据冗余差别 数据的意义是什么？就是减少数据冗余，避免更新异常。而如5所述，分析型数据库中没有更新操作。因此，减少数据冗余也就没那么重要了。 现在回到开篇是提到的第二个问题\"某大公司Hadoop Hive里的关系表不完全满足完整/参照性约束，也不完全满足范式要求，甚至第一范式都不满足。这种情况正常吗？\"，答曰是正常的。因为Hive是一种数据仓库，而数据仓库和分析型数据库的关系非常紧密(后文会讲到)。它只提供查询接口，不提供更新接口，这就使得消除冗余的诸多措施不需要被特别严格地执行了。 功能差别 - 数据读者差别 操作型数据库的使用者是业务环境内的各个角色，如用户，商家，进货商等；分析型数据库则只被少量用户用来做综合性决策。 功能差别 - 数据定位差别 这里说的定位，主要是指以何种目的组织起来。操作型数据库是为了支撑具体业务的，因此也被称为\"面向应用型数据库\"；分析型数据库则是针对各特定业务主题域的分析任务创建的，因此也被称为\"面向主题型数据库\"。 2 数据仓库 ","date":"2023-07-25","objectID":"/data_summary/:5:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.1 概述 数据仓库就是为了解决数据库不能解决的问题而提出的。那么数据库无法解决什么样的问题呢？这个我们得先说说什么是OLAP和OLTP。 ","date":"2023-07-25","objectID":"/data_summary/:6:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.2 OLTP和OLAP ","date":"2023-07-25","objectID":"/data_summary/:7:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.2.1 OLTP OLTP（OnLine Transaction Processing 联机事务处理） 。简单一些，就是数据库的增删查改。举个例子，你到银行，去取一笔钱出来，或者转账，或者只是想查一下你还有多少存款，这些都是面向“事务”类型的操作。这样的操作有几个显著的特点: 首先要求速度很快， 基本上都是高可靠的在线操作（比如银行）， 还有这些操作涉及的数据内容不会特别大（否则速度也就相应的降低）， 最后，“事务”型的操作往往都要求是精准操作，比如你去银行取款，必须要求一个具体的数字，你是不可能对着柜台员工说我大概想取400到500快之间吧，那样人家会一脸懵逼。 ","date":"2023-07-25","objectID":"/data_summary/:7:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.2.2 OLAP 这个东西又是上面发明关系型数据库的科德发明的。OLAP略有复杂，但这里我举一个简单的例子，大家就很容易理解了。 比如说，沃尔玛超市的数据库里有很多张表格，记录着各个商品的交易记录。超市里销售一种运动饮料，我们不妨称之为红牛。数据库中有一张表A，记录了红牛在一年的各个月份的销售额；还有一张表B，记录了红牛每个月在美国各个州的销售额：；甚至还有一张表C，记录了这家饮料公司在每个州对红牛饮料的宣传资金投入；甚至后来沃尔玛又从国家气象局拿到了美国各个州的一年365天每天的天气表。好，最后问题来了，请根据以上数据分析红牛在宣传资金不超过三百万的情况下，什么季节，什么天气，美国哪个州最好卖？凭借我们的经验，可能会得出，夏季的晴天，在美国的佛罗里达，最好卖，而且宣传资金投入越高销售额应该也会高。可能这样的结论是正确的，但决策者想要看到的是确凿的数据结论，而不是“可能”这样的字眼。 科学是不相信直觉的，如果我们人工进行手动分析，会发现这个要考虑的维度实在太多了，根本无法下手，何况这才四五个维度，要是更多了怎么办？OLAP就是为了解决这样的问题诞生的，但糟糕的是，传统数据库是无法满足OLAP所需要的数据信息的。 ","date":"2023-07-25","objectID":"/data_summary/:7:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3 数据仓库概念 ","date":"2023-07-25","objectID":"/data_summary/:8:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3.1 概述 数据库的大规模应用，使得信息行业的数据爆炸式的增长，为了研究数据之间的关系，挖掘数据隐藏的价值，人们越来越多的需要使用OLAP来为决策者进行分析，探究一些深层次的关系和信息。但很显然，不同的数据库之间根本做不到数据共享，就算同一家数据库公司，数据库之间的集成也存在非常大的挑战（最主要的问题是庞大的数据如何有效合并、存储）。 1988年，为解决企业的数据集成问题，IBM（卧槽，又是IBM）的两位研究员（Barry Devlin和Paul Murphy）创造性地提出了一个新的术语：数据仓库（Data Warehouse）。看到这里读者朋友们可能要问了，然后呢？然后…然后就没然后了。就在这个创世纪的术语诞生了之后，IBM就哑火了，只是将这个名词作为市场宣传的花哨概念，并没有在技术领域有什么实质性的研究和突破（可悲我大IBM=。=）。 然而，尽管IBM不为所动，其他企业却在加紧对数据仓库的研究和开发，大家都想在这个领域寻找到第一桶金。终于，到了1992年，后来被誉为“数据仓库之父”的比尔 恩门（Bill Inmon）给出了数据仓库的定义，二十多年后的今天他的定义依然没有被时代淘汰。我们来看看他是怎么定义的：数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理中的决策制定。 对于数据仓库的概念我们可以从两个层次予以理解： 首先,数据仓库用于支持决策,面向分析型数据处理,它不同于企业现有的操作型数据库; 其次,数据仓库是对多个异构的数据源有效集成,集成后按照主题进行了重组,并包含历史数据,而且存放在数据仓库中的数据一般不再修改。 我们可以不用管这个定义，简单的理解，其实就是我们为了进行OLAP，把分布在各个散落独立的数据库孤岛整合在了一个数据结构里面，称之为数据仓库。 这个数据仓库在技术上是怎么建立的读者朋友们并不需要关心，但是我们要知道，原来各个数据孤岛中的数据，可能会在物理位置（比如沃尔玛在各个州可能都有自己的数据中心）、存储格式（比如月份是数值类型，但但天气可能是字符类型）、商业平台（不同数据库可能用的是Oracle数据库，有的是微软SQL Server数据库）、编写的语言（Java或者Scale等）等等各个方面完全不同，数据仓库要做的工作就是将他们按照所需要的格式提取出来，再进行必要的转换（统一数据格式）、清洗（去掉无效或者不需要的数据）等，最后装载进数据仓库（我们所说的ETL工具就是用来干这个的）。这样，拿我们上面红牛的例子来说，所有的信息就统一放在了数据仓库中了。 自从数据仓库出现之后，信息产业就开始从以关系型数据库为基础的运营式系统慢慢向决策支持系统发展。这个决策支持系统，其实就是我们现在说的商务智能（Business Intelligence）即BI。 可以这么说，数据仓库为OLAP解决了数据来源问题，数据仓库和OLAP互相促进发展，进一步驱动了商务智能的成熟，但真正将商务智能赋予“智能”的，正是我们现在热谈的下一代技术：数据挖掘。 ","date":"2023-07-25","objectID":"/data_summary/:8:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3.2 数据仓库特点 面向主题 面向主题特性是数据仓库和操作型数据库的根本区别。 操作型数据库是为了支撑各种业务而建立， 而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；所谓主题：是指用户使用数据仓库进行决策时所关心的重点方面，如：收入、客户、销售渠道等；所谓面向主题，是指数据仓库内的信息是按主题进行组织的，而不是像业务支撑系统那样是按照业务功能进行组织的。 集成性 集成性是指数据仓库会将不同源数据库中的数据汇总到一起； 具体来说，是指数据仓库中的信息不是从各个业务系统中简单抽取出来的，而是经过一系列加工、整理和汇总的过程，因此数据仓库中的信息是关于整个企业的一致的全局信息。 企业范围 数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来； 历史性 较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年； 时变性 时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告； 数据仓库内的信息并不只是反映企业当前的状态，而是记录了从过去某一时点到当前各个阶段的信息。通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。 ","date":"2023-07-25","objectID":"/data_summary/:8:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3.3 数据仓库与BI 数据仓库平台逐步从BI报表为主到分析为主、到预测为主、再到操作智能为目标。 从过去报表发生了什么—\u003e分析为什么过去会发生—-\u003e将来会发生什么—-\u003e什么正在发生—–\u003e让正确的事情发生 商务智能（BI，Business Intelligence）是一种以提供决策分析性的运营数据为目的而建立的信息系统。 是属于在线分析处理：On Line Analytical Processing(OLAP)，将预先计算完成的汇总数据，储存于魔方数据库(Cube) 之中，针对复杂的分析查询，提供快速的响应。 在前10年，BI报表项目比较多，是数据仓库项目的前期预热项目（主要分析为主的阶段，是数据仓库的初级阶段），制作一些可视化报表展现给管理者: 它利用信息科技，将分散于企业内、外部各种数据加以整合并转换成知识，并依据某些特定的主题需求，进行决策分析和运算； 用户则通过报表、图表、多维度分析的方式，寻找解决业务问题所需要的方案； 这些结果将呈报给决策者，以支持策略性的决策和定义组织绩效，或者融入智能知识库自动向客户推送。 ","date":"2023-07-25","objectID":"/data_summary/:8:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3.4 数据仓库系统作用和定位 数据仓库系统的作用能实现跨业务条线、跨系统的数据整合，为管理分析和业务决策提供统一的数据支持。数据仓库能够从根本上帮助你把公司的运营数据转化成为高价值的可以获取的信息（或知识），并且在恰当的时候通过恰当的方式把恰当的信息传递给恰当的人。 是面向企业中、高级管理进行业务分析和绩效考核的数据整合、分析和展现的工具； 是主要用于历史性、综合性和深层次数据分析； 数据来源是ERP（例:SAP）系统或其他业务系统； 能够提供灵活、直观、简洁和易于操作的多维查询分析; 不是日常交易操作系统，不能直接产生交易数据。 传统离线数据仓库针对实时数据处理，非结构化数据处理能力较弱，以及在业务在预警预测方面应用相对有限。 但现在已经开始兴起实时数仓。 ","date":"2023-07-25","objectID":"/data_summary/:8:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.3.5 数据仓库能提供什么 ","date":"2023-07-25","objectID":"/data_summary/:8:5","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.4 数据仓库组件 数据仓库的核心组件有四个：业务系统各源数据库，ETL，数据仓库，前端应用。如下图所示： 业务系统 业务系统包含各种源数据库，这些源数据库既为业务系统提供数据支撑，同时也作为数据仓库的数据源(注：除了业务系统，数据仓库也可从其他外部数据源获取数据)； ETL 数据仓库会周期不断地从源数据库提取清洗好了的数据，因此也被称为\"目标系统\"。ETL分别代表： 提取extraction 表示从操作型数据库搜集指定数据 转换transformation 表示将数据转化为指定格式，并进行数据清洗保证数据质量 加载load 加载过程表示将转换过后满足指定格式的数据加载进数据仓库。 前端应用 和操作型数据库一样，数据仓库通常提供具有直接访问数据仓库功能的前端应用，这些应用也被称为BI(商务智能)应用。 数据仓库系统除了包含分析产品本身之外，还包含数据集成、数据存储、数据计算、门户展现、平台管理等其它一系列的产品。 ","date":"2023-07-25","objectID":"/data_summary/:9:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5 数据仓库开发流程 ","date":"2023-07-25","objectID":"/data_summary/:10:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.1 概述 数据仓库的开发流程和数据库的比较相似，因此本文仅就其中区别进行分析。 下图为数据仓库的开发流程： ","date":"2023-07-25","objectID":"/data_summary/:10:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.2 数据仓库需求 需求搜集是所有环节中最重要的一步，吃透了用户需求，往往就成功了大半。这些需求将指导后面如需求建模、实现、以及前端应用程序开发等。通常来说，需求都会通过ER图来表示(参考数据库需求与ER建模)，并和各业务方讨论搜集得到，最终整理成文档。 要特别强调的一点是数据仓库系统开发需求阶段过程是循环迭代式的，一开始的需求集并不大，但随着项目的进展，需求会越来越多。而且不论是以上哪个阶段发生了需求变动，整个流程都需要重新走一遍，决不允许隐式变更需求。 比如为一个学生选课系统进行ER建模，得到如下结果： ","date":"2023-07-25","objectID":"/data_summary/:10:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.3 数据仓库建模 也就是逻辑模型建模，可参考第二篇：数据库关系建模 ER建模环节完成后，需求就被描述成了ER图。之后，便可根据这个ER图设计相应的关系表了。 但从ER图到具体关系表的建立还需要经过两个步骤：1. 逻辑模型设计 2. 物理模型设计。其中前者将ER图映射为逻辑意义上的关系表，后者则映射为物理意义上的关系表。 逻辑意义上的关系表可以理解为单纯意义上的关系表，它不涉及到表中字段数据类型，索引信息，触发器等等细节信息。 概念模型 VS 逻辑模型 我们首先可以认为【概念模型建模和ER建模，需求可视化】表达的是一个意思。在这个环节中，数据开发人员绘制ER图，并和项目各方人员协同需求，达成一致。由于这部分的工作涉及到的人员开发能力比较薄弱，甚至不懂开发，因此ER图必须清晰明了，不能涉及到过多的技术细节，比如：要给多对多联系/多值属性等多建一张表，要设置外码，各种复合主码等，它们应当对非开发人员透明。而且ER图中每个属性只会出现一次，减少了蕴含的信息量，是更好的交流和文档化工具。在ER图绘制完毕之后，才开始将它映射为关系表。这个映射的过程，就叫做逻辑模型建模或者关系建模。 还有，ER模型所蕴含的信息，也没有全部被逻辑模型包含。比如联系的自定义基数约束，比如实体的复合属性，派生属性，用户的自定义约束等等。因此ER模型在整个开发流程(如物理模型建模，甚至前端开发)中是都会用到的，不能认为ER模型转换到逻辑模型后就可以扔一边了。 逻辑模型 物理模型 逻辑模型设计好后，就可以开始着手数据仓库的物理实现了，他也被称为物理模型建模，这个阶段不但需要参照逻辑模型，还应当参照ER图。 ","date":"2023-07-25","objectID":"/data_summary/:10:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.4 数据仓库实现 这一步的本质就是在空的数据仓库里实现2种前面创建的关系模型，一般通过使用SQL或者提供的前端工具实现。 ","date":"2023-07-25","objectID":"/data_summary/:10:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.5 开发前端应用程序 前端应用开发在需求搜集好了之后就开始进行，主要有网站、APP等前端形式。另外前端程序的实际实现涉及到和数据仓库之间交互，因此这一步的最终完成在数据库建模之后。 ","date":"2023-07-25","objectID":"/data_summary/:10:5","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.6 ETL工程 较之数据库系统开发流程，数据仓库开发只多出ETL工程部分。然而这一部分极有可能是整个数据仓库开发流程中最为耗时耗资源的一个环节。因为该环节要整理各大业务系统中杂乱无章的数据并协调元数据上的差别，所以工作量很大。在很多公司都专门设有ETL工程师这样的岗位，大的公司甚至专门聘请ETL专家。 ","date":"2023-07-25","objectID":"/data_summary/:10:6","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.7 数据仓库部署 顾名思义，这一步就是部署数据库系统的软硬件环境。数据库部署往往还包含将初始数据填入数据库中的意思。对于云数据仓库，这一步就叫\"数据上云\"。 ","date":"2023-07-25","objectID":"/data_summary/:10:7","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.8 数据仓库使用 这一步没啥多讲的，就再讲一个有关的故事吧。同样是在A公司，有一次某政企私有云项目完成后，我们有人被派去给他们培训如何使用。结果去的人回来后说政企意见很大，认为让他们学习SQL以外的东西都不行。拒绝用Python写UDF，更拒绝MR编程接口，只要SQL和图形界面操作方式。一开始我对政企的这种行为有点看不起，但后来我想，就是因为有这群挑剔的用户，才使得A公司云产品的易用性如此强大，从而占领国内云计算的大部分市场。用户的需求才是技术的唯一试金石。 ","date":"2023-07-25","objectID":"/data_summary/:10:8","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.5.9 数据库管理和维护 严格来讲，这部分不算开发流程，属于数据库系统开发完成后的工作。 ","date":"2023-07-25","objectID":"/data_summary/:10:9","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.6 数据仓库系统管理 数据仓库系统发行后，控制权便从数据仓库设计、实现、部署的团队移交给了数据仓库管理员，并由他们来对系统进行管理，涵盖了确保一个已经部署的数据仓库系统正确运行的各种行为。为了实现这一目标，具体包含以下范畴： ","date":"2023-07-25","objectID":"/data_summary/:11:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"2.7 数据质量体系 数据仓库系统需要重视数据质量问题。用一句话概括，数据质量就是衡量数据能否真实、及时反映客观世界的指标。具体来说，数据质量包含以下几大指标： 准确性 准确性要求数据能够正确描述客观世界。比如某用户姓名拼音mu chen错误的录入成了muc hen，就应该弹出警告语； 唯一性（视情况而定） 唯一性要求数据不能被重复录入，或者不能有两个几乎相同的关系。比如张三李四在不同业务环境下分别建立了近乎相同的关系，这时应将这两个关系合并； 完整性 完整性要求进行数据搜集时，需求数据的被描述程度要高。比如一个用户的购买记录中，必然要有支付金额这个属性；规则验证。 一致性 一致性要求不同关系、或者同一关系不同字段的数据意义不发生冲突。 比如某关系中昨天存货量字段+当天进货量字段-当天销售量字段等于当天存货量就可能是数据质量有问题； 及时性 及时性要求数据库系统中的数据\"保鲜\"。比如当天的购买记录当天就要入库； 统一性 统一性要求数据格式统一。比如nike这个品牌，不能有的字段描述为\"耐克\"，而有的字段又是\"奈克\"； 小结 数据质量和数据具体意义有很大相关性，因此无法单凭理论来保证。且由于具体业务及真实世界的复杂性，数据质量问题必然会存在，不可能完全预防得了。因此很多公司都提供了数据质量工程服务/软件，用来识别和校正数据库系统中的各种数据质量问题。 3 数据集市 Bill Inmon说过一句话叫“IT经理们面对最重要的问题就是到底先建立数据仓库还是先建立数据集市”，足以说明搞清楚这两者之间的关系是十分重要而迫切的！通常在考虑建立数据仓库之前，会涉及到如下一些问题： 采取自上而下还是自下而上的设计方法 企业范围还是部门范围 先建立数据仓库还是数据集市 建立领航系统还是直接实施 数据集市是否相互独立 数据集市可以理解为是一种\"小型数据仓库\"，它只包含单个主题，且关注范围也非全局。 数据集市可以分为两种: 一种是独立数据集市(independent data mart)，这类数据集市有自己的源数据库和ETL架构； 另一种是非独立数据集市(dependent data mart)，这种数据集市没有自己的源系统，它的数据来自数据仓库。当用户或者应用程序不需要/不必要/不允许用到整个数据仓库的数据时，非独立数据集市就可以简单为用户提供一个数据仓库的子集。 4 数据湖 ","date":"2023-07-25","objectID":"/data_summary/:12:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.1 概述 Pentaho首席技术官James Dixon创造了“数据湖”一词。它把数据集市描述成一瓶水（清洗过的，包装过的和结构化易于使用的）。 而数据湖更像是在自然状态下的水，数据流从源系统流向这个湖。用户可以在数据湖里校验，取样或完全的使用数据。 这个也是一个不精确的定义。数据湖还有以下特点： 从源系统导入所有的数据，没有数据流失。 数据存储时没有经过转换或只是简单的处理。 数据转换和定义schema 用于满足分析需求。 数据湖为什么叫数据湖而不叫数据河或者数据海？一个有意思的回答是： “河”强调的是流动性，“海纳百川”，河终究是要流入大海的，而企业级数据是需要长期沉淀的，因此叫“湖”比叫“河”要贴切； 同时，湖水天然是分层的，满足不同的生态系统要求，这与企业建设统一数据中心，存放管理数据的需求是一致的，“热”数据在上层，方便应用随时使用；温数据、冷数据位于数据中心不同的存储介质中，达到数据存储容量与成本的平衡。 不叫“海”的原因在于，海是无边无界的，而“湖”是有边界的，这个边界就是企业/组织的业务边界；因此数据湖需要更多的数据管理和权限管理能力。 叫“湖”的另一个重要原因是数据湖是需要精细治理的，一个缺乏管控、缺乏治理的数据湖最终会退化为“数据沼泽”，从而使应用无法有效访问数据，使存于其中的数据失去价值。 ","date":"2023-07-25","objectID":"/data_summary/:13:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.2 数据湖定义 ","date":"2023-07-25","objectID":"/data_summary/:14:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.2.1 维基百科对数据湖的定义 数据湖（Data Lake)是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。数据湖是以其自然格式存储的数据的系统或存储库，通常是对象blob或文件。 数据湖通常是企业所有数据的单一存储，包括源系统数据的原始副本，以及用于报告、可视化、分析和机器学习等任务的转换数据。 数据湖从企业的多个数据源获取原始数据，并且针对不同的目的，同一份原始数据还可能有多种满足特定内部模型格式的数据副本。因此，数据湖中被处理的数据可能是任意类型的信息，从结构化数据到完全非结构化数据。 企业对数据湖寄予厚望，希望它能帮助用户快速获取有用信息，并能将这些信息用于数据分析和机器学习算法，以获得与企业运行相关的洞察力。 数据湖可以包括: 来自关系数据库（行和列）的结构化数据 半结构化数据（CSV，日志，XML，JSON） 非结构化数据（电子邮件，文档，PDF）和二进制数据（图像，音频，视频）。 目前，HDFS是最常用的部署数据湖的技术，所以很多人会觉得数据湖就是HDFS集群。数据湖是一个概念，而HDFS是用于实现这个概念的技术。 ","date":"2023-07-25","objectID":"/data_summary/:14:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.2.2 AWS对数据湖的定义 AWS定义数据湖是一个集中式存储库，允许您以任意规模存储所有结构化和非结构化数据。 A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions. 数据湖是一个集中式存储库，允许您以任意规模存储所有结构化和非结构化数据。您可以按原样存储数据（无需先对数据进行结构化处理），并运行不同类型的分析 – 从控制面板和可视化到大数据处理、实时分析和机器学习，以指导做出更好的决策。 ","date":"2023-07-25","objectID":"/data_summary/:14:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.2.3 微软对数据湖的定义 微软的定义就更加模糊了，并没有明确给出什么是Data Lake，而是取巧的将数据湖的功能作为定义，数据湖包括一切使得开发者、数据科学家、分析师能更简单的存储、处理数据的能力，这些能力使得用户可以存储任意规模、任意类型、任意产生速度的数据，并且可以跨平台、跨语言的做所有类型的分析和处理。 Azure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages. It removes the complexities of ingesting and storing all of your data while making it faster to get up and running with batch, streaming, and interactive analytics. Azure Data Lake works with existing IT investments for identity, management, and security for simplified data management and governance. It also integrates seamlessly with operational stores and data warehouses so you can extend current data applications. We’ve drawn on the experience of working with enterprise customers and running some of the largest scale processing and analytics in the world for Microsoft businesses like Office 365, Xbox Live, Azure, Windows, Bing, and Skype. Azure Data Lake solves many of the productivity and scalability challenges that prevent you from maximizing the value of your data assets with a service that’s ready to meet your current and future business needs. Azure的数据湖包括一切使得开发者、数据科学家、分析师能更简单的存储、处理数据的能力，这些能力使得用户可以存储任意规模、任意类型、任意产生速度的数据，并且可以跨平台、跨语言的做所有类型的分析和处理。数据湖在能帮助用户加速应用数据的同时，消除了数据采集和存储的复杂性，同时也能支持批处理、流式计算、交互式分析等。数据湖能同现有的数据管理和治理的IT投资一起工作，保证数据的一致、可管理和安全。它也能同现有的业务数据库和数据仓库无缝集成，帮助扩展现有的数据应用。Azure数据湖吸取了大量企业级用户的经验，并且在微软一些业务中支持了大规模处理和分析场景，包括Office 365, Xbox Live, Azure, Windows, Bing和Skype。Azure解决了许多效率和可扩展性的挑战，作为一类服务使得用户可以最大化数据资产的价值来满足当前和未来需求。 ","date":"2023-07-25","objectID":"/data_summary/:14:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.2.4 数据湖定义小结 数据湖需要提供足够用的数据存储能力 这个存储保存了一个企业/组织中的所有数据。 数据湖可以存储海量的任意类型的数据 包括结构化、半结构化和非结构化数据。 数据湖中的数据是原始数据，是业务数据的完整副本。 数据湖中的数据保持了他们在业务系统中原来的样子。 数据湖需要具备完善的数据管理能力（完善的元数据） 可以管理各类数据相关的要素，包括数据源、数据格式、连接信息、数据schema、权限管理等。 数据湖需要具备多样化的分析能力 包括但不限于批处理、流式计算、交互式分析以及机器学习；同时，还需要提供一定的任务调度和管理能力。 数据湖需要具备完善的数据生命周期管理能力。 不光需要存储原始数据，还需要能够保存各类分析处理的中间结果，并完整的记录数据的分析处理过程，能帮助用户完整详细追溯任意一条数据的产生过程。 数据湖需要具备完善的数据获取和数据发布能力。 数据湖需要能支撑各种各样的数据源，并能从相关的数据源中获取全量/增量数据；然后规范存储。数据湖能将数据分析处理的结果推送到合适的存储引擎中，满足不同的应用访问需求。 对于大数据的支持，包括超大规模存储以及可扩展的大规模数据处理能力。 综上，个人认为数据湖应该是一种不断演进中、可扩展的大数据存储、处理、分析的基础设施；以数据为导向，实现任意来源、任意速度、任意规模、任意类型数据的全量获取、全量存储、多模式处理与全生命周期管理；并通过与各类外部异构数据源的交互集成，支持各类企业级应用。 ","date":"2023-07-25","objectID":"/data_summary/:14:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3 数据湖的处理架构 ","date":"2023-07-25","objectID":"/data_summary/:15:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3.1 概述 数据湖引擎介于管理数据系统、分析可视化和数据处理工具之间。数据湖引擎不是将数据从数据源移动到单个存储库，而是部署在现有数据源和数据使用者的工具(如BI工具和数据科学平台)之上。 BI分析工具，如Tableau、Power BI、R、Python和机器学习模型，是为数据生活在一个单一的、高性能的关系数据库中的环境而设计的。然而，多数组织使用不同的数据格式和不同的技术在多种解决方案中管理他们的数据。多数组织现在使用一个或多个非关系型数据存储，如云存储(如S3、ADLS)、Hadoop和NoSQL数据库(如Elasticsearch、Cassandra)。 当数据存储在一个独立的高性能关系数据库中时，BI工具、数据科学系统和机器学习模型可以很好运用这部分数据。然而，就像我们上面所说的一样，数据这并不是存在一个地方。因此，我们通常应用自定义ETL开发来集成来自不同系统的数据，以便于我们后续分析。通常分析技术栈分为以下几类： ODS 数据从不同的数据库转移到单一的存储区域，如云存储服务(如Amazon S3、ADLS)、HDFS。 数据仓库 虽然可以在Hadoop和云存储上直接执行SQL查询，但是这些系统的设计目的并不是提供交互性能。因此，数据的子集通常被加载到关系数据仓库或MPP数据库中，也就是构建数据仓库。 数据集市 为了在大型数据集上提供交互性能，必须通过在OLAP系统中构建多维数据集或在数据仓库中构建物化聚合表对数据进行预聚合 这种多层体系架构带来了许多挑战。例如： 灵活性，比如数据源的变化或新的数据需求，必须重新访问数据仓库每一层，以确保后续应用人员来使用，可能会花费较长的实施周期。 复杂性，数据分析人员必须了解所有存储数据的查询语法，增加了不必要的复杂性。 技术成本，该架构需要广泛的定制ETL开发、DBA专业知识和数据工程来满足业务中不断发展的数据需求。 基础设施成本，该架构需要大量的专有技术，并且通常会导致存储在不同系统中的数据产生许多副本。 数据治理，该架构如果血缘关系搞的不好，便使得跟踪、维护变得非常困难。 数据及时性，在ETL的过程中需要时间，所以一般数据是T-1的统计汇总。 数据湖引擎采用了一种不同的方法来支持数据分析。数据湖引擎不是将数据移动到单个存储库中，而是在数据原本存储的地方访问数据，并动态地执行任何必要的数据转换和汇总。此外，数据湖引擎还提供了一个自助服务模型，使数据使用者能够使用他们喜欢的工具(如Power BI、Tableau、Python和R)探索、分析数据，而不用关心数据在哪存、结构如何。 有些数据源可能不适合分析处理，也无法提供对数据的有效访问。数据湖引擎提供了优化数据物理访问的能力。有了这种能力，可以在不改变数据使用者访问数据的方式和他们使用的工具的情况下优化各个数据集。 与传统的解决方案相比，数据湖引擎使用多种技术使数据消费者能够访问数据，并集成这些技术功能到一个自助服务的解决方案中。 数据湖可以认为是新一代的大数据基础设施。为了更好的理解数据湖的基本架构，我们先来看看大数据基础设施架构的演进过程。 ","date":"2023-07-25","objectID":"/data_summary/:15:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3.2 第一阶段-以Hadoop为代表的离线数据处理基础设施 数据湖可以认为是新一代的大数据基础设施。为了更好的理解数据湖的基本架构，我们先来看看大数据基础设施架构的演进过程。 如下图所示，Hadoop是以HDFS为核心存储，以MapReduce（简称MR）为基本计算模型的批量数据处理基础设施。 围绕HDFS和MR，产生了一系列的组件，不断完善整个大数据平台的数据处理能力，例如面向在线KV操作的HBase、面向SQL的HIVE、面向工作流的PIG等。同时，随着大家对于批处理的性能要求越来越高，新的计算模型不断被提出，产生了Tez、Spark、Presto、Flink等计算引擎，MR模型也逐渐进化成DAG模型。 DAG模型一方面增加计算模型的抽象并发能力：对每一个计算过程进行分解，根据计算过程中的聚合操作点对任务进行逻辑切分，任务被切分成一个个的stage，每个stage都可以有一个或者多个Task组成，Task是可以并发执行的，从而提升整个计算过程的并行能力； 另一方面，为减少数据处理过程中的中间结果写文件操作，Spark、Presto等计算引擎尽量使用计算节点的内存对数据进行缓存，从而提高整个数据过程的效率和系统吞吐能力。 ","date":"2023-07-25","objectID":"/data_summary/:15:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3.3 第二阶段：lambda架构 随着数据处理能力和处理需求的不断变化，越来越多的用户发现，批处理模式无论如何提升性能，也无法满足一些实时性要求高的处理场景，流式计算引擎应运而生，例如Storm、Spark Streaming、Flink等。 然而，随着越来越多的应用上线，大家发现，其实批处理和流计算配合使用，才能满足大部分应用需求；而对于用户而言，其实他们并不关心底层的计算模型是什么，用户希望无论是批处理还是流计算，都能基于统一的数据模型来返回处理结果，于是Lambda架构被提出，如下图所示。 Lambda架构的核心理念是“流批一体”，如上图所示，整个数据流向自左向右流入平台。进入平台后一分为二，一部分走批处理模式，一部分走流式计算模式。无论哪种计算模式，最终的处理结果都通过统一服务层对应用提供，确保访问的一致性，底层到底是批或流对用户透明。 ","date":"2023-07-25","objectID":"/data_summary/:15:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3.4 第三阶段：Kappa架构 Lambda架构虽然解决了应用读取数据的统一性问题，但是“流批分离”的处理链路增大了研发的复杂性。因此，有人就提出能不能用一套系统来解决所有问题。目前比较流行的做法就是基于流计算来做。流计算天然的分布式特征，注定了他的扩展性更好。通过加大流计算的并发性，加大流式数据的“时间窗口”，来统一批处理与流式处理两种计算模式。 ","date":"2023-07-25","objectID":"/data_summary/:15:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.3.5 大数据基础设施架构小结 综上，从传统的hadoop架构往lambda架构，从lambda架构往Kappa架构的演进，大数据平台基础架构的演进逐渐囊括了应用所需的各类数据处理能力，大数据平台逐渐演化成了一个企业/组织的全量数据处理平台。当前的企业实践中，除了关系型数据库依托于各个独立的业务系统；其余的数据，几乎都被考虑纳入大数据平台来进行统一的处理。 然而，目前的大数据平台基础架构，都将视角锁定在了存储和计算，而忽略了对于数据的资产化管理，这恰恰是数据湖作为新一代的大数据基础设施所重点关注的方向之一。 大数据基础架构的演进，其实反应了一点：在企业/组织内部，数据是一类重要资产已经成为了共识；为了更好的利用数据，企业/组织需要对数据资产进行如下操作： 进行长期的原样存储，以便可回溯重放原始数据 进行有效管理与集中治理； 提供多模式的计算能力满足处理需求； 以及面向业务，提供统一的数据视图、数据模型与数据处理结果。 数据湖就是在这个大背景下产生的，除了有大数据平台所拥有的各类基础能力之外，数据湖更强调对于数据的管理、治理和资产化能力。 落到具体的实现上，数据湖需要包括一系列的数据管理组件，包括： 数据接入； 数据搬迁； 数据治理； 数据质量管理； 资产目录； 访问控制； 任务管理； 任务编排； 元数据管理等。 如下图所示，给出了一个数据湖系统的参考架构。 对于一个典型的数据湖而言，它与大数据平台相同的地方在于它也具备处理超大规模数据所需的存储和计算能力，能提供多模式的数据处理能力；增强点在于数据湖提供了更为完善的数据管理能力，具体体现在： 更强大的数据接入能力。 数据接入能力体现在对于各类外部异构数据源的定义管理能力，以及对于外部数据源相关数据的抽取迁移能力，抽取迁移的数据包括外部数据源的元数据与实际存储的数据。 更强大的数据管理能力。 管理能力具体又可分为基本管理能力和扩展管理能力： 基本管理能力包括对各类元数据的管理、数据访问控制、数据资产管理，是一个数据湖系统所必须的，后面我们会在“各厂商的数据湖解决方案”一节相信讨论各个厂商对于基本管理能力的支持方式。 扩展管理能力包括任务管理、流程编排以及与数据质量、数据治理相关的能力。任务管理和流程编排主要用来管理、编排、调度、监测在数据湖系统中处理数据的各类任务，通常情况下，数据湖构建者会通过购买/研制定制的数据集成或数据开发子系统/模块来提供此类能力，定制的系统/模块可以通过读取数据湖的相关元数据，来实现与数据湖系统的融合。而数据质量和数据治理则是更为复杂的问题，一般情况下，数据湖系统不会直接提供相关功能，但是会开放各类接口或者元数据，供有能力的企业/组织与已有的数据治理软件集成或者做定制开发。 可共享的元数据。 数据湖中的各类计算引擎会与数据湖中的数据深度融合，而融合的基础就是数据湖的元数据。 好的数据湖系统，计算引擎在处理数据时，能从元数据中直接获取数据存储位置、数据格式、数据模式、数据分布等信息，然后直接进行数据处理，而无需进行人工/编程干预。更进一步，好的数据湖系统还可以对数据湖中的数据进行访问控制，控制的力度可以做到“库表列行”等不同级别。 还有一点应该指出的是，前面数据湖系统的参考架构图的集中式存储更多的是业务概念上的集中，本质上是希望一个企业/组织内部的数据能在一个明确统一的地方进行沉淀。事实上，数据湖的存储应该是一类可按需扩展的分布式文件系统，大多数数据湖实践中也是推荐采用S3/OSS/OBS/HDFS等分布式系统作为数据湖的统一存储。 我们可以再切换到数据维度，从数据生命周期的视角来看待数据湖对于数据的处理方式，数据在数据湖中的整个生命周期如下图所示。理论上，一个管理完善的数据湖中的数据会永久的保留原始数据，同时过程数据会不断的完善、演化，以满足业务的需要。 ","date":"2023-07-25","objectID":"/data_summary/:15:5","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.4 数据湖能给企业带来多种能力 数据湖能给企业带来多种能力，例如，能实现数据的集中式管理，在此之上，企业能挖掘出很多之前所不具备的能力。 另外，数据湖结合先进的数据科学与机器学习技术，能帮助企业构建更多优化后的运营模型，也能为企业提供其他能力，如预测分析、推荐模型等，这些模型能刺激企业能力的后续增长。数据湖能从以下方面帮助到企业： 实现数据治理（data governance）； 通过应用机器学习与人工智能技术实现商业智能； 预测分析，如领域特定的推荐引擎； 信息追踪与一致性保障； 根据对历史的分析生成新的数据维度； 有一个集中式的能存储所有企业数据的数据中心，有利于实现一个针对数据传输优化的数据服务； 帮助组织或企业做出更多灵活的关于企业增长的决策。 ","date":"2023-07-25","objectID":"/data_summary/:16:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5 数据湖与数据仓库区别 ","date":"2023-07-25","objectID":"/data_summary/:17:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.1 概述 对于数据仓库与数据湖的不同之处，你可以想象一下仓库和湖泊的区别：仓库存储着来自特定来源的货物，而湖泊的水来自河流、溪流和其他来源，并且是原始数据。 数据仓库供应商包括AWS、Cloudera、IBM、谷歌、微软、甲骨文、Teradata、SAP、SnapLogic和Snowflake等。数据湖提供商包括AWS、谷歌、Informatica、微软、Teradata等。 ","date":"2023-07-25","objectID":"/data_summary/:17:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.2 数据湖保留全部的数据 存储范围 数据仓库开发期间，大量的时间花费在分析数据源，理解商业处理和描述数据。结果就是为报表设计高结构化的数据模型。这一过程大部分的工作就是来决定数据应不应该导入数据仓库。通常情况下，如果数据不能满足指定的问题，就不会导入到数据仓库。这么做是为了简化数据模型和节省数据存储空间。 相反，数据湖保留所有的数据。不仅仅是当前正在使用的数据，甚至不被用到的数据也会导进来。数据会一直被保存所有我们可以回到任何时间点来做分析。 因为数据湖使用的硬件与数据仓库的使用的不同，使这种方法成为了可能。现成的服务器与便宜的存储相结合，使数据湖扩展到TB级和PB级非常经济。 存储来源 数据仓库主要存储来自运营系统的大量数据 而数据湖则存储来自更多来源的数据，包括来自企业的运营系统和其他来源的各种原始数据资产集。 ","date":"2023-07-25","objectID":"/data_summary/:17:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.3 数据湖支持所有数据类型 在储存方面上，数据湖中数据为非结构化的，所有数据都保持原始形式，并且仅在分析时再进行转换。 数据仓库一般由从事务系统中提取的数据组成，并由定量度量和描述它们的属性组成。 诸如Web服务器日志，传感器数据，社交网络活动，文本和图像等非传统数据源在很大程度上被忽略。 这些数据类型的新用途不断被发现，但是消费和存储它们可能是昂贵和困难的。 数据湖方法包含这些非传统数据类型。 在数据湖中，我们保留所有数据，而不考虑源和结构。 我们保持它的原始形式，并且只有在我们准备好使用它时才会对其进行转换。 这种方法被称为“读时模式”。 数据仓库则是捕获结构化数据并将其按模式组织。 ","date":"2023-07-25","objectID":"/data_summary/:17:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.4 适用人群 由于数据湖中的数据可能不准确，并且可能来自企业运营系统之外的来源，因此不是很适合普通的业务分析用户;数据湖更适合数据科学家和其他数据分析专家，使用他们需要的非常庞大和多样化的数据集。 其他用户则可以使用更为结构化的数据视图如数据仓库来提供他们使用的数据，数据仓库非常适用于月度报告等操作用途，因为它具有高度结构化。 ","date":"2023-07-25","objectID":"/data_summary/:17:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.5 数据湖很容易适应变化 关于数据仓库的主要抱怨之一是需要多长时间来改变它们。在开发过程中花费大量时间来获得仓库的结构。一个好的仓库设计可以适应变化，但由于数据加载过程的复杂性以及为简化分析和报告所做的工作，这些更改必然会消耗一些开发人员资源并需要一些时间。 许多业务问题都迫不及待地让数据仓库团队适应他们的系统来回答问题。日益增长的对更快答案的需求促成了自助式商业智能的概念。 另一方面，在数据湖中，由于所有数据都以其原始形式存储，并且始终可供需要使用它的人访问，因此用户有权超越仓库结构以新颖方式探索数据并回答它们问题在他们的步伐。 如果一个探索的结果被证明是有用的并且有重复的愿望，那么可以应用更正式的模式，并且可以开发自动化和可重用性来帮助将结果扩展到更广泛的受众。如果确定结果无用，则可以丢弃该结果，并且不会对数据结构进行任何更改，也不会消耗开发资源。 所以，在架构方面： 数据湖通常在存储数据之后定义架构，使用较少的初始工作并提供更大的灵活性。 在数据仓库中存储数据之前定义架构。 ","date":"2023-07-25","objectID":"/data_summary/:17:5","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.5.6 数据湖支持快速洞察数据 最后的区别实际上是其他区别结果。由于数据湖包含所有数据和数据类型，因为它使用户能够在数据转换，清理和结构化之前访问数据，从而使用户能够比传统数据仓库方法更快地获得结果。 但是，这种对数据的早期访问是有代价的。通常由数据仓库开发团队完成的工作可能无法完成分析所需的部分或全部数据源。这让驾驶座位的用户可以根据需要探索和使用数据，但上述第一层业务用户可能不希望这样做。他们仍然只想要他们的报告和KPI。 在数据湖中，这些操作报告的使用者将利用更加结构化的数据湖中数据的结构视图，这些视图与数据仓库中以前一直存在的数据相似。不同之处在于，这些视图主要存在于位于湖泊中的数据之上的元数据，而不是需要开发人员更改的物理刚性表格。 ","date":"2023-07-25","objectID":"/data_summary/:17:6","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.6 数据湖和数据仓库理解误区 误解一：数据仓库和数据湖二者在架构上只能二选一 很多人认为数据仓库和数据湖在架构上只能二选一，其实这种理解是错误的。数据湖和数据仓库并不是对立关系，相反它们的并存可以互补给企业架构带来更多的好处： 数据仓库存储结构化的数据，适用于快速的BI和决策支撑， 而数据湖可以存储任何格式的数据，往往通过挖掘能够发挥出数据的更大作为。 所以在一些场景上二者的并存是可以给企业带来更多效益的。 误解二：相对于数据湖，数据仓库更有名更受欢迎 人工智能（AI）和机器学习项目的成功往往需要数据湖来做支撑。因为数据湖可让您存储几乎任何类型的数据而无需先准备或清理，所以可以保留尽可能多的潜在价值。而数据仓库存储的数据都是经过清洗，往往会丢失一些有价值的信息。 数据仓库虽然是这两种中比较知名的，但是随着数据挖掘需求的发展，数据湖的受欢迎程度可能会继续上升。数据仓库对于某些类型的工作负载和用例工作良好，而数据湖则是为其他类型的工作负载提供服务的另一种选择。 误解三：数据仓库易于使用，而数据湖却很复杂 确实，数据湖需要数据工程师和数据科学家的特定技能，才能对存储在其中的数据进行分类和利用。数据的非结构化性质使那些不完全了解数据湖如何工作的人更难以访问它。 但是，一旦数据科学家和数据工程师建立了数据模型或管道，业务用户就可以利用建立的数据模型以及流行的业务工具（定制或预先构建）的来访问和分析数据，而不在乎该数据存储在数据仓库中还是数据湖中。 ","date":"2023-07-25","objectID":"/data_summary/:18:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.7 数据湖建设的基本过程 个人认为数据湖是比传统大数据平台更为完善的大数据处理基础支撑设施，完善在数据湖是更贴近客户业务的技术存在。所有数据湖所包括的、且超出大数据平台存在的特性，例如元数据、数据资产目录、权限管理、数据生命周期管理、数据集成和数据开发、数据治理和质量管理等，无一不是为了更好的贴近业务，更好的方便客户使用。数据湖所强调的一些基本的技术特性，例如弹性、存储计算独立扩展、统一的存储引擎、多模式计算引擎等等，也是为了满足业务需求，并且给业务方提供最具性价比的TCO。 数据湖的建设过程应该与业务紧密结合；但是数据湖的建设过程与传统的数据仓库，甚至是大热的数据中台应该是有所区别的。区别在于，数据湖应该以一种更敏捷的方式去构建，“边建边用，边用边治理”。为了更好的理解数据湖建设的敏捷性，我们先来看一下传统数仓的构建过程。业界对于传统数仓的构建提出了“自下而上”和“自顶而下”两种模式，分别由Inmon和KimBall两位大牛提出。具体的过程就不详述了，不然可以再写出几百页，这里只简单阐述基本思想。 1）Inmon提出自下而上（EDW-DM）的数据仓库建设模式，即操作型或事务型系统的数据源，通过ETL抽取转换和加载到数据仓库的ODS层；ODS层中的数据，根据预先设计好的EDW（企业级数据仓库）范式进行加工处理，然后进入到EDW。EDW一般是企业/组织的通用数据模型，不方便上层应用直接做数据分析；因此，各个业务部门会再次根据自己的需要，从EDW中处理出数据集市层（DM）。 优势：易于维护，高度集成；劣势：结构一旦确定，灵活性不足，且为了适应业务，部署周期较长。此类方式构造的数仓，适合于比较成熟稳定的业务，例如金融。 2）KimBall提出自顶而下（DM-DW）的数据架构，通过将操作型或事务型系统的数据源，抽取或加载到ODS层；然后通过ODS的数据，利用维度建模方法建设多维主题数据集市（DM）。各个DM，通过一致性的维度联系在一起，最终形成企业/组织通用的数据仓库。 优势：构建迅速，最快的看到投资回报率，敏捷灵活；劣势：作为企业资源不太好维护，结构复杂，数据集市集成困难。常应用于中小企业或互联网行业。 其实上述只是一个理论上的过程，其实无论是先构造EDW，还是先构造DM，都离不开对于数据的摸底，以及在数仓构建之前的数据模型的设计，包括当前大热的“数据中台”，都逃不出下图所示的基本建设过程。 1） 数据摸底。对于一个企业/组织而言，在构建数据湖初始工作就是对自己企业/组织内部的数据做一个全面的摸底和调研，包括数据来源、数据类型、数据形态、数据模式、数据总量、数据增量等。在这个阶段一个隐含的重要工作是借助数据摸底工作，进一步梳理企业的组织结构，明确数据和组织结构之间关系。为后续明确数据湖的用户角色、权限设计、服务方式奠定基础。 2） 模型抽象。针对企业/组织的业务特点梳理归类各类数据，对数据进行领域划分，形成数据管理的元数据，同时基于元数据，构建通用的数据模型。 3） 数据接入。根据第一步的摸排结果，确定要接入的数据源。根据数据源，确定所必须的数据接入技术能力，完成数据接入技术选型，接入的数据至少包括：数据源元数据、原始数据元数据、原始数据。各类数据按照第二步形成的结果，分类存放。 4） 融合治理。简单来说就是利用数据湖提供的各类计算引擎对数据进行加工处理，形成各类中间数据/结果数据，并妥善管理保存。数据湖应该具备完善的数据开发、任务管理、任务调度的能力，详细记录数据的处理过程。在治理的过程中，会需要更多的数据模型和指标模型。 5） 业务支撑。在通用模型基础上，各个业务部门定制自己的细化数据模型、数据使用流程、数据访问服务。 上述过程，对于一个快速成长的互联网企业来说，太重了，很多情况下是无法落地的，最现实的问题就是第二步模型抽象，很多情况下，业务是在试错、在探索，根本不清楚未来的方向在哪里，也就根本不可能提炼出通用的数据模型；没有数据模型，后面的一切操作也就无从谈起，这也是很多高速成长的企业觉得数据仓库/数据中台无法落地、无法满足需求的重要原因之一。 数据湖应该是一种更为“敏捷”的构建方式，我们建议采用如下步骤来构建数据湖。 对比数据仓库/数据中台建设基本流程，依然是五步，但是这五步是一个全面的简化和“可落地”的改进。 1） 数据摸底。依然需要摸清楚数据的基本情况，包括数据来源、数据类型、数据形态、数据模式、数据总量、数据增量。但是，也就需要做这么多了。数据湖是对原始数据做全量保存，因此无需事先进行深层次的设计。 2） 技术选型。根据数据摸底的情况，确定数据湖建设的技术选型。事实上，这一步也非常的简单，因为关于数据湖的技术选型，业界有很多的通行的做法，基本原则个人建议有三个：“计算与存储分离”、“弹性”、“独立扩展”。建议的存储选型是分布式对象存储系统（如S3/OSS/OBS）；计算引擎上建议重点考虑批处理需求和SQL处理能力，因为在实践中，这两类能力是数据处理的关键，关于流计算引擎后面会再讨论一下。无论是计算还是存储，建议优先考虑serverless的形式；后续可以在应用中逐步演进，真的需要独立资源池了，再考虑构建专属集群。 3） 数据接入。确定要接入的数据源，完成数据的全量抽取与增量接入。 4） 应用治理。这一步是数据湖的关键，我个人把“融合治理”改成了“应用治理”。从数据湖的角度来看，数据应用和数据治理应该是相互融合、密不可分的。从数据应用入手，在应用中明确需求，在数据ETL的过程中，逐步形成业务可使用的数据；同时形成数据模型、指标体系和对应的质量标准。数据湖强调对原始数据的存储，强调对数据的探索式分析与应用，但这绝对不是说数据湖不需要数据模型；恰恰相反，对业务的理解与抽象，将极大的推动数据湖的发展与应用，数据湖技术使得数据的处理与建模，保留了极大的敏捷性，能快速适应业务的发展与变化。 从技术视角来看，数据湖不同于大数据平台还在于数据湖为了支撑数据的全生命周期管理与应用，需要具备相对完善的数据管理、类目管理、流程编排、任务调度、数据溯源、数据治理、质量管理、权限管理等能力。在计算能力上，目前主流的数据湖方案都支持SQL和可编程的批处理两种模式（对机器学习的支持，可以采用Spark或者Flink的内置能力）；在处理范式上，几乎都采用基于有向无环图的工作流的模式，并提供了对应的集成开发环境。对于流式计算的支持，目前各个数据湖解决方案采取了不同的方式。在讨论具体的方式之前，我们先对流计算做一个分类： 1） 模式一：实时模式。这种流计算模式相当于对数据采用“来一条处理一条”/“微批”的方式进行处理；多见于在线业务，如风控、推荐、预警等。 2） 模式二：类流式。这种模式需要获取指定时间点之后变化的数据/读取某一个版本的数据/读取当前的最新数据等，是一种类流式的模式；多见于数据探索类应用，如分析某一时间段内的日活、留存、转化等。 二者的本质不同在于，模式一处理数据时，数据往往还没有存储到数据湖中，仅仅是在网路/内存中流动；模式二处理数据时，数据已经存储到数据湖中了。综上，我个人建议采用如下图模式： 如图所示，在需要数据湖具备模式一的处理能力时，还是应该引入类Kafka中间件，作为数据转发的基础设施。完整的数据湖解决方案方案应该提供将原始数据导流至Kafka的能力。流式引擎具备从类Kafka组件中读取数据的能力。流式计算引擎在处理数据过后，根据需要，可以将结果写入OSS/RDBMS/NoSQL/DW，供应用访问。某种意义上，模式一的流计算引擎并非一定要作为数据湖不可分割的一部分存在，只需要在应用需要时，能够方便的引入即可。但是，这里需要指出的是： 1）流式引擎依然需要能够很方便的读取数据湖的元数据； 2）流式引擎任务也需要统一的纳入数据湖的任务管理； 3）流式处理任务依然需要纳入到统一的权限管理中。 对于模式二，本质上更接近于批处理。现在许多经典的大数据组件已经提供了支持方式，如HUDI/IceBerg/Delta等，均支持Spark、Presto等经典的计算引擎。以HUDI为例，通过支持特殊类型的表（COW/MOR），提供访问快照数据（指定版本）、增量数据、准实时数据的能力。目前AWS、腾讯等已经将HUDI集成到了其EMR服务中，阿里云的DLA也正在计划推出DLA on HUDI的能力。 让我们再回到本文开头的第一章，我们说过，数据湖的主要用户是数据科学家和数据分析师，探索式分析和机器学习是这类人群的常见操作；流式计算（实时模式）多用于在线业务，严格来看，并非数据湖目标用户的刚需。但是，流式计算（实时模式）是目前大多数互联网公司在线业务的重要组成部分，而数据湖作为企业/组织内部的数据集中存放地，需要在架构上保持一定的扩展能力，可以很方便的进行扩展，整合流式计算能力。 5） 业务支撑。虽然大多数数据湖解决方案都对外提供标准的访问接口，如JDBC，市面上流行的各类BI报表工具、大屏工具也都可以直接访问数据湖中的数据。但是在实际的应用中，我们还是建议将数据湖处理好的数据推送到对应的各类支持在线业务的数据引擎中去，能够让应用有更好的体验。 ","date":"2023-07-25","objectID":"/data_summary/:19:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8 主流厂商数据湖解决方案 ","date":"2023-07-25","objectID":"/data_summary/:20:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.1 AWS数据湖解决方案 整个方案基于AWS Lake Formation构建，AWS Lake Formation本质上是一个管理性质的组件，它与其他AWS服务互相配合，来完成整个企业级数据湖构建功能。上图自左向右，体现了数据流入、数据沉淀、数据计算、数据应用四个步骤。我们进一步来看其关键点： 数据流入 数据流入是整个数据湖构建的起始，包括元数据的流入和业务数据流入两个部分。 元数据流入包括数据源创建、元数据抓取两步，最终会形成数据资源目录，并生成对应的安全设置与访问控制策略。解决方案提供专门的组件，获取外部数据源的相关元信息，该组件能连接外部数据源、检测数据格式和模式（schema），并在对应的数据资源目录中创建属于数据湖的元数据。 业务数据的流入是通过ETL来完成的。 在具体的产品形式上，元数据抓取、ETL和数据准备AWS将其单独抽象出来，形成了一个产品叫AWS GLUE。AWS GLUE与AWS Lake Formation共享同一个数据资源目录，在AWS GLUE官网文档上明确指出：“Each AWS account has one AWS Glue Data Catalog per AWS region”。 对于异构数据源的支持。AWS提供的数据湖解决方案，支持S3、AWS关系型数据库、AWS NoSQL数据库，AWS利用GLUE、EMR、Athena等组件支持数据的自由流动。 数据沉淀 采用Amazon S3作为整个数据湖的集中存储，按需扩展/按使用量付费。 数据计算 整个解决方案利用AWS GLUE来进行基本的数据处理。GLUE基本的计算形式是各类批处理模式的ETL任务，任务的出发方式分为手动触发、定时触发、事件触发三种。不得不说，AWS的各类服务在生态上实现的非常好，事件触发模式上，可以利用AWS Lambda进行扩展开发，同时触发一个或多个任务，极大的提升了任务触发的定制开发能力；同时，各类ETL任务，可以通过CloudWatch进行很好的监控。 数据应用。 在提供基本的批处理计算模式之外，AWS通过各类外部计算引擎，来提供丰富的计算模式支持，例如通过Athena/Redshift来提供基于SQL的交互式批处理能力；通过EMR来提供各类基于Spark的计算能力，包括Spark能提供的流计算能力和机器学习能力。 权限管理 AWS的数据湖解决方案通过Lake Formation来提供相对完善的权限管理，粒度包括“库-表-列”。但是，有一点例外的是，GLUE访问Lake Formation时，粒度只有“库-表”两级；这也从另一个侧面说明，GLUE和Lake Formation的集成是更为紧密的，GLUE对于Lake Formation中的数据有更大的访问权限。 Lake Formation的权限进一步可以细分为数据资源目录访问权限和底层数据访问权限，分别对应元数据与实际存储的数据。实际存储数据的访问权限又进一步分为数据存取权限和数据存储访问权限： 数据存取权限类似于数据库中对于库表的访问权限 数据存储权限则进一步细化了对于S3中具体目录的访问权限（分为显示和隐式两种）。如下图所示，用户A在只有数据存取的权限下，无法创建位于S3指定bucket下的表。 个人认为这进一步体现了数据湖需要支持各种不同的存储引擎，未来的数据湖可能不只S3/OSS/OBS/HDFS一类核心存储，可能根据应用的访问需求，纳入更多类型的存储引擎，例如，S3存储原始数据，NoSQL存储处理过后适合以“键值”模式访问的数据，OLAP引擎存储需要实时出各类报表/adhoc查询的数据。虽然当前各类材料都在强调数据湖与数据仓库的不同；但是，从本质上，数据湖更应该是一类融合的数据管理思想的具体实现，“湖仓一体化”也很可能是未来的一个发展趋势。 综上，AWS数据湖方案成熟度高，特别是元数据管理、权限管理上考虑充分，打通了异构数据源与各类计算引擎的上下游关系，让数据能够自由“移动”起来。 在流计算和机器学习上，AWS的解决方案也比较完善： 流计算方面AWS推出了专门的流计算组件Kinesis，Kinesis中的Kinesis data Firehose服务可以创建一个完全被托管的数据分发服务，通过Kinesis data Stream实时处理的数据，可以借助Firehose方便的写入S3中，并支持相应的格式转换，如将JSON转换成Parquet格式。 AWS整个方案最牛的地方还在与Kinesis可以访问GLUE中的元数据，这一点充分体现了AWS数据湖解决方案在生态上的完备性。 同样，在机器学习方面，AWS提供了SageMaker服务，SageMaker可以读取S3中的训练数据，并将训练好的模型回写至S3中。但是，有一点需要指出的是，在AWS的数据湖解决方案中，流计算和机器学习并不是固定捆绑的，只是作为计算能力扩展，能方便的集成。 最后，让我们回到数据湖组件参考架构，看看AWS的数据湖解决方案的组件覆盖情况，参见下图 AWS 数据湖解决方案在参考架构中的映射。 综上，AWS的数据湖解决方案覆盖了除质量管理和数据治理的所有功能。其实质量管理和数据治理这个工作和企业的组织结构、业务类型强相关，需要做大量的定制开发工作，因此通用解决方案不囊括这块内容，也是可以理解的。事实上，现在也有比较优秀的开源项目支持这个项目，比如Apache Griffin，如果对质量管理和数据治理有强诉求，可以自行定制开发。 ","date":"2023-07-25","objectID":"/data_summary/:20:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.2 华为数据湖解决方案 华为的数据湖解决方案相关信息来自华为官网。目前官网可见的相关产品包括数据湖探索（Data Lake Insight，DLI）和智能数据湖运营平台（DAYU)： 其中DLI相当于是AWS的Lake Formation、GLUE、Athena、EMR（Flink\u0026Spark）的集合。官网上没找到关于DLI的整体架构图，我根据自己的理解，尝试画了一个，主要是和AWS的解决方案有一个对比，所以形式上尽量一致。 华为的数据湖解决方案比较完整，DLI承担了所有的数据湖构建、数据处理、数据管理、数据应用的核心功能。DLI最大的特色是在于分析引擎的完备性，包括基于SQL的交互式分析以及基于Spark+Flink的流批一体处理引擎。在核心存储引擎上，DLI依然通过内置的OBS来提供，和AWS S3的能力基本对标。华为数据湖解决方案在上下游生态上做的比AWS相对完善，对于外部数据源，几乎支持所有目前华为云上提供的数据源服务。 DLI可以与华为的CDM（云数据迁移服务）和DIS（数据接入服务）对接：1）借助DIS，DLI可以定义各类数据点，这些点可以在Flink作业中被使用，做为source或者sink；2）借助CDM，DLI甚至能接入IDC、第三方云服务的数据。 为了更好的支持数据集成、数据开发、数据治理、质量管理等数据湖高级功能，华为云提供了DAYU平台。DAYU平台是华为数据湖治理运营方法论的落地实现。DAYU涵盖了整个数据湖治理的核心流程，并对其提供了相应的工具支持；甚至在华为的官方文档中，给出了数据治理组织的构建建议。DAYU的数据治理方法论的落地实现如下图所示（来自华为云官网）。 可以看到，本质上DAYU数据治理的方法论其实是传统数据仓库治理方法论在数据湖基础设施上的延伸：从数据模型来看，依然包括贴源层、多源整合层、明细数据层，这点与数据仓库完全一致。根据数据模型和指标模型会生成质量规则和转换模型，DAYU会和DLI对接，直接调用DLI提供的相关数据处理服务，完成数据治理。华为云整个的数据湖解决方案，完整覆盖了数据处理的生命周期，并且明确支持了数据治理，并提供了基于模型和指标的数据治理流程工具，在华为云的数据湖解决方案中逐渐开始往“湖仓一体化”方向演进。 ","date":"2023-07-25","objectID":"/data_summary/:20:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.3 阿里云数据湖解决方案 阿里云上数据类产品众多，因为本人目前在数据BU，所以本节方案将关注在如何使用数据库BU的产品来构建数据湖，其他云上产品会略有涉及。阿里云的基于数据库产品的数据湖解决方案更加聚焦，主打数据湖分析和联邦分析两个场景。阿里云数据湖解决方案如下图所示。 整个方案依然采用OSS作为数据湖的集中存储。在数据源的支持上，目前也支持所有的阿里云数据库，包括OLTP、OLAP和NoSQL等各类数据库。核心关键点如下： 数据接入与搬迁。在建湖过程中，DLA的Formation组件具备元数据发现和一键建湖的能力，在本文写作之时，目前“一键建湖”还只支持全量建湖，但是基于binlog的增量建湖已经在开发中了，预计近期上线。增量建湖能力会极大的增加数据湖中数据的实时性，并将对源端业务数据库的压力降到最下。这里需要注意的是，DLA Formation是一个内部组件，对外并没有暴露。 数据资源目录。DLA提供Meta data catalog组件对于数据湖中的数据资产进行统一的管理，无论数据是在“湖中”还是在“湖外”。Meta data catalog也是联邦分析的统一元数据入口。 在内置计算引擎上，DLA提供了SQL计算引擎和Spark计算引擎两种。无论是SQL还是Spark引擎，都和Meta data catalog深度集成，能方便的获取元数据信息。基于Spark的能力，DLA解决方案支持批处理、流计算和机器学习等计算模式。 在外围生态上，除了支持各类异构数据源做数据接入与汇聚之外，在对外访问能力上，DLA与云原生数据仓库（原ADB）深度整合。一方面，DLA处理的结果可之际推送至ADB中，满足实时、交互式、ad hoc复杂查询；另一方面，ADB里的数据也可以借助外表功能，很方便的进行数据回流至OSS中。基于DLA，阿里云上各类异构数据源可以完全被打通，数据自由流动。 在数据集成和开发上，阿里云的数据湖解决方案提供两种选择：一种是采用dataworks完成；另一种是采用DMS来完成。无论是选择哪种，都能对外提供可视化的流程编排、任务调度、任务管理能力。在数据生命周期管理上，dataworks的数据地图能力相对更加成熟。 在数据管理和数据安全上，DMS提供了强大的能力。DMS的数据管理粒度分为“库-表-列-行”，完善的支持企业级的数据安全管控需求。除了权限管理之外，DMS更精细的地方是把原来基于数据库的devops理念扩展到了数据湖，使得数据湖的运维、开发更加精细化。 进一步细化整个数据湖方案的数据应用架构，如下图所示。 自左向右从数据的流向来看，数据生产者产生各类数据（云下/云上/其他云)，利用各类工具，上传至各类通用/标准数据源，包括OSS/HDFS/DB等。针对各类数据源，DLA通过数据发现、数据接入、数据迁移等能力，完整建湖操作。对于“入湖”的数据，DLA提供基于SQL和Spark的数据处理能力，并可以基于Dataworks/DMS，对外提供可视化的数据集成和数据开发能力；在对外应用服务能力上，DLA提供标准化的JDBC接口，可以直接对接各类报表工具、大屏展示功能等。阿里云的DLA的特色在于背靠整个阿里云数据库生态，包括OLTP、OLAP、NoSQL等各类数据库，对外提供基于SQL的数据处理能力，对于传统企业基于数据库的开发技术栈而言，转型成本相对较低，学习曲线比较平缓。 阿里云的DLA解决方案的另一个特色在于“基于云原生的湖仓一体化”。传统的企业级数据仓库在大数据时代的今天，在各类报表应用上依然是无法替代的；但是数仓无法满足大数据时代的数据分析处理的灵活性需求；因此，我们推荐数据仓库应该作为数据湖的上层应用存在：即数据湖是原始业务数据在一个企业/组织中唯一官方数据存储地；数据湖根据各类业务应用需求，将原始数据进行加工处理，形成可再次利用的中间结果；当中间结果的数据模式（Schema）相对固定后，DLA可以将中间结果推送至数据仓库，供企业/组织开展基于数仓的业务应用。阿里云在提供DLA的同时，还提供了云原生数仓（原ADB），DLA和云原生数仓在以下两点上深度融合。 1） 使用同源的SQL解析引擎。DLA的SQL与ADB的SQL语法上完全兼容，这意味着开发者使用一套技术栈即能同时开发数据湖应用和数仓应用。 2） 都内置了对于OSS的访问支持。OSS直接作为DLA的原生存储存在；对于ADB而言，可以通过外部表的能力，很方便的访问OSS上的结构化数据。借助外部表，数据可以自由的在DLA和ADB之间流转，做到真正的湖仓一体。 DLA+ADB的组合真正做到了云原生的湖仓一体（关于什么是云原生，不在本文的讨论范畴）。本质上，DLA可以看成一个能力扩展的数据仓库贴源层。与传统数仓相比，该贴源层：（1）可以保存各类结构化、半结构化和非结构化数据；（2）可以对接各类异构数据源；（3）具备元数据发现、管理、同步等能力；（4）内置的SQL/Spark计算引擎具备更强的数据处理能力，满足多样化的数据处理需求；（5）具备全量数据的全生命周期管理能力。基于DLA+ADB的湖仓一体化方案，将同时覆盖“大数据平台+数据仓库”的处理能力。 DLA还有一个重要能力是构建了一个“四通八达”的数据流动体系，并以数据库的体验对外提供能力，无论数据在云上还是云下，无论数据在组织内部还是外部；借助数据湖，各个系统之间的数据不再存在壁垒，可以自由的流进流出；更重要的是，这种流动是受监管的，数据湖完整的记录了数据的流动情况。 ","date":"2023-07-25","objectID":"/data_summary/:20:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.4 Microsoft Azure数据湖解决方案 Azure的数据湖解决方案包括数据湖存储、接口层、资源调度与计算引擎层，如下图所示（来自Azure官网）。 存储层是基于Azure object Storage构建的，依然是对结构化、半结构化和非结构化数据提供支撑。 接口层为WebHDFS，比较特别的是在Azure object Storage实现了HDFS的接口，Azure把这个能力称为“数据湖存储上的多协议存取”。 在资源调度上，Azure基于YARN实现。 计算引擎上，Azure提供了U-SQL、hadoop和Spark等多种处理引擎。 ​ 开发工具的支持 与visual studio的深度集成；Azure推荐使用U-SQL作为数据湖分析应用的开发语言。Visual studio为U-SQL提供了完备的开发环境；同时，为了降低分布式数据湖系统开发的复杂性，visual studio基于项目进行封装，在进行U-SQL开发时，可以创建“U-SQL database project”，在此类项目中，利用visual studio，可以很方便的进行编码与调试，同时，也提供向导，将开发好的U-SQL脚本发布到生成环境。U-SQL支持Python、R进行扩展，满足定制开发需求。 多计算引擎的适配： SQL, Apache Hadoop和Apache Spark。这里的hadoop包括Azure提供的HDInsight（Azure托管的Hadoop服务），Spark包括Azure Databricks。- 多种不同引擎任务之间的自动转换能力。微软推荐U-SQL为数据湖的缺省开发工具，并提供各类转换工具，支持U-SQL脚本与Hive、Spark（HDSight\u0026databricks）、Azure Data Factory data Flow之间的转化。 ","date":"2023-07-25","objectID":"/data_summary/:20:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.5 腾讯云 云端数智新引擎，腾讯云原生数据湖计算重磅发布 ","date":"2023-07-25","objectID":"/data_summary/:20:5","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.8.6 小结 本文所讨论的是数据湖的解决方案，不会涉及到任何云厂商的单个产品。我们从数据接入、数据存储、数据计算、数据管理、应用生态几个方面，简单做了一个类似下表的总结。 出于篇幅关系，其实知名云厂商的数据湖解决方案还有谷歌和腾讯的。这两家从其官方网站上看，数据湖解决方案相对来讲比较简单，也仅仅是一些概念上的阐述，推荐的落地方案是“oss+hadoop（EMR)”。其实数据湖不应该从一个简单的技术平台视角来看，实现数据湖的方式也多种多样，评价一个数据湖解决方案是否成熟，关键应该看其提供的数据管理能力，具体包括但不限于元数据、数据资产目录、数据源、数据处理任务、数据生命周期、数据治理、权限管理等；以及与外围生态的对接打通能力。 ","date":"2023-07-25","objectID":"/data_summary/:20:6","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.9 典型的数据湖应用案例 ","date":"2023-07-25","objectID":"/data_summary/:21:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.9.1 广告数据分析 近年来，流量获取的成本就越来越高，线上渠道获客成本的成倍增长让各行各业都面临着严峻的挑战。在互联网广告成本不断攀升的大背景下，以花钱买流量拉新为主要的经营策略必然行不通了。流量前端的优化已成强弩之末，利用数据工具提高流量到站后的目标转化，精细化运营广告投放的各个环节，才是改变现状更为直接有效的方式。说到底，要提高广告流量的转化率，必须依靠大数据分析。 为了能够提供更多的决策支撑依据，需要采取更多的埋点数据的收集和分析，包括但不限于渠道、投放时间、投放人群，以点击率为数据指标进行数据分析，从而给出更好的、更迅速的方案和建议，实现高效率高产出。因此，面对广告投放领域多维度、多媒体、多广告位等结构化、半结构化和非结构化数据采集、存储、分析和决策建议等要求，数据湖分析产品解决方案在广告主或者发布商进行新一代技术选型中上受到了很热烈的青睐。 DG是一家全球领先的企业国际化智能营销服务商，基于先进的广告技术、大数据和运营能力，为客户提供全球高质量用户获取及流量变现服务。DG从成立之初就决定以公有云为基础来构建其IT基础设施，最初DG选择了AWS云平台，主要将其广告数据在S3中以数据湖的形态进行存放，通过Athena进行交互式分析。然而随着互联网广告的飞速发展，广告行业带来了几大挑战，移动广告的发布与追踪系统必须解决几个关键问题： 1） 并发性与峰值问题。在广告行业，流量高峰时常出现，瞬间的点击量可能达到数万，甚至数十万，这就要求系统具备非常好的可扩展性以快速响应和处理每一次点击 2） 如何实现对海量数据的实时分析。为了监控广告投放效果，系统需要实时对用户的每一次点击和激活数据进行分析，同时把相关数据传输到下游的媒体； 3） 平台的数据量在急剧增长，每天的业务日志数据在持续的产生和上传，曝光、点击、推送的数据在持续处理，每天新增的数据量已经在10-50TB左右，对整个数据处理系统提出了更高的要求。如何高效地完成对广告数据的离线/近实时统计，按照广告客户的维度要求进行聚合分析。 针对上述三点业务挑战，同时DG这个客户日增量数据正在急剧变大（当前日数据扫描量达到100+TB），继续在AWS平台使用遇到Athena读取S3数据带宽瓶颈、数据分析滞后时间越来越长、为应对数据和分析需求增长而急剧攀升的投入成本等，经过认真、仔细的测试和分析，最终决定从AWS云平台全量搬站到阿里云平台，新架构图如下： 从AWS搬站到阿里云后，我们为该客户设计了“利用Data Lake Analytics + OSS”极致分析能力来应对业务波峰波谷。一方面轻松应对来自品牌客户的临时分析。另一方面利用Data Lake Analytics的强大计算能力，分析按月、季度广告投放，精确计算出一个品牌下面会有多少个活动，每个活动分媒体，分市场，分频道，分DMP的投放效果，进一步增强了加和智能流量平台为品牌营销带来的销售转化率。并且在广告投放与分析的总拥有成本上，Data Lake Analytics提供的Serverless的弹性服务为按需收费，不需要购买固定的资源，完全契合业务潮汐带来的资源波动，满足弹性的分析需求，同时极大地降低了运维成本和使用成本。 总体上，DG从AWS切换到阿里云后，极大地节省了硬件成本、人力成本和开发成本。由于采用DLA serverless云服务，DG无需先期投入大量的资金去购买服务器、存储等硬件设备，也无需一次性购买大量的云服务，其基础设施的规模完全是按需扩展：需求高的时候增加服务数量，需求减少的时候减少服务数量，提高了资金的利用率。使用阿里云平台带来的第二个显著好处是性能的提升。在DG业务的快速增长期以及后续多条业务线接入期，DG在移动广告系统的访问量经常呈爆发式增长，然而原先AWS方案和平台在Athena读取S3数据遇到数据读取带宽的极大瓶颈，数据分析的时间变得越来越长，阿里云DLA联合OSS团队等进行了极大的优化和改造，同时，DLA数据库分析在计算引擎上（与TPC-DS打榜世界第一的AnalyticDB共享计算引擎）比Presto原生计算引擎的能力提升数十倍性能，也极大的为DG提升了分析性能。 ","date":"2023-07-25","objectID":"/data_summary/:21:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.9.2 游戏运营分析 数据湖是一类TCO表现极其优秀的大数据基础设施。对于很多快速增长的游戏公司而言，一个爆款游戏，往往在短期内相关数据增长极快；同时，公司的研发人员的技术栈很难在短期内与数据的增量和增速进行匹配；此时，呈爆发增长的数据很难被有效利用。数据湖是一个解决此类问题的技术选择。 YJ是一家高速成长的游戏公司，公司希望能依托相关用户行为数据进行深入分析，指导游戏的开发和运营。数据分析背后的核心逻辑在于随着游戏行业市场竞争局面的扩大，玩家对于品质的要求越来越高，游戏项目的生命周期越来越短，直接影响项目的投入产出比，通过数据运营则可以有效的延长项目的生命周期，对各个阶段的业务走向进行精准把控。而随着流量成本的日益上升，如何构建经济、高效的精细化数据运营体系，以更好的支撑业务发展，也变得愈发重要起来。数据运营体系就需要有其配套的基础支撑设施，如何选择这类基础支撑设施，是公司技术决策者需要思考的问题。思考的出发点包括： 1） 要有足够的弹性。对于游戏而言，往往就是短时间爆发，数据量激增；因此，能否适应数据的爆发性增长，满足弹性需求是一个重点考量的点；无论是计算还是存储，都需要具备足够的弹性。 2） 要有足够的性价比。对于用户行为数据，往往需要拉到一个很长的周期去分析去对比，比如留存率，不少情况下需要考虑90天甚至180天客户的留存率；因此，如何以最具性价比的方式长期存储海量数据是需要重点考虑的问题。 3） 要有够用的分析能力，且具备可扩展性。许多情况下，用户行为体现在埋点数据中，埋点数据又需要与用户注册信息、登陆信息、账单等结构化数据关联分析；因此，在数据分析上，至少需要有大数据的ETL能力、异构数据源的接入能力和复杂分析的建模能力。 4） 要与公司现有技术栈相匹配，且后续利于招聘。对于YJ，其在技术选型的时候一个重要点就是其技术人员的技术栈，YJ的技术团队大部分只熟悉传统的数据库开发，即MySQL；并且人手紧张，做数据运营分析的技术人员只有1个，短时间内根本没有能力独立构建大数据分析的基础设施。从YJ的角度出发，最好绝大多数分析能够通过SQL完成；并且在招聘市场上，SQL开发人员的数量也远高于大数据开发工程师的数量。针对客户的情况，我们帮助客户对现有方案做了改造。 改造前，客户所有的结构化数据都在一个高规格的MySQL里面；而玩家行为数据则是通过LogTail采集至日志服务（SLS）中，然后从日志服务中分别投递到OSS和ES里。这个架构的问题在于：1）行为数据和结构化数据完全割裂，无法联动分析；2）对于行为数据智能提供检索功能，无法做深层次的挖掘分析；3）OSS仅仅作为数据存储资源使用，并没有挖掘出足够的数据价值。 事实上，我们分析客户现存架构其实已经具备了数据湖的雏形：全量数据已经在OSS中保存下来了，现在需要进一步补齐客户对于OSS中的数据的分析能力。而且数据湖基于SQL的数据处理模式也满足客户对于开发技术栈的需求。综上，我们对客户的架构做了如下调整，帮助客户构建了数据湖。 总体上，我们没有改变客户的数据链路流转，只是在OSS的基础上，增加了DLA组件，对OSS的数据进行二次加工处理。DLA提供了标准SQL计算引擎，同时支持接入各类异构数据源。基于DLA对OSS的数据进行处理后，生成业务直接可用的数据。但是DLA的问题在于无法支撑低延迟需求的交互式分析场景，为了解决这个问题，我们引入了云原生数据仓库ADB来解决交互式分析的延迟性问题；同时，在最前端引入QuickBI作为客户的可视化分析工具。YJ方案是图14所示的湖仓一体化解决方案在游戏行业的一个经典落地案例。 YM是一家数据智能服务提供商，面向各类中小商家提供一系列数据分析运营服务。具体实现的技术逻辑如下图所示。 平台方提供多端SDK供用户（商家提供网页、APP、小程序等多种接入形式）接入各类埋点数据，平台方以SaaS的形式提供统一的数据接入服务和数据分析服务。商家通过访问各类数据分析服务来进行更细粒度的埋点数据分析，完成行为统计、客户画像、客户圈选、广告投放监测等基本分析功能。然而，这种SaaS模式下，会存在一定的问题： 1） 由于商家类型和需求的多样化，平台提供SaaS类分析功能很难覆盖所有类型的商家，无法满足商家的定制化需求；如有些商家关注销量，有些关注客户运营，有些关注成本优化，很难满足所有的需求。 2） 对于一些高级分析功能，如依赖于自定义标签的客户圈选、客户自定义扩展等功能，统一的数据分析服务无法满足的；特别是一些自定义的标签依赖于商家自定义的算法，无法满足客户的高级分析需求。 3） 数据的资产化管理需求。在大数据时代，数据是一个企业/组织的资产已经成为了大家的共识，如何能让属于商家的数据合理、长期的沉淀下来，也是SaaS服务需要考虑的事情。 综上，我们在上图的基本模式上引入了数据湖模式，让数据湖作为商家沉淀数据、产出模型、分析运营的基础支撑设施。引入数据湖后的SaaS数据智能服务模式如下。 如图所示，平台方为每个用户提供一键建湖服务，商家使用该功能构建自己的数据湖，“一键建湖”能力一方面帮助商家将所有埋点数据的数据模型（schema）同步至数据湖中；另一方面，将属于该商家的所有埋点数据全量同步至数据湖中，并基于“T+1”的模式，将每天的增量数据归档入湖。基于数据湖的服务模式在传统的数据分析服务的基础上，赋予了用户数据资产化、分析模型化和服务定制化三大能力： 1） 数据资产化能力。利用数据湖，商家可以将属于自己的数据持续沉淀下来，保存多长时间的数据，耗费多少成本，完全由商家自主决定。数据湖还提供了数据资产管理能力，商家除了能管理原始数据外，还能将处理过的过程数据和结果数据分门别类保存，极大的提升了埋点数据的价值。 2） 分析模型化能力。数据湖中不仅仅有原始数据，还有埋点数据的模型（schema）。埋点数据模型体现了全域数据智能服务平台对于业务逻辑的抽象，通过数据湖，除了将原始数据作为资产输出外，还将数据模型进行了输出，借助埋点数据模型，商家可以更深入的理解埋点数据背后所体现的用户行为逻辑，帮助商家更好的洞察客户行为，获取用户需求。 3） 服务定制化能力。借助数据湖提供的数据集成和数据开发能力，基于对埋点数据模型的理解，商家可以定制数据处理过程，不断对原始数据进行迭代加工，从数据中提炼有价值的信息，最终获得超越原有数据分析服务的价值。 ","date":"2023-07-25","objectID":"/data_summary/:21:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.10 LakeHouse ","date":"2023-07-25","objectID":"/data_summary/:22:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"4.11 数据湖总结 数据湖作为新一代大数据分析处理的基础设施，需要超越传统的大数据平台。个人认为目前在以下方面，是数据湖解决方案未来可能的发展方向。 云原生架构 关于什么是云原生架构，众说纷纭，很难找到统一的定义。但是具体到数据湖这个场景，个人认为就是以下三点特征： 存储和计算分离，计算能力和存储能力均可独立扩展； 多模态计算引擎支持，SQL、批处理、流式计算、机器学习等； 提供serverless态服务，确保足够的弹性以及支持按需付费。 足够用的数据管理能力 数据湖需要提供更为强大的数据管理能力，包括但不限于数据源管理、数据类目管理、处理流程编排、任务调度、数据溯源、数据治理、质量管理、权限管理等。 大数据的能力，数据库的体验 目前绝大多数数据分析人员都只有数据库的使用经验，大数据平台的能力虽强，但是对于用户来说并不友好，数据科学家和数据数据分析师应该关注数据、算法、模型及其与业务场景的适配，而不是花大量的时间精力去学习大数据平台的开发。 数据湖要想快速发展，如何为用户提供良好的使用体验是关键。基于SQL的数据库应用开发已经深入人心，如何将数据湖的能力通过SQL的形式释放出来，是未来的一个主要方向。 完善的数据集成与数据开发能力 对各种异构数据源的管理与支持，对异构数据的全量/增量迁移支持，对各种数据格式的支持都是需要不断完善的方向。同时，需要具备一个完备的、可视化的、可扩展的集成开发环境。 与业务的深度融合与集成 典型数据湖架构的构成基本已经成为了业界共识：分布式对象存储+多模态计算引擎+数据管理。 决定数据湖方案是否胜出的关键恰恰在于数据管理，无论是原始数据的管理、数据类目的管理、数据模型的管理、数据权限的管理还是处理任务的管理，都离不开与业务的适配和集成；未来，会有越来越多的行业数据湖解决方案涌现出来，与数据科学家和数据分析师形成良性发展与互动。如何在数据湖解决方案中预置行业数据模型、ETL流程、分析模型和定制算法，可能是未来数据湖领域差异化竞争的一个关键点。 5 数据中台 ","date":"2023-07-25","objectID":"/data_summary/:23:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.1 基础概念 ","date":"2023-07-25","objectID":"/data_summary/:24:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.1.1 产生的背景 企业在过去信息化的历程中形成了大量生产经营及专业业务应用成果，同时也累积了大量的企业数据资产。限于传统的数据仓库技术手段，数据管理和分析能力成为信息化工作中的短板。 企业信息系统众多，系统管理独立，数据存储分散，横向的数据共享和分析应用仅由具体业务驱动，难以对全局数据开展价值挖掘，从规模上和效果上都无法真正体现集团庞大数据资产的价值。 市场竞争和产业链日益全球化，企业不只满足于内部数据的分析，更要通过互联网、微信、APP等新技术手段结合外部市场数据进行整体分析。 传统的数据仓库不能满足数据分析需求 企业在数据分析应用方面呈现“五大转变”（从统计分析向预测分析转变、从单领域分析向跨领域转变、从被动分析向主动分析转变、从非实时向实时分析转变、从结构化数据向多元化转变），并且对统一的数据中台平台诉求强烈，对数据中台的运算能力、核心算法、及数据全面性提出了更高的要求。 数据中台的处理架构发生了变化 一是以Hadoop、Spark等分布式技术和组件为核心的“计算\u0026存储混搭”的数据处理架构，能够支持批量和实时的数据加载以及灵活的业务需求。 二是数据的预处理流程正在从传统的ETL结构向ELT转变： 传统的数据仓库集成处理架构是ETL结构，这是构建数据仓库的重要一环，即用户从数据源抽取出所需的数据，经过数据清洗，将数据加载到数据仓库中去。 而大数据背景下的架构体系是ELT结构，其根据上层的应用需求，随时从数据中台中抽取想要的原始数据进行建模分析。 ","date":"2023-07-25","objectID":"/data_summary/:24:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.1.2 阿里集团为什么要建立一个“大中台、小前台“？ 我们从阿里共享业务事业部的发展史说起。起初，阿里只有一个淘宝事业部，后来成立了天猫事业部，此时淘宝的技术团队同时支撑着这两个事业部。当时的淘宝和天猫的电商系统像我们很多大型企业的一样是分为两套独立的烟囱式体系，两套体系中都包含的有商品、交易、支付、评价、物流等功能。因为上述原因，阿里集团又成立了共享业务事业部，其成员主要来自之前的淘宝技术团队，同时将两套电商业务做了梳理和沉淀 中台其实就是一个共享服务的体系结构。 我们需要在日常的开发过程中将通用的服务抽离出来做到共享服务的体系结构当中。大中台，小前台的体系结构可以使得管理更加高效，小团队更加扁平化。 由于资源的共享可以让开发更加敏捷，更能够知道需要做什么，该怎么做？ 通过抽象各条业务线，把共用的服务抽象出来共享，不限于用户、订单等基础模块服务，还包括具体的业务的抽象，比如教育培训相关的课程、讲师、学员等服务，通过抽象并以微服务的形式实现，避免重复投入资源造轮子。 ","date":"2023-07-25","objectID":"/data_summary/:24:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.1.3 中台目标 首先、把当前系统中各个业务的前端应用与后端服务解耦。将各个功能中的服务能力进行梳理、并沉淀。例如我们从外呼业务中梳理出工单管理和问卷管理的能力；从知识库中梳理出知识搜索的能力；从85电商平台中梳理出商品销售和库存管理的能力等等。 其次、将重复、类似的服务进行整合。同时在单个服务的完善和增强的过程中注意服务的通用性，避免其他相似“双胞胎”服务的出现。 最后，由于服务能力的集中管控，很大程度会促进我们一体化运维的能力，但在“大中台、小前台”的模式下，每一个服务都负责对N多个前端业务应用提供支持，这就要求运维在信息安全、备份、监控等方面要有更强的能力。 ","date":"2023-07-25","objectID":"/data_summary/:24:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.1.4 中台分类 甄别是不是中台，还要回到中台要解决的问题上，一切以“以用户为中心的持续规模化创新”为目的，将后台各式各样的资源转化为前台易于使用的能力，帮助我们打赢这场以用户为中心的战争的平台，我们都可以称之为中台： 业务中台提供重用服务 例如用户中心，订单中心之类的开箱即用可重用能力，为战场提供了强大的后台炮火支援能力，随叫随到，威力强大； 数据中台提供了数据分析能力 帮助我们从数据中学习改进，调整方向，为战场提供了强大及时的雷达监测能力，帮助我们掌控战场； 移动及算法中台提供了战场一线火力支援能力 帮助我们提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡； 技术中台提供了自建系统部分的技术支撑能力 帮助我们解决了基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备； 研发中台提供了自建系统部分的管理和技术实践支撑能力 帮助我们快速搭建项目，管理进度，测试，持续集成，持续交付，是前台特种兵的训练基地及快速送达战场的机动运输部队； 组织中台为我们的项目提供投资管理、风险管理、资源调度等， 是战场的指挥部，战争的大脑，指挥前线，调度后方。 所以，评判一个平台是否称得上中台，最终评判标准不是技术也不是长什么模样，最终还是得前台说了算，毕竟前台才是战争的关键，才是感受得到战场的残酷、看得见用户的那部分人。 ","date":"2023-07-25","objectID":"/data_summary/:24:4","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.2 数据中台和数仓的关系 ","date":"2023-07-25","objectID":"/data_summary/:25:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.2.1 传统数仓 传统数仓有几个特点： 数据具有历史性 基于文件存储 以表为形态，自带元数据存储（比如Hive） 在数仓的数据是其他原始数据的拷贝或者拷贝的加工 传统数仓需要拷贝数据的重要原因是数据计算和数据存储需要尽可能的近。所以我们需要把MySQL等数据源的数据同步到数仓，才能进行进一步处理。（这里有点疑问，我觉得是因为需要直接对数仓数据进行离线操作，而不是对业务数据库进行繁重的操作，也就是说数据分析不能影响业务） 另外传统数仓更关注的是数据的历史状态，所以导致数据规模庞大。 数仓本身也具备计算能力，同时也可以作为存储供其他计算系统使用。 ","date":"2023-07-25","objectID":"/data_summary/:25:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.2.2 数据中台 数据中台概念，不同于数据平台。数据中台，业务侧包含 数据触手(埋点) 数据接入(标准化) 数据仓库(抽象化) 数据治理(可靠性) 数据服务(产品化) 整体是一个闭环的解决方案 其中，闭环是最重要的一点。 数据服务接口 数据中台设计立足点本身是数据计算和存储分离的。那就意味着，数据中台本身并没有数据，数据来源是其他地方，比如传统数仓、业务数据库、用户在中台上传的文件（临时使用）、各个业务系统的API(瞬时，我们不关心API之前的数据结果是什么样的)。因为数据中台拥有这些数据源的适配器，所以相当于建立了互联管道。 关于元数据 我们知道数仓的优势是有元数据，通过表的方式很好的规整了数据。数据需要加工，所以一般数仓是有分层的，往上走一层，数据信息损耗就高一些。 **数据中台也有一个全局的元数据管理系统，管理也是以表为主，粒度到字段级别。**数据中台这个元信息包含了各个子存储的元信息，以数据中台需要的形态进行组织。 数据地图 数据中台的元数据其中承载的一个重要功能是数据地图，虽然在数据中台中，修建了通往所有数据的道路，但是当用户进来的时候无法知道具体某个数据的地址，也就没办法利用这些修好的道路。 数据地图就是解决这个问题 我们需要结合自然语言处理，检索技术，目录分类技术，机器学习以及数据规范化来帮助找到数据地址。数据地址从来都不是面向人类友好的。 通过数据中台的数据地图，以及数据中台到各数据源的建立好的管道，那么我们就可以很好的找到我们要的数据以及对他们进行关联和处理，分析，甚至进一步成为机器学习的素材。 数据地图和传统数仓元数据的区别在于： 它记录了散落在各个孤岛的数据，而不像传统数仓，只是在自己的数据。 数据格式是异构的，不仅仅是文件或表。 他不仅仅存储表以及字段相关信息，同时还让这些信息可检索，可查询，可以更好的面向人而不是机器。 ","date":"2023-07-25","objectID":"/data_summary/:25:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.2.3 结论 数仓是数据中台的一个重要组成部分，也是元数据的一个重要来源，但是随着技术的发展，数据计算和存储必定是分离的，这就需要一个新的元信息系统（数据地图）来进行承载。 ","date":"2023-07-25","objectID":"/data_summary/:25:3","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.3 数据中台建设是数字化转型的支撑 数据中台成为热点，“中台”这个概念，是相对于前台和后台而生，是前台和后台的链接点，将业务共同的工具和技术予以沉淀。数据中台是指数据采集交换、共享融合、组织处理、建模分析、管理治理和服务应用于一体的综合性数据能力平台，在大数据生态中处于承上启下的功能，提供面向数据应用支撑的底座能力。 广义上来给数据中台一个企业级的定义：“聚合和治理跨域数据，将数据抽象封装成服务，提供给前台以业务价值的逻辑概念”。 中台战略核心是数据服务的共享。中台战略并不是搭建一个数据平台，但是中台的大部分服务都是围绕数据而生，数据中台是围绕向上层应用提供数据服务构建的，中台战略让数据在数据平台和业务系统之间形成了一个良性的闭环，也就是实现应用与数据之间解藕，并实现紧密交互。 ","date":"2023-07-25","objectID":"/data_summary/:26:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.4 公司平台分层与大中台小前台战略 ","date":"2023-07-25","objectID":"/data_summary/:27:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.4.1 互联网巨头“大中台，小前台”战略 阿里巴巴在2015年12月进行组织升级，就是“大中台，小前台”的模式。主要的思路是打破原来树状结构，小前台距离一线更近，业务全能，这样便于快速决策、敏捷行动；支持类的业务放在中台，扮演平台支撑的角色。 其实，这个最早由阿里在2015年提出的“大中台，小前台”战略中延伸出来的概念，灵感来源于一家芬兰的小公司Supercell——一家仅有300名员工，却接连推出爆款游戏，是全球最会赚钱的明星游戏公司: 这家看似很小的公司，设置了一个强大的技术平台，来支持众多的小团队进行游戏研发。这样一来，他们就可以专心创新，不用担心基础却又至关重要的技术支撑问题。恰恰是这家小公司，开创了中台的“玩法”，并将其运用到了极致。对于这种多项目并行，各项目相对独立，但业务需求所需要的支持类似的公司，“中台”就有存在的价值。 这种类似的思维应用到大企业中，就是需要一个资源整合和能力沉淀的平台，对不同的部门进行总协调和支持，“中台”也就应运而生。 中台战略是构建符合DT时代的更具备创新性和灵活性的组织机制和业务机制，实现管理模式的创新。将公共的业务、数据、技术等公共能力从前台下沉，成为独立的中台，并且通过组织结构的调整物理拆分为独立的中台部门。 大中台，小前台”适用场景 不适合初创公司！初创公司的初创阶段没有任何的公共资源的积累，没有下沉为中台的内容。初创公司的首要任务是积累所有资源活下来，快速迭代主要业务，保存自己和核心竞争力。 适合高速发展公司或者快速成长公司。有一定的公共资源的积累，公共部分下沉为中台，保其高可用高性能，为前端业务百花齐放，快速迭代提供坚实的后盾。 ","date":"2023-07-25","objectID":"/data_summary/:27:1","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.4.2 公司平台分层 5.4.2.1 概述 阿里组织架构，业务中台、数据中台、技术中台公共组成中台。： 前台 由各类前台系统组成的前端平台。每个前台系统就是一个用户触点，即企业的最终用户直接使用或交互的系统，是企业与最终用户的交点。例如用户直接使用的网站，手机 app，微信公众号等都属于前台范畴。 中台 “中台”的设置就是为了提炼各个业务条线的共性需求，并将这些打造成组件化的资源包，然后以接口的形式提供给前台各业务部门使用，可以使产品在更新迭代、创新拓展的过程中研发更灵活、业务更敏捷，最大限度地减少“重复造轮子”的KPI项目。 “前台”要做什么业务，需要什么资源可以直接同公共服务部要。搜索、共享组件、数据技术等模块不需要每次去改动底层进行研发，而是在底层不变动的情况下，在更丰富灵活的“大中台”基础上获取支持，让“小前台”更加灵活敏捷。 后台 由后台系统组成的后端平台。每个后台系统一般管理了企业的一类核心资源（数据+计算），例如财务系统，产品系统，客户管理系统，仓库物流管理系统等，这类系统构成了企业的后台。基础设施和计算平台作为企业的核心计算资源，也属于后台的一部分。后台并不为前台而生 另外，由于后台往往并不能很好的支撑前台快速创新响应用户的需求，后台更多解决的是企业管理效率问题，而中台要解决的才是前台的创新问题。 5.4.2.2 敏捷前台/小前台 一线作战单元，强调敏捷交互及稳定交付的组织能力建设。 对于阿里来说，小前台就是各个业务部门，个性化的各种前台服务，例如阿里的天猫、淘宝、河马、支付宝等一系列的品牌。 5.4.2.3 业务中台 能力固化与赋能，固化通用能力，赋能前线部队，提升配置效率，加快前线响应，产品化业务化，开辟全新生态。 具体来说，业务中台对应公司的公共基础业务和通用服务，例如短信中心、用户中心、支付中心交易中心、搜索服务等。下图中的公共逻辑层，就是业务中台。 5.4.2.4 技术中台 技术中台主要负责基础服务、基础组件、基础平台、存储体系、云平台、运维相关等技术支撑。 5.4.2.5 数据中台 负责大数据统计分析相关的DaaS（数据即服务）和PaaS（平台即服务）相关服务建设，资产整合与共享，整合多维数据，统一资产管理，连通数据孤岛，共享数据资源，深入挖掘数据，盘活资产价值。 5.4.2.6 稳定后台 以共享中心建设为核心，为前中台提供专业的内部服务支撑。 ","date":"2023-07-25","objectID":"/data_summary/:27:2","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.5 数据中台定义及处理架构 数据中台是指通过企业内外部多源异构的数据采集、治理、建模、分析，应用，使数据对内优化管理提高业务，对外可以数据合作价值释放，成为企业数据资产管理中枢。数据中台建立后，会形成数据API，为企业和客户提供高效各种数据服务。 数据中台整体技术架构上采用云计算架构模式，将数据资源、计算资源、存储资源充分云化，并通过多租户技术进行资源打包整合，并进行开放，为用户提供“一站式”数据服务。 利用大数据技术，对海量数据进行统一采集、计算、存储，并使用统一的数据规范进行管理，将企业内部所有数据统一处理形成标准化数据，挖掘出对企业最有价值的数据，构建企业数据资产库，提供一致的、高可用大 数据服务。 数据中台不是一套软件，也不是一个信息系统，而是一系列数据组件的集合，企业基于自身的信息化建设基础、数据基础以及业务特点对数据中台的能力进行定义，基于能力定义利用数据组件搭建自己的数据中台。 ","date":"2023-07-25","objectID":"/data_summary/:28:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.6 数据中台带来价值 数据中台对一个企业的数字化转型和可持续发展起着至关重要的作用。数据中台为解耦而生，企业建设数据中台的最大意义就是应用与数据解藕。这样企业就可以不受限制地按需构建满足业务需求的数据应用。 构建了开放、灵活、可扩展的企业级统一数据管理和分析平台， 将企业内、外部数据随需关联，打破了数据的系统界限。 利用大数据智能分析、数据可视化等技术，实现了数据共享、日常报表自动生成、快速和智能分析，满足集团总部和各分子公司各级数据分析应用需求。 深度挖掘数据价值，助力企业数字化转型落地。实现了数据的目录、模型、标准、认责、安全、可视化、共享等管理，实现数据集中存储、处理、分类与管理，建立大数据分析工具库、算法服务库，实现报表生成自动化、数据分析敏捷化、数据挖掘可视化，实现数据质量评估、落地管理流程。 ","date":"2023-07-25","objectID":"/data_summary/:29:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"5.7 传统数据仓库与数据中台的差异点 作为工业企业，一般采用混搭架构： 6 各种概念对比 ","date":"2023-07-25","objectID":"/data_summary/:30:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"6.1 数据仓库vs.数据集市 数据集市和数据仓库经常会被混淆，但两者的用途明显不同。 数据集市通常是数据仓库的子集;它等数据通常来自数据仓库 – 尽管还可以来自其他来源。数据集市的数据专门针对特定的用户社区(例如销售团队)，以便他们能够快速找到所需的数据。通常，数据保存在那里用于特定用途，例如财务分析。 数据集市也比数据仓库小得多 – 它们可以容纳数十千兆字节，相比之下，数据仓库可以存储数百千兆字节到PB级数据，并可用于数据处理。 数据集市可从现有数据仓库或其他数据源系统构建，你只需设计和构建数据库表，使用相关数据填充数据库表并决定谁可以访问数据集即可。 ","date":"2023-07-25","objectID":"/data_summary/:31:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"6.2 数据仓库vs.ODS 操作数据存储(ODS)是一种数据库，用作所有原始数据的临时存储区域，这些数据即将进入数据仓库进行数据处理。我们可以将其想象成仓库装卸码头，货物在此处交付、检查和验证。在ODS中，数据在进入仓库前可以被清理、检查(因为冗余目的)，也可检查是否符合业务规则。 在ODS中，我们可以对数据进行查询，但是数据是临时的，因此它仅提供简单信息查询，例如正在进行的客户订单状态。 ODS通常运行在关系数据库管理系统(RDBMS)或Hadoop平台。 ","date":"2023-07-25","objectID":"/data_summary/:32:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["big data"],"content":"6.3 关系型数据库vs.数据仓库和数据湖 数据仓库、数据湖与关系数据库系统之间的主要区别在于： 关系数据库用于存储和整理来自单个来源(例如事务系统)的结构化数据， 而数据仓库则用于存储来自多个来源的结构化数据。 数据湖的不同之处在于它可存储非结构化、半结构化和结构化数据。 关系数据库创建起来相对简单，可用于存储和整理实时数据，例如交易数据等。关系数据库的缺点是它们不支持非结构化数据库数据或现在不断生成的大量数据。这使得我们只能在数据仓库与数据湖间做出选择。尽管如此，很多企业仍然继续依赖关系数据库来完成运营数据分析或趋势分析等任务。 内部或云端可用的关系数据库包括Microsoft SQL Server、Oracle数据库、MySQL和IBM Db2、以及Amazon Relational Database Service、Google Cloud Spanner等。 7 数据治理 可参考 华为云-DAYU数据治理方法论 本文转自 https://blog.csdn.net/baichoufei90/article/details/87880848，如有侵权，请联系删除。 ","date":"2023-07-25","objectID":"/data_summary/:33:0","tags":["big data","database","data warehouse","data lake","data mark","data center"],"title":"数据库, 数据仓库, 数据集市, 数据湖, 数据中台","uri":"/data_summary/"},{"categories":["CentOS"],"content":" 因为每次都要去网上找教程 (😂), 所以记录一下自己的配置过程 VMware 默认配置就为 NAT 模式 NAT 模式 关闭防火墙 # 关闭 \u003e systemctl stop firewalld # 禁用 \u003e systemctl disable firewalld # 查看状态 \u003e systemctl status firewalld 禁用 selinux # 将 SELINUX 设置为 `disabled` \u003e vi /etc/selinux/config 禁用 selinux 安全增强型 Linux（Security-Enhanced Linux）简称 SELinux, 它是一个 Linux 内核模块, 也是 Linux 的一个安全子系统. 获取 mac vmnet8 的 gateway 地址 \u003e cat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf 找到 # NAT gateway address 一行, 然后记下 ip 和 netmask 修改虚拟机中的网卡配置 \u003e vi /etc/sysconfig/network-scripts/ifcfg-ens33 ifcfg-ens33 上图红框为修改内容, 绿框为新增内容 BOOTPROTO=static ONBOOT=yes IPADDR=172.16.143.101 GATEWAY=172.16.143.2 NETMASK=255.255.255.0 DNS1=114.114.114.114 DNS2=8.8.8.8 # 其中 GATEWAY 为第 4 步中的 ip, NETMASK 为第 4 步 中的 netmask 重启虚拟机网卡 \u003e systemctl restart network ping 一下外网就可以啦! \u003e ping www.baidu.com 修改主机名 \u003e vi /etc/hostname # 将其中内容删除, 改为: `buli_server1` \u003e vi /etc/hosts /etc/hosts 重启 \u003e reboot -f ","date":"2023-07-11","objectID":"/mac_vmware_set_net/:0:0","tags":["CentOS","Operating System","Network"],"title":"mac 配置 VMware 的 CentOS 网络 (NAT 模式)","uri":"/mac_vmware_set_net/"},{"categories":["MySQL"],"content":"之前项目里一直在用的 SeaTunnel 版本是 2.1.3, 有些旧了. 而且 Spark 和 Flink 的脚本还要写两套. 所以就在考虑要升级到 v2.3.2. 所以最近在做 SeaTunnel v2.3.2 的性能调研. 之前做 v2.3.0 的调研的时候就发现在写入 MySQL 的时候性能特别低, 写入速度竟然 200/s. 有些不能接受… 因为 v2.1.3 写入 MySQL 的速度可以达到 2000+/s. 但是当时也没有深究这个问题, 就搁置了. 最近这个问题又被提起的时候, 就再次去社区找找看有没有类似的问题. 然后发现了也有人面临着这个问题: issue. 里面提出在链接 URI 后面加上参数 rewriteBatchStatements=true. 但是这个参数在这里的提高的并不明显.. 我甚至觉得这里增加这个参数之后, 其实并没有提升. 😂 这个参数的意义是数据库会更高性能的执行批量处理. 当写入的时候, 再去查看服务器的时候发现, 硬盘的 IO 有明显的增高. 于是就猜想, 是不是 v2.3.2 版本里, 写入的时候, 每条都作为一个事务提交的, 然后就想到了 innodb_flush_log_at_trx_commit 这个参数, 在补充这部分的知识的时候, 发现了也可以调整一下相关的参数sync_binlog 以达到调优的目的. 于是就有了这篇博客记录一下. ","date":"2023-07-03","objectID":"/increase_write/:0:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["MySQL"],"content":"innodb_flush_log_at_trx_commit 这个参数是用来配置 MySQL 日志何时写入硬盘的参数. 查看 MySQL 配置: SHOW VARIABLES LIKE 'innodb_flush_log_at_trx_commit'; 0: 日志缓存区将每隔 1 秒写到日志中, 并且将日志文件的数据刷新 (flush) 到磁盘上. 该模式下在事务提交时不会主动触发写入磁盘的操作. 1: 每次提交事务时, MySQL 都会把日志文件的数据写入, 并且刷新到磁盘上, 默认为该模式. 2: 每次提交事务时, MySQL 都会把日志文件的数据写入, 但是刷新到磁盘的操作不会同时进行, 而是每秒执行一次刷新到磁盘的操作. 所以说: 当设置为 0 的时候, 速度最快, 但是不安全, mysqld 进程的崩溃会导致上一秒所有事务数据的丢失. 当设置为 1 的时候, 速度最慢, 但是最安全. 在 mysqld 服务崩溃或服务器崩溃的情况下, 日志只可能丢失最多一个语句或一个事务. 当设置为 2 的时候, 速度快, 也比设置为 0 时安全, 只有在操作系统崩溃或系统断电的情况下, 上一秒所有事务数据才可能丢失. 所以我将 MySQL 改成该模式. ","date":"2023-07-03","objectID":"/increase_write/:1:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["MySQL"],"content":"sync_binlog 默认情况下, 并不是每次写入时都将 binlog 日志文件与磁盘同步. 因此如果操作系统或服务器崩溃. 有可能 binlog 中最后的语句丢失. 为了防止这种情况, 可以使用 sync_binlog 全局变量 (1 是最安全的值, 但也是最慢的), 使 binlog 在每 N 次 binlog 日志文件写入后与磁盘同步. 查看 MySQL 配置: SHOW VARIABLES LIKE 'sync_binlog'; 所以最终, 我将 innodb_flush_log_at_trx_commit 设置为 2, sync_binlog 设置为 100 . 然后经过测试, v2.3.2 对于 MySQL 的写入速度达到了 2000+/s 的速度. 😄 ","date":"2023-07-03","objectID":"/increase_write/:2:0","tags":["Database","MySQL"],"title":"MySQL写入速度调优之`innodb_flush_log_at_trx_commit`","uri":"/increase_write/"},{"categories":["Java"],"content":" 传送门: JJTree ","date":"2023-06-13","objectID":"/jjtree/:0:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"概述 JJTree 是 JavaCC 的预处理器, 可在 JavaCC 源代码的不同位置插入解析树构建操作. JJTree 的输出通过 JavaCC 运行以创建解析器. 此文档描述了如何使用 JJTree, 以及如何将解析器连接到 JJTree. 默认情况下, JJTree 生成代码来为语言中的每个非终节点结构解析书节点. 但是可以修改这个行为, 为了某些非终节点不会生成节点, 或者为生产扩展的一部分生成节点. JJTree 定义了一个 Java 接口 Node , 并且所有解析树节点都必须实现它. 这个接口提供了例如设置父节点、添加和检索子节点等操作的方法. JJTree 以两种模式之一运行, 简单模式和多模式 (需要更好的术语). 在简单模式下, 每个解析树节点都是具体类型的 SimpleNode, 在多模式下, 解析树节点的类型源自节点的名称. 如果不提供节点类的实现, 那么 JJTree 会生成基于 SimpleNode 的示例实现. 之后可以修改该实现来进行适配. 虽然 JavaCC 是一个自上而下的解析器, 但是 JJTree 是自下而上地构造解析树. 为此, 它使用一个堆栈, 在创建节点后推送节点. 当它为它们找到父级时, 它会从堆栈中弹出 (pop) 子级并将它们添加到父级, 并且最后推送新的父节点本身. 堆栈是开放的, 这意味着可以从语法操作中访问它: 可以用认为合适的方式进行推送 (push), 弹出 (pop)和其他方式操作其内容 (详细信息可以参考 节点范围和用户操作 ) JJTree 提供了两种基本类型的节点的装饰, 以及一些语法速记以方便它们的使用. 确定节点 一个确定节点是用特定数量的子节点构成的. 许多节点从堆栈中弹出并成为新节点的子节点, 然后将其推送到堆栈本身上. 可以标记一个确定节点, 类似这样: #ADefiniteNode(INTEGER EXPRESSION) 确定节点描述符表达式可以是任何整数表达式, 尽管文字整数常量是迄今为止最常见的表达式. 条件节点 当且仅当条件计算为真 (true) 时才使用其节点范围内被推送到堆栈的所有子节点构造条件节点. 如果计算条件为假 (false) 时, 该节点不会被构造, 并且其所有子节点会留在节点堆栈中. 可以标记一个条件节点, 类似这样: #ConditionalNode(BOOLEAN EXPRESSION) 条件节点描述符表达式可以是任何布尔表达式. 条件节点有两种常见的简写: 1. 不定节点 (Indefinite nodes) #IndefiniteNode is short for #IndefiniteNode(true) 2. 大于节点 (Greater-than nodes) #GTNode(\u003e1) is short for #GTNode(jjtree.arity() \u003e 1) 不定节点速记 (1) 在后面跟着带括号的展开时可能会导致 JJTree 源中的歧义. 在这种情况下, 速记必须替换为完整表达式: ( ... ) #N ( a() ) 上面这个是有歧义的, 所以必须使用显示条件: ( ... ) #N(true) ( a() ) 注意: 节点描述符表达式不应该有副作用. JJTree没有指定表达式将被计算多少次. 默认情况下, JJTree 将每个非终节点视为不定节点, 并从其生产名称派生节点的名称. 可以使用以下语法为其命名: void P1() #MyNode : { ... } { ... } 当解析器识别出一个 P1 非终节点时, 它会开始一个不定节点. 它标记堆栈, 以便在 P1 扩展中由非终节点创建并推送到堆栈上的任何解析树节点都将被弹出并成为节点 MyNode 的子节点. 如果要禁止为生产创建节点, 可以使用以下语法: void P2() #void : { ... } { ... } 现在, 在 P2 的扩展中由非终节点推送的任何解析树节点都将保留在堆栈上, 以便弹出并使其成为树上更向上的生产子节点. 可以使用 NODE_DEFAULT_VOID 选项将此设置为未装饰节点的默认行为. void P3() : {} { P4() ( P5() )+ P6() } 在这个例子中, 开始一个不定节点 P3, 标记堆栈, 然后解析一个 P4 节点、一个或多个 P5 节点和一个 P6 节点. 可以进一步自定义生成的树: void P3() : {} { P4() ( P5() )+ #ListOfP5s P6() } 现在, P3 节点将有一个 P4 节点、一个 ListOfP5s 节点和一个 P6 节点作为子节点. #Name 构造充当后缀运算符, 其作用域是紧接在前的扩展单元. ","date":"2023-06-13","objectID":"/jjtree/:1:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点范围和用户操作 每个节点都与一个节点作用域 (node scope) 相关联. 此范围内的用户操作可以通过使用特殊标识符 jjtThis 来访问正在构建的节点, 以引用该节点. 该标识符被隐式声明为该节点的正确类型, 因此可以轻松地访问该节点拥有的任何字段和方法. 作用域是紧靠在节点修饰之前的扩展单元. 这可以是带括号的表达式. 当对生产签名进行装饰时 (可能隐式带有默认节点), 作用域是生产的整个右侧, 包括其声明块. 还可以在扩展引用的左侧使用涉及 jjtThis 的表达式. 🌰: ... ( jjtThis.my_foo = foo() ) #Baz ... 这里的 jjtThis 指向的是一个 Baz 节点, 他有一个名为 my_foo 的字段. 将解析生成的 foo() 的结果赋给 myfoo. 节点范围内的最终用户操作与所有其他操作不同. 当其中的代码执行时, 节点的子节点已经从堆栈中弹出并添加到节点中, 节点本身已被推送到堆栈中. 现在, 可以通过节点的方法 (比如 jjtGetChild()) 来访问子节点. 除最后一个操作之外的其他用户操作只能访问堆栈上的子级. 它们还没有被加入到节点中, 所以还不能通过节点的方法进行访问. 如果条件节点的节点描述符表达式的计算结果为 false, 则不会将其添加到堆栈中, 也不会向其添加子节点. 条件节点范围内的最终用户操作可以通过调用 nodeCreated() 方法来确定节点是否被创建. 只有当节点的条件被满足且被创建且被推入到节点堆栈中时才返回 true, 否则返回 false. ","date":"2023-06-13","objectID":"/jjtree/:1:1","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"异常处理 由节点作用域中的扩展抛出的异常不在该节点作用域内捕获, 由 JJTree 本身捕获. 发生这种情况时, 任何已被压入节点范围内的堆栈的节点都将被弹出并丢弃. 然后重新抛出异常. 这么做的目的是使解析器能够实现错误恢复, 并在已知状态下继续使用节点堆栈. 注意: JJTree 目前无法检测节点作用域内的用户操作是否引发异常. 这样的异常可能会被错误地处理. ","date":"2023-06-13","objectID":"/jjtree/:1:2","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点作用域钩子 如果 NODE_SCOPE_HOOK 设置为 true, 那么 JJTree 将在每个节点作用域的入口和出口上生成对两个用户定义的解析器方法的调用. 这些方法必须具有以下声明: void jjtreeOpenNodeScope(Node n) void jjtreeCloseNodeScope(Node n) 如果解析器是 STATIC, 那么这些方法也必须被声明为 static. 它们都以当前节点作为参数调用. 一种用途可能是将解析器对象本身存储在节点中, 以便可以提供应该由该解析器生成的所有节点共享的状态. 🌰, 解析器可能维护一个符号表. void jjtreeOpenNodeScope(Node n) { ((SimpleNode)n).jjtSetValue(getSymbolTable()); } void jjtreeCloseNodeScope(Node n) {} 其中 getSymbolTable() 是一个用户自定义的方法, 用来返回节点的符号表的结构. ","date":"2023-06-13","objectID":"/jjtree/:1:3","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"跟踪令牌 跟踪每个节点的最初和最后的令牌通常很有用, 这样就可以轻松地再次复制输入. 通过设置 TRACK_TOKENS 选项, 生成的 SimpleNode 类将包含 4 个额外的方法: public Token jjtGetFirstToken() public void jjtSetFirstToken(Token token) public Token jjtGetLastToken() public void jjtSetLastToken(Token token) 当解析器运行的时候, 将自动设置每个节点的最初和最后的令牌. ","date":"2023-06-13","objectID":"/jjtree/:1:4","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点的生命周期 节点在创建的时候将进行一系列的确定的步骤. 这是从节点本身的角度来看的序列: 使用唯一的整数参数调用节点的构造函数. 此参数标识节点的类型, 在简单模式下特别有用. JJTree 会自动生成一个叫做 \u003cparser\u003eTreeConstants.java 的文件, 用来声明有效常量. 常量的命名是通过在节点的大写名称前加上 JJT 来派生的, 点 (.) 替换为下划线 (_). 为方便起见, 在同一文件中维护了一个名为 jjtNodeName[] 的字符串数组, 它将常量映射到未修改的节点名称. 节点的 jjtOpen() 方法被调用. 如果设置了 NODE_SCOPE_HOOK 选项, 用户自定义解析器方法 openNodeScope() 会被调用, 并且节点作为参数进行传递. 该方法可以初始化节点中的字段或调用其方法. 🌰, 它可以存储节点的第一个令牌. 如果在解析节点时抛出未处理的异常, 则将该节点抛弃. JJTree 再也不会引用它了. 它不会被关闭, 并且不会使用它作为参数来调用用户定义的节点作用域钩子 CloseNodeHook(). 否则, 如果该节点是有条件的, 并且其条件表达式的计算结果为 false, 则该节点将被放弃. 它不会被关闭, 尽管可以使用它作为参数调用用户定义的节点作用域钩子 CloseNodeHook(). 否则, 由确定节点的整数表达式指定的节点的所有子节点, 或在条件节点范围内推送到堆栈上的所有节点都将添加到该节点. 它们添加的顺序是不确定的. 节点的 jjtClose() 方法被调用. 节点被推到堆栈中. 如果设置了 NODE_SCOPE_HOOK 选项, 用户自定义解析器方法 closeNodeScope() 会被调用, 并且节点作为参数进行传递. 如果节点不是根节点, 那么它将被添加为另一个节点的子节点, 并调用其 jjtSetParent() 方法. ","date":"2023-06-13","objectID":"/jjtree/:1:5","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"访客支持 JJTree 为访问者设计模式提供了一些基本支持. 如果 VISITOR 选项设置为 true, JJTree 将把 jjtAccept() 方法插入到它生成的所有节点类中, 并且还生成可以实现并传递给节点以接受的访问者接口. 访问者接口的命名是通过将 Visitor 附加到解析器的名称来构造的. 每次运行 JJTree 时都会重新生成接口, 以便准确表示解析器使用的节点集. 如果没有为新节点更新实现类, 这将导致编译时错误. 这是一个功能. ","date":"2023-06-13","objectID":"/jjtree/:1:6","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"选项 选项 缺省值 描述 BUILD_NODE_FILES true 为 SimpleNode 和语法中使用的任何其他节点生成示例实现. MULTI false 生成多模式解析树. 缺省值为 false, 生成一个简单模式解析树. NODE_DEFAULT_VOID false 与其使每个未装饰的产品成为不定节点, 不如使其无效. NODE_CLASS \"\" 如果设置定义将扩展 SimpleNode 的用户提供的类名称, 那么创建的任何树节点都将是 NODE_CLASS 的子类. NODE_FACTORY \"\" 指定一个包含工厂方法的类, 该方法具有以下签名以构造节点: public static Node jjtCreate(int id). 为了向后兼容, 也可以指定值 false, 这意味着 SimpleNode 将用作工厂类. NODE_PACKAGE \"\" 生成节点类的包. 默认为解析器包. NODE_EXTENDS \"\" 弃用. SimpleNode 类的超类. 通过提供自定义超类, 可以避免编辑生成的SimpleNode.java. NODE_PREFIX \"AST\" 用于在多模式下从节点标识符构造节点类名的前缀. NODE_SCOPE_HOOK false 在每个节点作用域的入口和出口上生成对两个用户定义的解析器方法的调用. NODE_USES_PARSER false JJTree 将使用另一种形式的节点构造例程, 在其中传递解析器对象. 🌰: public static Node MyNode.jjtCreate(MyParser p, int id);MyNode(MyParser p, int id); TRACK_TOKENS false 在 SimpleNode 中插入 jjtGetFirstToken(), jjtSetFirstToken(), getLastToken() 和 jjtSetLastToken() 方法. FirstToken 在进入节点作用域时自动设置; LastToken 在退出节点作用域时自动设置. STATIC true 为静态解析器生成代码. 这必须与等效的 JavaCC 选项一致使用. 该选项的值在 JavaCC 源中发出. VISITOR false 在节点类中插入一个 jjtAccept() 方法, 并为语法中使用的每个节点类型生成一个包含条目的访问者实现. VISITOR_DATA_TYPE \"Object\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中用作数据参数的类型. VISITOR_RETURN_TYPE \"Object\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中用作返回参数的类型. VISITOR_EXCEPTION \"\" 如果设置了该选项, 它将在生成的jjtAccept() 方法和 visit() 方法的签名中. JJTREE_OUTPUT_DIRECTORY \"OUTPUT_DIRECTORY\" 默认情况下, JJTree 在全局 OUTPUT_DIRECTORY 设置中指定的目录中生成其输出. 显式设置此选项允许用户将解析器与树文件分开. ","date":"2023-06-13","objectID":"/jjtree/:1:7","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"JJTree 接口 ","date":"2023-06-13","objectID":"/jjtree/:2:0","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"JJTree 状态 JJTree 将其状态保存在一个名为 jjtree 的解析器类字段中. 可以使用此成员中的方法来操作节点堆栈. final class JJTreeState { // 调用它来重新初始化节点堆栈 void reset(); // 返回抽象语法树 (AST) 的根节点 Node rootNode(); // 判断当前节点是否实际关闭并推送 boolean nodeCreated(); // 返回节点上当前推送的节点数 // 当前节点作用域内的堆栈 int arity(); // 推送节点到堆栈 void pushNode(Node n); // 返回堆栈顶部的节点, 并将其从堆栈中移除 Node popNode(); // 返回当前位于堆栈顶部的节点 Node peekNode(); } ","date":"2023-06-13","objectID":"/jjtree/:2:1","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":"节点对象 /** * 所有 AST 节点必须实现该接口. * 它为构建节点之间的父子关系提供了基本机制. */ public interface Node { // 将节点设为当前节点后调用此方法. // 表示现在可以向其中添加子节点. public void jjtOpen(); // 添加所有子节点后调用此方法. public void jjtClose(); // 这对方法用于通知节点其父节点. public void jjtSetParent(Node n); public Node jjtGetParent(); // 此方法通知节点将其参数添加到节点的子节点列表中. public void jjtAddChild(Node n, int i); // 此方法返回子节点. // 子节点编号从 0 开始, 从左至右. public Node jjtGetChild(int i); // 返回节点拥有的子节点数. int jjtGetNumChildren(); } SimpleNode 类实现了 Node 接口, 如果它不存在, 则由 JJTree 自动生成. 可以使用该类作为节点实现的模板或超类, 或修改其进行适配. SimpleNode 还提供了递归转储节点及其子节点的基本机制. 可以像这样使用这个命令: { ((SimpleNode)jjtree.rootNode()).dump(\"\u003e\"); } dump() 的字符串参数用作填充, 以指示树层次结构. 如果设置了 VISITOR 选项, 则会生成另一个实用程序方法: { public void childrenAccept(MyParserVisitor visitor); } 这会轮流遍历节点的子节点, 要求它们接受访问者. 这在实现前序和后序遍历时很有用. ","date":"2023-06-13","objectID":"/jjtree/:2:2","tags":["Java","JavaCC","JJTree"],"title":"JJTree","uri":"/jjtree/"},{"categories":["Java"],"content":" 因为最近的项目里会有大量的 SQL 生成/解析, 所以就打算自己写一个. 于是就有了今天的这篇 blog. 传送门: JavaCC ","date":"2023-06-13","objectID":"/javacc/:0:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"概述 JavaCC 的全称是 Java Compiler Compiler (没错就是俩 Compiler), 是用于 Java 应用程序的最流行的解析器生成器. 它是一个工具, 用来读取语法规范并将其转换为可以识别与语法匹配的 Java 程序. 除了解析器生成器本身之外, JavaCC 和提供与解析器生成相关的其他标准功能, 例如树构建 (通过 JavaCC 中包含的名为 JJTree 的工具, 这个工具之后会重点用他来完成 SQL 语法解析)、操作和调试. ","date":"2023-06-13","objectID":"/javacc/:1:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"特点 JavaCC 的生成是自上而下 (递归下降) 解析器, 而不是像类似 YACC 的工具生成的自下而上的解析器. 这就允许使用更通用的语法, 尽管做递归是不被允许的. 自上而下的解析器还有许多的有点 (除了更通用的语法之外), 例如更容易调试, 能够解析语法中的任何非终端, 以及能够向上传递值 (属性) 并且在解析期间沿着解析树向下移动. 默认情况下, JavaCC 生成一个 LL(1) 解析器. 但是, 可能有部分语法不是 LL(1). JavaCC 提供了句法和语义前瞻的能力, 能够在这些点直接解决轮班歧义 (shift-shift ambiguities). 🌰, 解析器仅在这些点是 LL(k), 但在其他的地方都保持 LL(1) 以获得更好的性能表现. 移位归约 (Shift-reduce) 和 reduce-reduce 冲突对于自上而下的解析器来说不是问题. JavaCC 生成百分之百纯 Java 的解析器, 因此对 JavaCC 没有运行时依赖性, 也不需要在不同机器平台上运行的特殊移植工作. JavaCC 允许在此法和语法规范中扩展巴克斯范式 (extended BNF), 例如 (A)*, (A)+ 等等. 扩展巴克斯范式在一定程度上减少了对于左递归的需求. 实际上, 扩展巴克斯范式通常更容易阅读, 例如 A::=y(x)* 与 A::=Ax|y. 词法规范 (比如正则表达式、字符串) 和语法规范 (比如巴克斯范式) 都写在同一个文件中. 这样使得语法更易读, 因为可以在语法规范中内联使用正则表达式, 并且也更易于维护. JavaCC 的词法分析器可以处理完整的 Unicode 输入, 并且词法规范也可以包括任何 Unicode 字符. 这个特性有助于描述语言元素, 例如允许某些 Unicode 字符 (不是 ASCII ) 但不允许其他字符的 Java 标识符. JavaCC 提供了类 Lex 的词法状态和词法动作功能. JavaCC 中优于其他公祖的特定方面是一流状态, 它提供了诸如 TOKEN, MORE, SKIP 和状态更改等概念. 这就允许更清晰的规范以及来自 JavaCC 的更好的错误和警告消息. 词法规范中顶一个为特殊标记的标记 (Token) 在解析期间将被忽略. 一个有用的应用是处理评论. 最后一句话, 没看明白, 原文是: A useful application of this is in the processing of comments. 词法规范可以在整个词法规范的全局级别或在单个词法规范的基础上定义不区分大小写的标记. JavaCC 带有 JJTree, 一个非常强大的树构建预处理器. JavaCC 还包括 JJDoc, 这是一种将愈发文件转换为文档文件的工具, 可选择采用 HTML 格式. JavaCC 提供了许多自定义它的行为和生成的解析器的行为的选项. 此类选型的示例是对输入流执行的 Unicode 处理的种类、要执行的歧义检查的标记数等. JavaCC 的错误报告在解析器生成器中也是前列的. JavaCC 生成的解析器能够通过完整的诊断信息清楚地解析错误的位置. 通过使用 DEBUG_PARSER, DEBUG_LOOKAHEAD 和 DEBUG_TOKEN_MANAGER 选项, 用户可以深入分析解析和 token 处理步骤. JavaCC 版本包含范围广泛的实例, 包括 Java 和 HTML 语法. 这些实例及其文档是熟悉 JavaCC 的好方法. ","date":"2023-06-13","objectID":"/javacc/:2:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"示例 这个示例识别匹配的大括号后跟零个或多个行终止符, 然后是文件结尾. 此语法中合法字符串的示例是: {}, }}} 等等 非法字符串的例子是: {}{}, }{}}, { }, {x} 等等 ","date":"2023-06-13","objectID":"/javacc/:3:0","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"语法 PARSER_BEGIN(Example) /** 简单大括号匹配器 */ public class Example { /** 主入口 */ public static void main(String args[]) throws ParseException { Example parser = new Example(System.in); parser.Input(); } } PARSER_END(Example) /** 根生产 */ void Input() : {} { \"{\" [ MatchedBaraces() ] \"}\" } ","date":"2023-06-13","objectID":"/javacc/:3:1","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["Java"],"content":"输出 $ java Example \u003creturn\u003e $ java Example {x\u003creturn\u003e Lexical error at line 1, column 2. Encountered: \"x\" TokenMgrError: Lexical error at line 1, column 2. Encountered: \"x\" (120), after : \"\" at ExampleTokenManager.getNextToken(ExampleTokenManager.java:146) at Example.getToken(Example.java:140) at Example.MatchedBraces(Example.java:51) at Example.Input(Example.java:10) at Example.main(Example.java:6) $ java Example {}}\u003creturn\u003e ParseException: Encountered \"}\" at line 1, column 3. Was expecting one of: \u003cEOF\u003e \"\\n\" ... \"\\r\" ... at Example.generateParseException(Example.java:184) at Example.jj_consume_token(Example.java:126) at Example.Input(Example.java:32) at Example.main(Example.java:6) ","date":"2023-06-13","objectID":"/javacc/:3:2","tags":["Java","JavaCC"],"title":"JavaCC","uri":"/javacc/"},{"categories":["big data"],"content":"简介 在大多数生产环境中, 密码等敏感配置项需要加密, 不能以明文存储, SeaTunnel 为此提供了便捷的一站式解决方案. ","date":"2023-05-25","objectID":"/seatunnel_encryption/:1:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"使用方法 SeaTunnel 自带 base64 加解密功能, 但不建议用于生产使用, 建议用户实现自定义加解密逻辑, 可以参考本章 如何实现用户自定义加解密 获取更多关于它的详细信息. Base64 加密支持加密以下参数: username password auth 接下来展示怎么使用 base64 进行加密: 在配置文件中添加新的配置项 shade.identifier, 这个配置项指示要使用的加密方法, 在这个示例中, 应该在配置中添加 shade.identifier = base64, 如下所示： # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # env { execution.parallelism = 1 shade.identifier = \"base64\" } source { MySQL-CDC { result_table_name = \"fake\" parallelism = 1 server-id = 5656 port = 56725 hostname = \"127.0.0.1\" username = \"seatunnel\" password = \"seatunnel_password\" database-name = \"inventory_vwyw0n\" table-name = \"products\" base-url = \"jdbc:mysql://localhost:56725\" } } transform { } sink { # choose stdout output plugin to output data to console Clickhouse { host = \"localhost:8123\" database = \"default\" table = \"fake_all\" username = \"seatunnel\" password = \"seatunnel_password\" # cdc options primary_key = \"id\" support_upsert = true } } 使用基于不同计算引擎的命令行来加密配置文件, 🌰中使用 Zeta (SeaTunnel 自研): \u003e ${SEATUNNEL_HOME}/bin/seatunnel.sh --config config/v2.batch.template --encrypt 然后就可以在命令行中看到加密的配置: { \"env\" : { \"execution.parallelism\" : 1, \"shade.identifier\" : \"base64\" }, \"source\" : [ { \"base-url\" : \"jdbc:mysql://localhost:56725\", \"hostname\" : \"127.0.0.1\", \"password\" : \"c2VhdHVubmVsX3Bhc3N3b3Jk\", \"port\" : 56725, \"database-name\" : \"inventory_vwyw0n\", \"parallelism\" : 1, \"result_table_name\" : \"fake\", \"table-name\" : \"products\", \"plugin_name\" : \"MySQL-CDC\", \"server-id\" : 5656, \"username\" : \"c2VhdHVubmVs\" } ], \"transform\" : [], \"sink\" : [ { \"database\" : \"default\", \"password\" : \"c2VhdHVubmVsX3Bhc3N3b3Jk\", \"support_upsert\" : true, \"host\" : \"localhost:8123\", \"plugin_name\" : \"Clickhouse\", \"primary_key\" : \"id\", \"table\" : \"fake_all\", \"username\" : \"c2VhdHVubmVs\" } ] } 当然, 不仅支持加密配置文件, 如果用户想查看解密后的配置文件, 可以执行以下命令: \u003e ${SEATUNNEL_HOME}/bin/seatunnel.sh --config config/v2.batch.template --decrypt ","date":"2023-05-25","objectID":"/seatunnel_encryption/:2:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"如何实现用户自定义加解密 如果想自定义加密方法和加密配置, 本节将帮助来解决问题. 创建一个 Java 的 maven 项目 添加 seatunnel-api 模块在 pom.xml中: \u003cdependency\u003e \u003cgroupId\u003eorg.apache.seatunnel\u003c/groupId\u003e \u003cartifactId\u003eseatunnel-api\u003c/artifactId\u003e \u003cversion\u003e${seatunnel.version}\u003c/version\u003e \u003c/dependency\u003e 创建一个新的类, 并实现 ConfigShade 接口, 这个接口有下列方法: /** * The interface that provides the ability to encrypt and decrypt {@link * org.apache.seatunnel.shade.com.typesafe.config.Config} */ public interface ConfigShade { /** * The unique identifier of the current interface, used it to select the correct {@link * ConfigShade} */ String getIdentifier(); /** * Encrypt the content * * @param content The content to encrypt */ String encrypt(String content); /** * Decrypt the content * * @param content The content to decrypt */ String decrypt(String content); /** To expand the options that user want to encrypt */ default String[] sensitiveOptions() { return new String[0]; } } 在 resources/META-INF/services 添加 org.apache.seatunnel.api.configuration.ConfigShade. 将其打成 jar 包, 并添加到 ${SEATUNNEL_HOME}/lib. 将选项 shade.identifier 的值更改为上面定义在配置文件中的 ConfigShade#getIdentifier 的值. 完成! ","date":"2023-05-25","objectID":"/seatunnel_encryption/:3:0","tags":["SeaTunnel","big data"],"title":"SeaTunnel 配置文件加解密","uri":"/seatunnel_encryption/"},{"categories":["big data"],"content":"连接器 V2 与 V1 的区别 自从 https://github.com/apache/incubator-seatunnel/issues/1608 之后添加了连接器 V2 功能. 连接器 V2 是基于 Seatunnel Connector API 接口定义的. 和连接器 V1 不同的是, 连接器 V2 支持以下功能: 多引擎支持: SeaTunnel Connector API 是一套独立与引擎的 API. 在这套 API 的基础上进行研发的连接器 V2 是可以运行在多个引擎上的. 现在是支持 Flink 和 Spark 的, 未来还会支持更多的引擎. 多引擎版本支持: 通过翻译层 (translation layer) 将连接器与引擎解耦, 解决了大多数连接器需要修改代码才能支持新版本的底层引擎的问题. 统一批处理和流处理: 连接器 V2 支持批处理和流处理. 不需要分别开发批处理和流处理的连接器. 多路复用的 JDBC/Log 连接器: 连接器 V2 支持 JDBC 资源重用和共享数据库日志解析. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:1:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"源连接器特点 (Source) 源连接器拥有一些共同的特点, 并且每种连接器不同程度的支持它们. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"exactly-once (精确一次) 如果数据源中的每条数据只会被源发送到下游一次, 则认为这个源连接器支持只写一次. 在 SeaTunnel 中, 可以在检查点时将读取的分割 (Split) 及其偏移量 (offset) (当时拆分中读取数据的位置, 如行号、字节大小、偏移量等) 保存为状态快照 (StateSnapshot) . 如果任务重新启动, 我们将获得最后一个状态快照, 然后定位上次读取的分割和偏移量并继续向下游发送数据. 例如: File, Kafka ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:1","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"column projection (列投影) 如果连接器支持仅从数据源读取指定的列 (注意! 如果先读取所有的列, 然后通过 schema 过滤不需要的列, 那么这种不是真正的列投影). 例如 JDBC 源 可以用 sql 定义读取的列. Kafka 源 会读取指定主题的所有内容, 然后使用 schema 过滤不需要的列, 这个就不是列投影. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:2","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"batch (批) 批处理模式, 数据读取是有边界的, 当所有数据被读取完之后作业就会结束. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:3","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"stream (流) 流处理模式, 数据读取是无边界的, 并且作业是不会停止的. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:4","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"parallelism (并行度) 并行源连接器支持配置 parallelism, 每个并行度都回创建一个任务来读取数据. 在并行源连接器中, 源会被拆分为多个, 然后通过枚举器 (enumerator) 分配给源阅读器 (SourceReader) 进行处理. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:5","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"support user-defined split (支持用户定义分割) 用户可以配置分割规则. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:2:6","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"接收连接器特点 (Sink) 接收连接器拥有一些共同的特点, 并且每种连接器不同程度的支持它们. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:0","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"exactly-once (精确一次) 当任何数据流入分布式系统时, 如果系统在整个处理过程中只对任何一条数据准确地处理了一次, 并且处理结果是正确的, 则认为系统满足了精确的一次一致性. 对于接收连接器, 如果任何数据只写入目标一次, 则接收连接器支持精确一次. 通常有两种方法可以实现这一点: 目标数据源支持键消重 (key deduplication), 例如: MySQL, Kudu. 目标支持 XA Transaction (此事务可以跨会话使用. 即使创建事务的程序已经结束了, 新创建的程序只需要知道上一个事务的 ID 来重新提交或回滚事务). 所以可以使用两段提交 (Two-phase Commit) 来确保 exactly-once. 例如: File, MySQL. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:1","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["big data"],"content":"cdc (change data capture 变更数据捕获) 如果接收连接器支持基于主键的写入行类型 (INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE), 那么则认为支持 cdc. ","date":"2023-05-24","objectID":"/seatunnel_connector_v2/:3:2","tags":["SeaTunnel","big data"],"title":"Seatunnel 连接器 V2 版本","uri":"/seatunnel_connector_v2/"},{"categories":["DatasourceX"],"content":"前提 之前项目里用的数据源连接都是 DatasourceX , 但是这个项目从2022年5月之后就没有维护过了, 其中也有一些bug都没有修复. 所以想着要不要自己写一个, 或者再看看有没有其他的. 前两天在ChatGPT上询问了一下有没有类似的工具的时候, 推荐了同样是袋鼠云的 Taier , 突然发现从 Taier 的 1.3.0 版本之后 DatasourceX 就移到这个项目中了. 所以就想着从中剥离出来 DatasourceX 放到自己的项目中去. 现在记录一下这个过程. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:1","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"开始 第一步肯定是先把代码下载下来: (这里下载的是1.3.0-RELEASE) \u003e git clone https://github.com/DTStack/Taier.git -b 1.3.0-RELEASE 把代码下载下来之后找到 DatasoruceX 的目录: taier-datasource 这个模块中有两个子模块: taier-datasource-api 和 taier-datasource-plugin. 其中: taier-datasource-api 这个就是需要剥离出来的接口. taier-datasource-plugin 这个里面就是各种数据源的依赖, 现在就不要这个了, 因为 GitHub 上已经下载好了现成的: plugins 传送门 在打包之前先修改一下 taier-datasource.pom.xml: 将 slf4j-api 的 \u003cscope\u003e 去掉, 否则打完包会报找不到 slf4j 的错: \u003cdependency\u003e \u003cgroupId\u003eorg.slf4j\u003c/groupId\u003e \u003cartifactId\u003eslf4j-api\u003c/artifactId\u003e \u003cversion\u003e1.7.21\u003c/version\u003e \u003c/dependency\u003e 在 \u003cbuild\u003e 中加入 maven-assembly-plugin 插件, 目的是将 taier-datasource-api 的依赖也打到包里面去: \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-assembly-plugin\u003c/artifactId\u003e \u003cversion\u003e2.4.1\u003c/version\u003e \u003cconfiguration\u003e \u003cdescriptorRefs\u003e \u003cdescriptorRef\u003ejar-with-dependencies\u003c/descriptorRef\u003e \u003c/descriptorRefs\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003emake-assembly\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003esingle\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e 这样打包之后会同时生成一个后面带 jar-with-dependencies 的包, 这个就是我们需要的了. mvn clean install 或者 mvn clean package. 这样就打包成功了. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:2","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"导入 这样就生成了一个本地的 jar 包, 这里我用的导入方法是先将这个 jar 包导入到本地的 maven 仓库中. 导入到 maven 本地仓库: \u003e mvn install:install-file -Dfile=上面打包的jar包路径 -DgroupId=com.dtstack.taier -DartifactId=taier.datasource.x -Dversion=1.0.0 -Dpackaging=jar 在项目的 pom 中引入: \u003cdependency\u003e \u003cgroupId\u003ecom.dtstack.taier\u003c/groupId\u003e \u003cartifactId\u003etaier.datasource.x\u003c/artifactId\u003e \u003cversion\u003e1.0.0\u003c/version\u003e \u003c/dependency\u003e ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:3","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["DatasourceX"],"content":"配置 这里需要对插件的载入重写一下: import com.dtstack.taier.datasource.api.base.ClientCache; import com.dtstack.taier.datasource.api.constant.ConfigConstants; import com.dtstack.taier.datasource.api.context.ClientEnvironment; import com.dtstack.taier.datasource.api.manager.list.ClientManager; import com.dtstack.taier.datasource.api.config.Configuration; private void p_initDatasourcePluginsDir() { String pluginsDir = 这里是插件路径 Map\u003cString, Object\u003e config = new HashMap\u003c\u003e(); config.put(ConfigConstants.PLUGIN_DIR, pluginsDir); Configuration configuration = new Configuration(config); ClientEnvironment clientEnvironment = new ClientEnvironment(configuration); clientEnvironment.start(); ClientCache.setEnv(clientEnvironment.getManagerFactory().getManager(ClientManager.class)); } 至此, 这个就已经把 Taier 中的 DatasourceX 移进来了. ","date":"2023-04-27","objectID":"/taier_datasourcex/:0:4","tags":["DatasourceX","Taier","Work","DTStack"],"title":"关于剥离 Taier 中 DatasourceX 组件","uri":"/taier_datasourcex/"},{"categories":["tools"],"content":" IINA 作为一款开源的 Swift 编写的播放器, 我愿称之为 mac 最好用的播放器 ","date":"2023-04-21","objectID":"/iina/:0:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装IINA IINA的安装可以点开上面的地址进行安装. ","date":"2023-04-21","objectID":"/iina/:1:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装youtube-dl \u003e brew install youtube-dl 安装好之后可以也可以把ffmpeg安装一下 \u003e brew install ffmpeg ","date":"2023-04-21","objectID":"/iina/:2:0","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"配置IINA 配置IINA youtube-dl路径: /usr/local/bin 额外参数: format=\"bestvideo[height\u003c=?480]+bestaudio/best[height\u003c=?480]\" 参数意义为播放视频分辨率 ","date":"2023-04-21","objectID":"/iina/:2:1","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["tools"],"content":"安装IINA浏览器插件 Open In IINA - Chrome 应用商店 (google.com) ","date":"2023-04-21","objectID":"/iina/:2:2","tags":["tools"],"title":"IINA配置youtube-dl","uri":"/iina/"},{"categories":["big data"],"content":" 搭建 CDH v6.2.0 (离线版) ","date":"2023-03-01","objectID":"/cdh/:0:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"简介 ","date":"2023-03-01","objectID":"/cdh/:1:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"官网介绍 CDH是Cloudera的100%开源平台发行版, 包括Apache Hadoop, 专为满足企业需求而构建. CDH 提供开箱即用的企业使用所需的一切. 通过将 Hadoop与十几个其他关键的开源项目集成, Cloudera 创建了一个功能先进的系统, 可帮助您执行端到端的大数据工作流程. ","date":"2023-03-01","objectID":"/cdh/:1:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"简单来说 CDH 是一个拥有集群自动化安装、中心化管理、集群监控、报警功能的一个工具(软件), 使得集群的安装可以从几天的时间缩短到几个小时, 运维人数也会从几个人降低到几个人, 极大的提高了集群管理的效率. 提示: 所有的操作, 均在root用户下进行!!! ","date":"2023-03-01","objectID":"/cdh/:1:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"环境准备 ","date":"2023-03-01","objectID":"/cdh/:2:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"集群机器 虚拟机IP hostname Hadoop MySQL Spark ZooKeeper 192.168.100.101 cdh1 ✅ ✅ ✅ ✅ 192.168.100.102 cdh2 ✅ ✅ ✅ 192.168.100.103 cdh3 ✅ ✅ ✅ ","date":"2023-03-01","objectID":"/cdh/:2:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"准备安装包 jdk: oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm MySQL: mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar mysql-connector-java-5.1.47.jar cloudera-repos-6.2.0: cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm parcel-6.2.0: CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha PHOENIX-1.0.jar PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel.sha manifest.json 下载地址: 百度网盘 ","date":"2023-03-01","objectID":"/cdh/:2:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"环境安装 如果没关防火墙, 关一下: systemctl stop firewalld.service 因为这是我自己的虚拟机, 所以我索性让防火墙不启动了: systemctl disable firewalld.service ","date":"2023-03-01","objectID":"/cdh/:3:0","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"修改 hostname 和 hosts (所有节点) 设置 hostname [root@localhost ~]# vi /etc/hostname cdh1 [root@localhost ~]# vi /etc/sysconfig/network # Created by anaconda NETWORKING=yes HOSTNAME=server4 设置 hosts [root@localhost ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.100.101 cdh1 192.168.100.102 cdh2 192.168.100.103 cdh3 重启! [root@localhost ~]# reboot 同样的操作在三台服务器上! 记得对应不同的 hostsname 这里之后我还是习惯用vim, 所以就安装了vim: yum install -y vim ","date":"2023-03-01","objectID":"/cdh/:3:1","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"SSH免密登录 (所有节点) 执行指令: [root@cdh1 ~]# ssh-keygen -t rsa -P \"\" Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:jGyqRTlDiqwQOP/Id2hVDhKNuigVipeSrkafdVXzP4s root@cdh1 The key's randomart image is: +---[RSA 2048]----+ | .o | |. . ... o | |=o +o . . . o | |*==o + * . . | |+*o.= = S . | |=+.+ O . o | |= + X o . o| |.. B . E . | |. . | +----[SHA256]-----+ 查看生成的公钥和私钥 [root@cdh1 ~]# cd .ssh/ [root@cdh1 .ssh]# ll 总用量 8 -rw-------. 1 root root 1679 3月 1 08:20 id_rsa -rw-r--r--. 1 root root 391 3月 1 08:20 id_rsa.pub 生成 authorized_keys 文件, 并把公钥刚进去 [root@cdh1 .ssh]# cp id_rsa.pub authorized_keys 然后将 cdh2 和 cdh3 的公钥拷贝进去 [root@cdh1 .ssh]# cat authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRqj0kcgwR68Ci2otjFu5QYQe/wBlyDpX9eI68XU6q6XzyxTrvLkbEpDpHkNwMpmqfr+y/tOMpk5/dAIBUn3pOiRKBpXVDxA4xZkpIpC1PXHi3BxY8zz58QXgBQbm1tFxFplAzkCjlxsYE08Oq0X/xD3vR6T4ZRsFfMdKMo7R2LmyoPohDTf8aiiqvE6ftF+Tv8YmNdb0TKbNwgD76f5vatrYhRZ+XitHCC8NGffDyOA62ogkd04G9mXOKYEhvsu7eYHa98xddhiXmK0mdeBRcV4BfeEkIEBdlcYl1LpYNjKR1oePFzudfG7czeKltgdPTqy5wPvAEPic9HBw3t2St root@cdh1 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDEhVBXXBXUiz21kxafL7FadgIrKgNJlvds+Gwf5G9fB+28kXyDgvRFL5F1twCGlIakt266K2kuZioffcwWJ3Co5N6XlyF/ABHq4cxjD1js9uKbXVENDove8CCR8zCIMmojJqEZO53TUljJjau+TEQQrXBaSUcQ6dkmMSEo7XuPYbCukXcZY4xX4tXX4XQyUFnX0T3xGkFkgHZ6JjPMBEo1deWpJn0igt4LEZcdfBNCywmZr4bIFDVnpER7gmZJB15AW1cEqy8++2qL5a/acee85olRzLeGWFZTkP4XRH5Rqkva7U924f+8uWZQefz7VTlcdruhmLJNhN+DJntrL/Sr root@cdh2 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsBR+l8JM2NMR9JJ8XeQaFV50VFwto6ZJkkdygj6fQPbGms7XYQLTutNRaERXgQPCNgfz2Y7ZncIu0zNoK5ibmFH8qU1RGvpkscTX0AaBnrRtpESBH5R/Gn0ZCqsjSE829GC+VkSxwHgKkrgS63iWVhUjNnkcoHiBueiyR6c8NHxpC0PrM9ePazYGFo/LvjlFFtHXzQYHlA5Dldk6oPm2pKOof1oEuWYhPvsIPluyExZpDeg0dSZaJSWNXJlaHgnO0lp2O2rUjY+A6FVxVh1VbKZlj0R4ZP5WHKkkGZjibdz810iR8VK+COsGXRwLhc+BmGVf10ik/Z2k2qISHKBUZ root@cdh3 再将 authorized_keys 分发到 cdh2 和 cdh3 [root@cdh1 .ssh]# scp authorized_keys root@cdh2:/root/.ssh/ [root@cdh1 .ssh]# scp authorized_keys root@cdh3:/root/.ssh/ 测试连接 [root@cdh1 .ssh]# ssh cdh2 Last failed login: Wed Mar 1 08:29:01 EST 2023 from cdh1 on ssh:notty There were 4 failed login attempts since the last successful login. Last login: Wed Mar 1 08:10:27 2023 from 192.168.100.1 [root@cdh2 ~]# 到此, 免密登录就完成啦! 😁 ","date":"2023-03-01","objectID":"/cdh/:3:2","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 NTP 服务 (所有节点) 使用 NTP 的目的是对网络内所有具有时钟的设备进行时钟同步, 使网络内所有设备的时钟保持一致, 从而使设备能够提供基于统一时间的多种应用. 关闭 chronyd 服务 (所有节点) 因为 CentOS 默认的使用的是 chronyd 服务进行时间同步的, 所以先关掉 [root@cdh1 ~]# systemctl stop chronyd [root@cdh1 ~]# systemctl disable chronyd 修改时区 (改为中国标准时区) (所有节点) [root@cdh1 ~]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 安装 NTP (所有节点) [root@cdh1 ~]# yum -y install ntp 修改 NTP 服务器配置 (cdh1) [root@cdh1 ~]# vim /etc/ntp.conf ... # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). # server 0.centos.pool.ntp.org iburst # server 1.centos.pool.ntp.org iburst # server 2.centos.pool.ntp.org iburst # server 3.centos.pool.ntp.org iburst server 0.cn.pool.ntp.org iburst server 1.cn.pool.ntp.org iburst server 2.cn.pool.ntp.org iburst server 3.cn.pool.ntp.org iburst ... 修改同步配置 (cdh1) [root@cdh1 ~]# vim /etc/sysconfig/ntpd # Command line options for ntpd OPTIONS=\"-g\" SYNC_HWCLOCK=yes 重启 NTP 服务 (cdh1) [root@cdh1 ~]# systemctl restart ntpd 配置其余服务器 (cdh2, cdh3) 修改服务器配置 [root@cdh2 ~]# vim /etc/ntp.conf ... # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server cdh1 iburst ... 手动同步时间 [root@cdh3 ~]# systemctl stop ntpd [root@cdh3 ~]# date 2023年 03月 01日 星期三 22:22:46 CST [root@cdh3 ~]# ntpdate cdh1 3 Mar 10:53:47 ntpdate[10943]: step time server 192.168.100.101 offset 131446.778897 sec [root@cdh3 ~]# systemctl start ntpd [root@cdh3 ~]# date 配置开机启动(所有节点) [root@cdh3 ~]# systemctl enable ntpd ","date":"2023-03-01","objectID":"/cdh/:3:3","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 httpd (所有节点) [root@cdh1 ~]# yum -y install httpd [root@cdh1 ~]# systemctl start httpd [root@cdh1 ~]# systemctl enable httpd ","date":"2023-03-01","objectID":"/cdh/:3:4","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 xsync (所有节点) xsync 是对 rsync 的二次封装, 所以先安装 rsync [root@cdh1 ~]# yum install -y rsync 在用户主目录的bin目录下添加脚本: [root@cdh1 ~]# mkdir bin [root@cdh1 ~]# vim ./bin/xsync 脚本内容: #!/bin/sh #1. 判断参数个数 if [ $# -lt 1 ] then echo Not Enought Arguement! exit; fi #2. 遍历集群所有机器 for host in cdh1 cdh2 cdh3 do echo ============== $host ============== #3. 遍历所有目录, 挨个发送 for file in $@ do #4. 判断文件是否存在 if [ -e $file ] then #5. 获取父目录 pdir=$(cd -P $(dirname $file); pwd) #6. 获取当前文件的名称 fname=$(basename $file) ssh $host \"mkdir -p $pdir\" rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi done done 赋权限: [root@cdh1 ~]# chmod 755 ./bin/xsync 测试一下: [root@cdh1 ~]# xsync ./bin ============== cdh1 ============== sending incremental file list sent 60 bytes received 12 bytes 48.00 bytes/sec total size is 824 speedup is 11.44 ============== cdh2 ============== sending incremental file list ./bin ./bin/xsync sent 934 bytes received 38 bytes 1,944.00 bytes/sec total size is 824 speedup is 0.85 ============== cdh3 ============== sending incremental file list ./bin/ ./bin/xsync sent 934 bytes received 38 bytes 1,944.00 bytes/sec total size is 824 speedup is 0.85 然后到其他机器上看一下文件是不是真的分发下去了. ","date":"2023-03-01","objectID":"/cdh/:3:5","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"配置 xcall (所有节点) xcall的作用就是所有节点都执行命令的时候, 只需要在一台服务器上执行就可以了. #!/bin/bash # 获取控制台指令 cmd=$* # 判断指令是否为空 if [ ! -n \"$cmd\" ] then echo \"command can not be null !\" exit fi # 获取当前登录用户 user=`whoami` # 在从机执行指令,这里需要根据你具体的集群情况配置，host与具体主机名一致 for host in cdh1 cdh2 cdh3 do echo \"================current host is $host=================\" echo \"--\u003e excute command \\\"$cmd\\\"\" ssh $user@$host $cmd done echo \"excute successfully !\" 测试一下: [root@cdh1 ~]# xcall ip addr /root/bin/xcall:行8: ((: #ip addr -eq # : 语法错误: 期待操作数 （错误符号是 \"#ip addr -eq # \"） ================current host is cdh1================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:7a:6b:88 brd ff:ff:ff:ff:ff:ff inet 192.168.100.101/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe7a:6b88/64 scope link valid_lft forever preferred_lft forever ================current host is cdh2================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:ee:a3:5c brd ff:ff:ff:ff:ff:ff inet 192.168.100.102/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feee:a35c/64 scope link valid_lft forever preferred_lft forever ================current host is cdh3================= --\u003e excute command \"ip addr\" 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:fe:d4:de brd ff:ff:ff:ff:ff:ff inet 192.168.100.103/24 brd 192.168.100.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fefe:d4de/64 scope link valid_lft forever preferred_lft forever excute successfully ! ","date":"2023-03-01","objectID":"/cdh/:3:6","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 JDK (所有节点) 因为我这里是新的 minimal 的虚拟机, 所以肯定没有安装过 jdk, 这里就直接安装了. 有一点需要说明一下, 一定要安装 cdh 对应版本的 jdk!!! [root@cdh1 ~]# rpm -ivh /opt/oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm 这一步这三台一起安装, 安装包就可以用 xsync 同步啦. 配置环境变量 [root@cdh1 ~]# vim /etc/profile.d/java.sh export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera export PATH=$JAVA_HOME/bin:$PATH 然后分发一下, 后面需要分发的地方就不写命令了 [root@cdh1 ~]# xsync /etc/profile.d/java.sh 所有服务器都刷新一下环境变量 [root@cdh1 ~]# source /etc/profile [root@cdh1 ~]# java -version java version \"1.8.0_181\" Java(TM) SE Runtime Environment (build 1.8.0_181-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) ","date":"2023-03-01","objectID":"/cdh/:3:7","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 MySQL (cdh1) 创建目录 [root@cdh1 ~]# mkdir /usr/share/java 上传 mysql-connector-java.jar [root@cdh1 ~]# ll /usr/share/java/ 总用量 984 -rw-r--r--. 1 root root 1007502 3月 3 11:34 mysql-connector-java.jar 解压安装包 [root@cdh1 mysql]# cd /opt/mysql/ [root@cdh1 mysql]# ll 总用量 1036896 -rw-r--r--. 1 root root 530882560 3月 3 11:52 mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar [root@cdh1 mysql]# tar -xf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar [root@cdh1 mysql]# ll 总用量 1036896 -rw-r--r--. 1 root root 530882560 3月 3 11:52 mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar -rw-r--r--. 1 7155 31415 25381952 4月 15 2019 mysql-community-client-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 280904 4月 15 2019 mysql-community-common-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 3838100 4月 15 2019 mysql-community-devel-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 47076368 4月 15 2019 mysql-community-embedded-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 24086952 4月 15 2019 mysql-community-embedded-compat-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 130023844 4月 15 2019 mysql-community-embedded-devel-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 2274268 4月 15 2019 mysql-community-libs-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 2118444 4月 15 2019 mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 173541272 4月 15 2019 mysql-community-server-5.7.26-1.el7.x86_64.rpm -rw-r--r--. 1 7155 31415 122249684 4月 15 2019 mysql-community-test-5.7.26-1.el7.x86_64.rpm 安装 因为我安装的时候, 遇到了 mariadb 的依赖冲突, 所以先解决一下这个问题: [root@cdh1 mysql]# rpm -qa | grep mariadb mariadb-libs-5.5.56-2.el7.x86_64 [root@cdh1 mysql]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 然后继续安装 MySQL [root@cdh1 mysql]# rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-devel-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm [root@cdh1 mysql]# rpm -ivh mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpm 启动 MySQL [root@cdh1 mysql]# mysqld --initialize --user=root [root@cdh1 mysql]# cat /var/log/mysqld.log 2023-03-03T04:18:29.973382Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2023-03-03T04:18:30.126918Z 0 [Warning] InnoDB: New log files created, LSN=45790 2023-03-03T04:18:30.149953Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2023-03-03T04:18:30.218530Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 740806b3-b97a-11ed-96b3-000c297a6b88. 2023-03-03T04:18:30.220950Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2023-03-03T04:18:30.221618Z 1 [Note] A temporary password is generated for root@localhost: 7uTGbVWuja,. [root@cdh1 mysql]# systemctl enable mysqld [root@cdh1 mysql]# systemctl start mysqld 更改 MySQL 密码, 给 root 赋权限, 这几步就不写了, 网上有很多. 重新登录 root, 创建数据表: create database cmserver default charset utf8 collate utf8_general_ci; grant all on cmserver.* to 'cmserveruser'@'%' identified by 'root'; create database metastore default charset utf8 collate utf8_general_ci; grant all on metastore.* to 'hiveuser'@'%' identified by 'root'; create database amon default charset utf8 collate utf8_general_ci; grant all on amon.* to 'amonuser'@'%' identified by 'root'; create database rman default charset utf8 collate utf8_general_ci; grant all on rman.* to 'rmanuser'@'%' identified by 'root'; ","date":"2023-03-01","objectID":"/cdh/:3:8","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"安装 CM 组件 (所有节点) 上传安装包到各服务器 [root@cdh1 opt]# ll 总用量 1169544 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 9984 3月 3 12:47 cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm [root@cdh2 opt]# ll 总用量 1169532 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm [root@cdh3 opt]# ll 总用量 1169532 -rw-r--r--. 1 root root 10215488 3月 3 12:43 cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm -rw-r--r--. 1 root root 1187380436 3月 3 12:45 cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm 安装 (cdh1 安装 3 个, cdh2 和 cdh3 安装 2 个) yum localinstall -y cloudera-manager-daemons-6.2.0-968826.el7.x86_64.rpm yum localinstall -y cloudera-manager-agent-6.2.0-968826.el7.x86_64.rpm yum localinstall -y cloudera-manager-server-6.2.0-968826.el7.x86_64.rpm ","date":"2023-03-01","objectID":"/cdh/:3:9","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"CDH6.2.0 (cdh1) 上传安装包 [root@cdh1 parcel-repo]# cd /opt/cloudera/parcel-repo/ [root@cdh1 parcel-repo]# ll 总用量 2431572 -rw-r--r--. 1 root root 2087665645 3月 3 13:02 CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel -rw-r--r--. 1 root root 41 3月 3 13:00 CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha -rw-r--r--. 1 root root 33725 3月 3 13:00 manifest.json -rw-r--r--. 1 root root 402216960 3月 3 13:00 PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel -rw-r--r--. 1 root root 41 3月 3 12:59 PHOENIX-5.0.0-cdh6.2.0.p0.1308267-el7.parcel.sha [root@cdh1 parcel-repo]# cd /opt/cloudera/csd/ [root@cdh1 csd]# ll 总用量 8 -rw-r--r--. 1 root root 5306 3月 3 13:03 PHOENIX-1.0.jar 修改配置 cdh1 [root@cdh1 ~ ]# vim /etc/cloudera-scm-server/db.properties # Auto-generated by scm_prepare_database.sh on 2020年 10月 16日 星期五 14:26:26 CST # # For information describing how to configure the Cloudera Manager Server # to connect to databases, see the \"Cloudera Manager Installation Guide.\" # com.cloudera.cmf.db.type=mysql com.cloudera.cmf.db.host=localhost com.cloudera.cmf.db.name=cmserver com.cloudera.cmf.db.user=root com.cloudera.cmf.db.setupType=EXTERNAL com.cloudera.cmf.db.password=root cdh1, cdh2, cdh3 [root@cdh1 ~]# vi /etc/cloudera-scm-agent/config.ini ... [General] # Hostname of the CM server. server_host=cdh1 ... [root@cdh1 ~]# xsync /etc/cloudera-scm-agent/config.ini ","date":"2023-03-01","objectID":"/cdh/:3:10","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"修改Linux swappiness参数 (所有节点） [root@cdh1 ~]# vim /etc/sysctl.conf # sysctl settings are defined through files in # /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/. # # Vendors settings live in /usr/lib/sysctl.d/. # To override a whole file, create a new file with the same in # /etc/sysctl.d/ and put new settings there. To override # only specific settings, add a file with a lexically later # name in /etc/sysctl.d/ and put new settings there. # # For more information, see sysctl.conf(5) and sysctl.d(5). vm.swappiness=0 [root@cdh1 ~]# xsync /etc/sysctl.conf 所有节点执行 echo 0 \u003e /proc/sys/vm/swappiness ","date":"2023-03-01","objectID":"/cdh/:3:11","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"禁用透明页 (所有节点) [root@cdh1 ~]# vim /etc/rc.local #!/bin/bash # THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES # # It is highly advisable to create own systemd services or udev rules # to run scripts during boot instead of using this file. # # In contrast to previous versions due to parallel execution during boot # this script will NOT be run after all other services. # # Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure # that this script will be executed during boot. touch /var/lock/subsys/local echo never \u003e /sys/kernel/mm/transparent_hugepage/defrag echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled [root@cdh1 ~]# xsync /etc/rc.local 所有节点执行 [root@cdh1 ~]# chmod +x /etc/rc.d/rc.local [root@cdh1 ~]# source /etc/rc.local ","date":"2023-03-01","objectID":"/cdh/:3:12","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"启动 CM 和 CDH 集群 启动 server (cdh1) [root@cdh1 ~]# systemctl start cloudera-scm-server [root@cdh1 ~]# systemctl status cloudera-scm-server ● cloudera-scm-server.service - Cloudera CM Server Service Loaded: loaded (/usr/lib/systemd/system/cloudera-scm-server.service; enabled; vendor preset: disabled) Active: active (running) since 五 2023-03-03 13:11:11 CST; 7s ago Main PID: 13692 (java) CGroup: /system.slice/cloudera-scm-server.service └─13692 /usr/java/jdk1.8.0_181-cloudera/bin/java -cp .:/usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:lib/* -server -Dlog4j.configuration=fil... 启动 agent (所有节点) [root@cdh2 opt]# systemctl start cloudera-scm-agent [root@cdh2 opt]# systemctl status cloudera-scm-agent ● cloudera-scm-agent.service - Cloudera Manager Agent Service Loaded: loaded (/usr/lib/systemd/system/cloudera-scm-agent.service; enabled; vendor preset: disabled) Active: active (running) since 五 2023-03-03 13:13:03 CST; 10s ago Main PID: 11790 (cmagent) CGroup: /system.slice/cloudera-scm-agent.service └─11790 /usr/bin/python2 /opt/cloudera/cm-agent/bin/cm agent ","date":"2023-03-01","objectID":"/cdh/:3:13","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":"登录 Web 界面安装 登录 打开地址: http://cdh1:7180/cmf/login 登录名/密码: admin/admin 登录界面 勾选同意 勾选同意 选择免费版 选择免费版 进入安装页面啦! 安装页面 输入集群名 输入集群名 添加集群 添加集群 选择存储库 (默认就可以) 选择存储库 开始安装 安装 安装完成","date":"2023-03-01","objectID":"/cdh/:3:14","tags":["big data","CDH"],"title":"CDH环境搭建","uri":"/cdh/"},{"categories":["big data"],"content":" 该文档针对 Seatunnel v2.3.0版本. ","date":"2023-02-28","objectID":"/seatunnel_engine/:0:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"1. 下载 Seatunnel Engine是 Seatunnel 的默认引擎. Seatunnel 的安装包已经包含了所有Seatunnel Engine的内容. ","date":"2023-02-28","objectID":"/seatunnel_engine/:1:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"2. 配置SEATUNNEL_HOME 添加文件: /etc/profile.d/seatunnel.sh 添加内容: export SEATUNNEL_HOME=${seatunnel install path} export PATH=$PATH:$SEATUNNEL_HOME/bin ","date":"2023-02-28","objectID":"/seatunnel_engine/:2:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"3. 配置Seatunnel Engine的 JVM 参数 Seatunnel Engine支持两种方式设置 jvm 参数 在$SEATUNNEL_HOME/bin/seatunnel-cluster.sh中添加. 修改该文件: 在第一行添加内容JAVA_OPTS=\"-Xms2G -Xmx2G\" 在启动Seatunnel Engine的时候添加 jvm 参数. 🌰: seatunnel-cluster.sh -DJvmOption=\"-Xms2G -Xmx2G\" ","date":"2023-02-28","objectID":"/seatunnel_engine/:3:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4. 配置Seatunnel Engine Seatunnel Engine提供了很多需要在seatunnel.yaml(在$SEATUNNEL_HOME/config/下)中设置的参数. ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.1 Backup count (备份数) Seatunnel Engine实现了基于Hazelcast IMDG的集群管理. 集群的状态数据(任务运行状态, 资源状态)存放在Hazelcast IMap中. 这些状态数据在 Hazelcast IMap 中是分布式的并且是存储在所有集群节点中的. Hazelcast 会将数据分区存储在 Imap 中. 每个分区可以指定备份的数量. 因此, Seatunnel Engine可以在不使用其他服务(例如 zookeeper)的情况下实现集群. backup count参数定义了同步备份的数量. 举个🌰, 如果设置为1, 分区的备份将会存放在另一个分区上; 设置为2, 将会存放在另外两个分区上. backup count建议为min(1, max(5, N/2)), 其中N为集群节点数 seatunnel: engine: backup-count: 1 # other config ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.2 Slot service (插槽服务) 插槽的数量决定了集群节点可以并行运行的任务组的数量. Seatunnel Engine是一个数据同步引擎, 并且大多数的任务都是IO密集型的. 建议使用动态插槽: seatunnel: engine: slot-service: dynamic-slot: true # other config ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"4.3 Checkpoint Manager (检查点管理) 像是 FLink, Seatunnel Engine也支持 Chandy–Lamport 算法(分布式快照算法). Seatunnel Engine可以在没有数据丢失和重复的前提下实现数据同步. interval (间隔) 两个检查点之间的间隔单位是毫秒. 如果checkpoint.interval参数在任务的配置文件的env块中设置了, 这个值将会被覆盖掉. timeout (超时) 检查点的超时. 如果一个检查点没有在超时范围内完成的话, 那么这个检查点就触发失败. 因此, 任务就会被恢复. max-concurrent (最大并发) 最多可以同时执行多少个检查点. tolerable-failure (重试) 检查点失败后最大重试次数. seatunnel: engine: backup-count: 1 print-execution-info-interval: 10 slot-service: dynamic-slot: true checkpoint: interval: 300000 timeout: 10000 max-concurrent: 1 tolerable-failure: 2 checkpoint storage (检查点存储) todo ","date":"2023-02-28","objectID":"/seatunnel_engine/:4:3","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5. 配置Seatunnel Engine服务 所有Seatunnel Engine服务配置都在hazelcast.yaml文件中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5.1 cluster-name (集群名称) Seatunnel Engine节点通过集群名来判断对方是否和自身属于同一集群. 如果两个节点之间的集群名是不同的, 那么Seatunnel Engine会拒绝服务请求. ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"5.2 Network (网络) 基于Hazelcast,Seatunnel Engine集群是运行Seatunnel Engine服务的集群成员网络. 集群成员自动连接在一起组成一个集群. 这种自动加入是通过集群成员用来相互查找的各种发现机制进行的. 需要注意一点, 一旦集群建立, 无论是通过什么样的发现机制, 各集群成员之间的通信总是通过 TCP/IP 完成的. Seatunnel Engine使用以下发现机制: TCP 可以将Seatunnel Engine配置为一个完整的 TCP/IP 集群. 🌰 hazelcast: cluster-name: seatunnel network: join: tcp-ip: enabled: true member-list: - hostname1 port: auto-increment: false port: 5801 properties: hazelcast.logging.type: log4j2 TCP是在单独的SeaTunnel引擎集群中的建议方式. 另一方面, Hazelcast 提供了一些其他服务发现方法. 点此查看详情: Hazelcast Network ","date":"2023-02-28","objectID":"/seatunnel_engine/:5:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6. 配置SeaTunnel Engine客户端 所有SeaTunnel Engine客户端配置都在hazelcast-client.yaml文件中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6.1 cluster-name (集群名称) 客户端必须和Seatunnel Engine的cluster-name一致. 否则, Seatunnel Engine将会拒绝客户端的请求. ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:1","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"6.2 Network (网络) cluster-members (集群成员) 所有Seatunnel Engine服务节点地址需要添加在这: hazelcast-client: cluster-name: seatunnel properties: hazelcast.logging.type: log4j2 network: cluster-members: - hostname1:5801 ","date":"2023-02-28","objectID":"/seatunnel_engine/:6:2","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"7. 启动SeaTunnel Engine服务节点 mkdir -p $SEATUNNEL_HOME/logs nohup seatunnel-cluster.sh \u0026 日志会写在$SEATUNNEL_HOME/logs/seatunnel-server.log中. ","date":"2023-02-28","objectID":"/seatunnel_engine/:7:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data"],"content":"8. 安装Seatunnel Engine客户端 只需要将Seatunnel Engine节点上的$Seatunnel Engine目录复制到客户端节点, 并像Seatunnel Engine服务节点一样配置$Seatunnel Engine就可以了. ","date":"2023-02-28","objectID":"/seatunnel_engine/:8:0","tags":["SeaTunnel","big data"],"title":"部署Seatunnel引擎","uri":"/seatunnel_engine/"},{"categories":["big data","Hadoop"],"content":"介绍 HDFS (Hadoop Distributed File System) 是 Hadoop下的分布式文件系统, 具有高容错、高吞吐量等特性, 可以部署在低成本的硬件上. ","date":"2023-02-27","objectID":"/hdfs/:1:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"设计原理 ","date":"2023-02-27","objectID":"/hdfs/:2:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"架构 HDFS 遵从主/从架构, 由单个 NameNode(NN) 和 多个 DataNote(DN) 组成: NameNode: 负责执行有关文件系统命名空间的操作, 🌰: 打开, 关闭、重命名文件和目录等. 同事还负责集群元数据的存储, 记录着文件中各个数据块的位置信息. 也就是说主要是用来存储元数据的. DataNode: 负责提供来自文件系统客户端的读写请求, 执行块的创建, 删除等操作. 用来存储具体文件的. ","date":"2023-02-27","objectID":"/hdfs/:2:1","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"文件系统命名空间 HDFS 的文件系统命名空间的层次结构与大多数文件系统类似(🌰: Linux), 支持目录和文件的创建、移动、删除和重命名等操作, 支持配置用户和访问权限, 但不支持硬链接和软连接. NameNode负责维护文件系统名称空间, 记录对名称空间或其属性的任何更改. ","date":"2023-02-27","objectID":"/hdfs/:2:2","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"数据复制 由于 Hadoop 是被设计运行在廉价的机器上的, 这就一位置硬件其实是不可靠的. 所以为了保证高容错, HDFS 提供了数据复制机制. HDFS 将每个文件存储为一系列块, 每个块由多个副本来保证容错, 块的大小和复制因子可配置 (默认下, 块为128M, 复制因子为3). 数据复制的实现原理 大型的 HDFS 实例通常分布在多个机架的多台服务器上, 不同机架上的两台服务器之间通过交换机进行通讯. 大多数情况下, 同一机架中的服务期间的网络带宽大于不同机架中的服务器之间的带宽. 所以, HDFS 采用机架感知副本放置策略. 🌰, 对于默认情况, 当复制因子为3时, HDFS 的放置策略是: 在写入程序位于 DN 上时, 就优先将写入文件的一个副本放置在该 DN 上, 否则放在随机 DN 上. 之后另一个远程机架上的任意一个节点上放置另一个副本, 并在该机加上的另一个节点放置最后一个副本. 这么做的好处就是可以减少机架间的写入流量, 从而提高写入性能. 如果复制因子大于3, 则随机确定第4个和之后副本的放置位置, 同时需要保持每个机架的副本数量低于上限, 上限值通常为 (复制系数 - 1)/机架数 + 2. 注意: 不允许同一个 DN 上具有同一块的相同副本. 每个机架最多存储两份备份. 但是在这两个条件无法被满足的一些情况下, 这些条件会被忽略. 并且 HDFS 允许自定义布局算法. 副本的选择 副本的选择为了最大限度地减少带宽消耗和读取延迟, HDFS 在执行读取请求时, 优先读取距离读取其最近的副本(物理层面上). 也就是说, 如果与读取器节点相同的机架上存在副本, 则优先选择该副本. 如果 HDFS 集群跨越多个数据中心, 优先选择本地数据中心上的副本. ","date":"2023-02-27","objectID":"/hdfs/:2:3","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"架构的稳定性 心跳机制和重新复制 每个 DN 定期向 NN 发送心跳, 如果超过指定时间没有收到心跳, 会将该 DN 标记为已死亡. NN 不会将任何新的 IO请求 转发给标记为死亡的 DN, 也不会再使用已死亡的 DN 上的数据. DN 也会将其所有数据块列表定时发送给 NN, 并且在发送之前 DN 会检测校验和是否正常, 若不正常, 则不会发送给 NN. 所以 NN 会检测出那些数据块已经损坏. 由于数据不再可用, 可能会导致某些块副本数小于复制因子, 所以 NN 会跟踪这些块, 并在必要的时候进行重新复制. 数据的完整性 由于存储设备故障等原因, 存储在 DN 上的数据块也会发生损坏. 为了避免读取到一损坏的数据而导致错误, HDFS 提供了数据完整性机制来保存数据的完整性: 当客户端创建 HDFS 文件时, 它会计算文件的每个块的校验和, 并将校验和存储在同一 HDFS 命名空间下的单独的隐藏文件中. 当客户端检索文件内容时, 它会校验从每个 DN 接收的数据是否与存储在关联校验和文件中的校验和匹配. 如果不匹配, 代表数据已经损坏, 这个时候客户端会选择其他 DN 获取副本, 并重新校验. 元数据的磁盘故障 FsImage 和 EditLog 是 HDFS 的核心数据, 这些数据的以为丢失可能导致整个 HDFS 服务不可用. 为了避免, 可以配置 NN 使其支持 FsImage 和 EditLog 多副本同步, 这样 FsImage 或 EditLog 的任何改变都会引起每个副本的同步更新. 支持快照 快照支持在特定时刻存储数据副本, 在数据意外损坏时, 可以通过回滚操作恢复到简况的数据状态. ","date":"2023-02-27","objectID":"/hdfs/:2:4","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["big data","Hadoop"],"content":"特点 高容错 上面已经介绍了 HDFS 采用数据的多副本方案, 所以部分硬件的损坏并不会导致全部数据的丢失. 高吞吐量 HDFS 设计的重点是支持高吞吐量的数据访问, 而不是低延迟的数据访问. 大文件支持 文件大小应是 GB 到 TB 级别的. 简单一致性模型 HDFS 更适合一次写入多次读取 (write-once-read-many) 的访问模型. 支持将内容追加到文件末尾, 但不支持数据的随机访问, 不能从文件任意位置新增数据. 跨平台移植性 HDFS 具有良好的跨平台移植性, 使得其他大数据计算框架都将其作为数据持久化存储的首选方案. ","date":"2023-02-27","objectID":"/hdfs/:3:0","tags":["big data","Hadoop","HDFS"],"title":"Hadoop分布式文件系统——HDFS","uri":"/hdfs/"},{"categories":["Kafka"],"content":"生产者消息发送流程 ","date":"2022-09-02","objectID":"/kafka_3/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"发送原理 在消息发送的过程中, 涉及到了两个线程–main线程和Sender线程. 在 main 线程中创建了一个双端队列RecordAccumulator. main 线程将消息发送个 RecordAccumulator, Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker. 发送流程 ","date":"2022-09-02","objectID":"/kafka_3/:1:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者重要参数 参数名称 描述 bootstrap.servers 生产者连接集群所需的 boker 地址清单. 可以设置一个或多个, 中间用,隔开. 注意: 并非需要所有的 broker 地址, 因为生产者从给定的 broker 里查找其他 broker 信息 key.serializer和value.serializer 指定发送消息的 key 和 value 的序列化类型. 一定要写全类名! buffer.memory RecordAccumulator 缓冲区总大小, 默认32m. batch.size 缓冲区一批数据最大值, 默认16k. (适当增加可以提高吞吐量; 但是如果过大的话, 会导致数据传输延迟增加). linger.ms 如果数据一直未到达 batch.size, sender 等待 linger.ms 之后就会发送数据, 单位是ms. 默认0ms, 表示没有延迟. (生产环境一般 5-100ms 之间) acks 0: 生产者发送过来的数据, 不需要等数据落盘应答; 1: 生产者发送过来的数据, Leader 收到数据后应答; -1(all): 生产者发送过来的数据, Leader+ 和 isr 队列里面的所有节点收齐数据后应答. 默认是-1, -1和all是等价的. max.in.flight.request.per.connection 允许最多没有返回 ack 的次数, 默认为5, 开启幂等性要保证该值是 1-5 的数字. retries 当消息发送出现错误的时候, 系统会重发消息. 该值表示重复次数. 默认为int最大值,2147483647. 如果设置了重试, 还想保证消息的有序性, 需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候, 其他的消息可能发送成功了. retry.backoff.ms 两次重试之间的时间间隔, 默认为100ms. enable.idempotence 是否开启幂等性, 默认为true, 开启幂等性. compression.type 生产者发送的所有数据的压缩方式. 默认为none, 也就是不压缩. 支持压缩类型: none, gzip, snappy, lz4和zstd. ","date":"2022-09-02","objectID":"/kafka_3/:1:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者分区 ","date":"2022-09-02","objectID":"/kafka_3/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"分区的好处 便于合理使用存储资源, 每个 Partition 在一个 Broker 上存储, 可以吧海量的数据按照分区切割成小块存储在多台 Broker 上. 合理控制分区的任务, 可以实现负载均衡的效果. 提高并行度, 生产者可以以分区为单位发送数据; 消费者可以以分区为单位消费数据. ","date":"2022-09-02","objectID":"/kafka_3/:2:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"数据传递语义 至少一次(At Least Once) = ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2 最多一次(At Most Once) = ACK级别设置为0. 精确一次(Exactly Once): 对于一些非常重要的信息, 例如和钱相关的数据, 要求数据既不能重复也不能丢失. 也就是说, At Least Once 可以保证数据不丢失, 但是不能保证数据不重复; At Most Once 可以保证数据不重复, 但是不能保证数据不丢失. ","date":"2022-09-02","objectID":"/kafka_3/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"幂等性 ","date":"2022-09-02","objectID":"/kafka_3/:4:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"幂等性原理 幂等性就是指 Producer 不论向 Broker 发送多少次重复数据, Broker 端都只会持久化一条, 保证了不重复. 所以上面的 Exactly Once = 幂等性 + At Least Once(ack=-1 + partitions\u003e=2 + ISR最小副本数\u003e=2) 重复数据的判断标准: 具有\u003cPID, Partition, SeqNumber\u003e相同逐渐的消息提交时, Borker 只会持久化一条. 其中: PID 是 Kafka 每次重启都会分配一个新的; Partition 表示分区号 Sequence Number 单调自增 综上, 幂等性只能保证的是在单分区单会话内不重复. ","date":"2022-09-02","objectID":"/kafka_3/:4:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"如何使用幂等性 开启参数enable.idempotence. ","date":"2022-09-02","objectID":"/kafka_3/:4:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"生产者事务 开启事务, 必须开启幂等性! Kafka事务原理 ","date":"2022-09-02","objectID":"/kafka_3/:5:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka生产者","uri":"/kafka_3/"},{"categories":["Kafka"],"content":"主题命令行操作 查看操作主题命令参数 $ bin/kafka-topics.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 --create 创建主题 --delete 删除主题 --alter 修改主题 --list 查看所有主题 --describe 查看主题详细描述 --partitions \u003cInteger:# of partitions\u003e 设置分区数 --replication-factor \u003cInteger: replication factor\u003e 设置分区副本 --config \u003cString:name=value\u003e 更新系统默认配置 查看当前服务器中的所有 topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --list 创建 first topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first 查看 first 主题详情 $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first 修改分区数 (注意: 分区数只能增加, 不能减少!!!) $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3 删除topic $ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first ","date":"2022-09-02","objectID":"/kafka_2/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"生产者命令行操作 查看操作生产者命令参数 $ bin/kafka-console-producer.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 发送消息 $ bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first \u003ehello world \u003ehello buli-home ","date":"2022-09-02","objectID":"/kafka_2/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"消费者命令行操作 查看操作生产者命令参数 $ bin/kafka-console-producer.sh 参数 描述 --bootstrap-server \u003cString:server toconnect to\u003e 连接的 Kafka Broker 主机名称和端口号 --topic \u003cString:topic\u003e 操作的 topic 名称 --from-beginning 从头开始消费 --group \u003cString:consumer group id\u003e 指定消费者组名称 消费消息 消费 first 主题中的数据 $ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first 把主题中所有的数据都读取出来 (包括历史数据) $ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first ","date":"2022-09-02","objectID":"/kafka_2/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka命令行操作","uri":"/kafka_2/"},{"categories":["Kafka"],"content":"定义 Kafka传统定义: Kafka 是一个分布式的基于发布/订阅模式的消息队列(Message Queue) 发布/订阅: 消息的发布者不会讲消息直接发送给特定的订阅者, 而是将发布的消息分为不同的类型, 订阅者只接受感兴趣的消息. Kafka最新定义: Kafka是一个开元的分布式事件流平台(Event Streaming Platform), 被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用. ","date":"2022-07-26","objectID":"/kafka_1/:1:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列 目前比较常见的消息队列产品主要有Kafka、ActiveMQ、RabbitMQ、RocketMQ等. 大数据场景主要采用 Kafka, JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ . ","date":"2022-07-26","objectID":"/kafka_1/:2:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"传统消息队列的应用场景 缓存/消峰: 有助于控制和优化数据流经过系统的速度, 解决生产消息和消费消息的处理速度不一致的情况. 解耦: 允许独立的扩展或修改两边的处理过程, 只要确保它们遵守同样的接口约束. 异步通信: 允许用户把一个消息放入队列, 但并不立即处理它, 然后在需要的时候再去处理它们. ","date":"2022-07-26","objectID":"/kafka_1/:2:1","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"消息队列的两种模式 点对点模式: 消费者主动拉取数据, 消息收到后清除消息 发布/订阅模式: 可以有多个 topic(主题) (浏览、点赞、收藏、评论等) 消费者消费数据之后, 不能删除数据 每个消费者相互独立, 都可以消费到数据 ","date":"2022-07-26","objectID":"/kafka_1/:2:2","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["Kafka"],"content":"Kafka基础框架 Kafka基础框架 Producer: 消息生产者, 向 Kafka broker 发消息的客户端. Consumer: 消息消费者, 向 Kafka broker 取消息的客户端. Consumer Group(CG): 消费者组, 由多个 consumer 组成. 消费者组内每个消费者负责消费不同分区的数据, 一个分区只能由一个组内消费者消费; 消费者组之间互不影响. 所有的消费者都属于某个消费者组, 即消费者组是逻辑上的一个订阅者. Broker: 一台 Kafka 服务器就是一个 broker. 一个集群由多个 broker 组成. 一个 broker 可以容纳多个topic. Topic: 可以理解为一个队列, 生产者和消费者面向的都是一个 topic. Partition: 为了实现扩展性, 一个非常大的 topic 可以分布到多个 broker(即服务器) 上, 一个 topic 可以分为多个 partition, 每个 partition 是一个有序的队列. Replica: 副本. 一个 topic 的每个分区都有若干个副本, 一个 Leader 和若干个 Follower. Leader: 每个分区多个副本的\"主\", 生产者发送数据的对象, 以及消费者消费数据的对象都是Leader. Follower: 每个分区多个副本中的\"从\", 实时从 Leader 中同步数据, 保持和 Leader 数据的同步. Leader 发生故障时, 某个 Follower 会成为新的 Leader. ","date":"2022-07-26","objectID":"/kafka_1/:3:0","tags":["Kafka","Message Queue","big data"],"title":"Kafka概述","uri":"/kafka_1/"},{"categories":["canal"],"content":"Canal概述 ","date":"2022-07-26","objectID":"/canal/:1:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的起源 阿里巴巴 B2B 公司, 因为业务的特性, 卖家主要集中在国内, 买家主要集中在国外, 所以衍生出了同步杭州和美国异地机房的需求, 从2010年开始, 阿里系公司开始逐步的尝试基于数据库的日志解析, 获取增量变更进行同步, 由此衍生出了增量 订阅\u0026消费 的业务. Canal 是用 Java 开发的基于数据库增量日志解析, 提供增量数据 订阅\u0026消费 的中间件. 目前, Canal 主要支持了 MySQL 的 Binlog 解析, 解析完成后才利用 Canal Client 来处理获得的相关数据. (数据库同步需要阿里的 Otter 中间件, 基于 Canal ). ","date":"2022-07-26","objectID":"/canal/:1:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的Binlog 什么是Binlog MySQL 的二进制日志可以说是 MySQL 最重要的日志了, 它记录了所有的 DDL 和 DML (除了数据查询语句) 语句, 以事件行驶记录, 还包括语句所执行的消耗的时间. Binlog是事务安全性的. 一般来说, SQL语言分为三类: DML(Data Manipulation Language): 数据操纵语言, 最常用的增删改查就是这类, 操作对象是数据表中的记录 DDL(Data Definition Language): 数据定义语言, 例如建库、建表等 DCL(Data Control Language): 数据控制语言, 如 Grant、Rollback 等, 常见于数据库安全管理 Binlog两个最重要的使用场景: MySQL Replication 在 Master 端开启 Binlog, Master 把它的 Binlog 传递给 Slaves 来达到 Master-Slave 数据一致的目的 数据恢复, 通过使用 MySQL Binlog 工具来使恢复数据 Binlog分类 MySQL Binlog 的格式有三种, 分别是 STATEMENT,MIXED,ROW . 在配置文件中可以选配: binlog_format=statement|mixed|row statement: 语句级, binlog 会记录每次执行写操作的语句. 相对row模式节省空间, 但是可能产生不一致性. 🌰: update table set create_date=now(), 如果用这种模式进行恢复, 由于执行时间的不同产生的数据就可能不同. 优势: 节省空间 劣势: 可能造成数据不一致 row: 行级, binlog 会记录每次操作后每行记录的变化. 优势: 保持数据的绝对一致性 劣势: 占用空间大 mixed: statement的升级版, 一定程度上解决了因为一些情况而造成的 statement模式 不一致的问题. mixed默认还是statement, 在某些情况下(🌰): 当函数中包含UUID()时; 包含AUTO_INCREMENT字段的表被更新等. 这些情况下会按照row的方式处理. 优势: 节省空间, 同事兼顾了一定的一致性 劣势: 还有一些极个别情况依旧会造成不一致, 另外 statement 和 mixed 对于需要对 binlog 的监控情况都不方便 综上, Canl想做监控分析, 选择 row 格式比较合适 ","date":"2022-07-26","objectID":"/canal/:1:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的工作原理 MySQL主从复制过程 Master 主库将改变记录, 写到 binlog 中; Slave 从库向 MySQL Slave 发送 dump 协议, 将 Master 主库的 binlog events 拷贝到它的中继日志(relay log); Slave 从库读取并重做中继日志中的事件, 将改变的数据同步到自己的数据库. MySQL主从复制过程 Canal的工作原理 理解了上面的过程, Canal的原理就很简单, 就是把自己伪装成Slave, 假装从 Master 复制数据 ","date":"2022-07-26","objectID":"/canal/:1:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"MySQL的准备 MySQL的安装这里就不说了, 网上有很多. 首先, 建数据库: canal_test 然后建表: create table user_info { `id` int, `name` varchar(255), `gender` varchar(255) } 修改配置文件开启binlog [root@hadoop102 ~]# vi /etc/my.cnf # 打开binlog log-bin=mysql-bin # 选择ROW模式 binlog-format=row # 配置MYSQL replaction需要定义, 不要和canal的slaveId重复 server_id=1 binlog-do-db=canal_test binlog-do-db根据实际情况配置, 如果不配置, 则表示所有数据库均开启 binlog. 重启 MySQL sudo systemctl restart mysqld ","date":"2022-07-26","objectID":"/canal/:2:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Canal的下载和安装 ","date":"2022-07-26","objectID":"/canal/:3:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"下载 下载地址: https://github.com/alibaba/canal/releases 下载对应版本: canal.deployer-xxx.tar.gz 解压到对应位置 ","date":"2022-07-26","objectID":"/canal/:3:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 canal.properties [root@hadoop102 conf]# pwd /opt/module/canal/conf [root@hadoop102 conf]# vi canal.properties 这个文件是 canal 的基本通用配置, canal端口号默认是11111. 多实例配置: 一个 canal 服务中可以有多个instance, conf/ 下每一个 example 即是一个实例, 每个实例下面都有独立的配置文件. 需修改canal.destinations=实例1,实例2,实例3 ","date":"2022-07-26","objectID":"/canal/:3:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"修改 instance.properties [root@hadoop102 example]# pwd /opt/module/canal/conf/example [root@hadoop102 example]# vi instance.properties ## mysql serverId , v1.0.26+ will autoGen ## v1.0.26版本后会自动生成slaveId，所以可以不用配置 # canal.instance.mysql.slaveId=0 # 数据库地址 canal.instance.master.address=127.0.0.1:3306 # binlog日志名称 canal.instance.master.journal.name=mysql-bin.000001 # mysql主库链接时起始的binlog偏移量 canal.instance.master.position=154 # mysql主库链接时起始的binlog的时间戳 canal.instance.master.timestamp= canal.instance.master.gtid= # username/password # 在MySQL服务器授权的账号密码 canal.instance.dbUsername=canal canal.instance.dbPassword=canal # 字符集 canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false # table regex .*\\\\..*表示监听所有表 也可以写具体的表名，用，隔开 canal.instance.filter.regex=.*\\\\..* # mysql 数据解析表的黑名单，多个表用，隔开 canal.instance.filter.black.regex= # 解析表字段的黑名单, 多个字段用/隔开, 多个表用,隔开(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) canal.instance.filter.black.field=data_center.canal_test_2:column_2,data_center.canal_test_1:column_1,data_center.canal_test_3:column_2/column_3 ","date":"2022-07-26","objectID":"/canal/:3:3","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"实时监控测试 ","date":"2022-07-26","objectID":"/canal/:4:0","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"TCP模式测试 创建一个 maven 项目, 在pom.xml中配置: \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba.otter\u003c/groupId\u003e \u003cartifactId\u003ecanal.client\u003c/artifactId\u003e \u003cversion\u003e1.1.2\u003c/version\u003e \u003c/dependency\u003e 创建类: CanalClient package org.mustard.app; import com.alibaba.fastjson.JSONObject; import com.alibaba.otter.canal.client.CanalConnector; import com.alibaba.otter.canal.client.CanalConnectors; import com.alibaba.otter.canal.protocol.CanalEntry; import com.alibaba.otter.canal.protocol.Message; import com.google.protobuf.ByteString; import com.google.protobuf.InvalidProtocolBufferException; import java.net.InetSocketAddress; import java.util.List; public class CanalClient { public static void main(String[] args) throws InvalidProtocolBufferException { // 获取连接对象 CanalConnector canalConnector = CanalConnectors.newSingleConnector(new InetSocketAddress(\"hadoop102\", 11111), \"example\", \"\", \"\"); // 获取连接 canalConnector.connect(); // 指定要监控的数据库 canalConnector.subscribe(\"canal.*\"); long idx = 0; while (true) { // 获取message Message msg = canalConnector.get(100); List\u003cCanalEntry.Entry\u003e entries = msg.getEntries(); if (entries.size() \u003c= 0) { System.out.println((++idx) + \". 没有数据, 等一会儿\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } else { for (CanalEntry.Entry entry : entries) { // 获取表名 String tableName = entry.getHeader().getTableName(); // Entry类型 CanalEntry.EntryType entryType = entry.getEntryType(); if (CanalEntry.EntryType.ROWDATA.equals(entryType)) { // 序列化数据 ByteString storeValue = entry.getStoreValue(); // 反序列化 CanalEntry.RowChange rowChange = CanalEntry.RowChange.parseFrom(storeValue); // 获取事件类型 CanalEntry.EventType eventType = rowChange.getEventType(); // 获取具体数据 List\u003cCanalEntry.RowData\u003e rowDatasList = rowChange.getRowDatasList(); // 遍历打印 for (CanalEntry.RowData rowData : rowDatasList) { List\u003cCanalEntry.Column\u003e beforeColumnsList = rowData.getBeforeColumnsList(); JSONObject beforeData = new JSONObject(); for (CanalEntry.Column column : beforeColumnsList) { beforeData.put(column.getName(), column.getValue()); } List\u003cCanalEntry.Column\u003e afterColumnsList = rowData.getAfterColumnsList(); JSONObject afterData = new JSONObject(); for (CanalEntry.Column column : afterColumnsList) { afterData.put(column.getName(), column.getValue()); } System.out.println(\"TableName: \" + tableName + \", EventType: \" + eventType + \", Before: \" + beforeData + \", After: \" + afterData); } } } } } } } ","date":"2022-07-26","objectID":"/canal/:4:1","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["canal"],"content":"Kafka模式测试 修改 canal.properties 中 canal 的输出 model ######### common argument ############# canal.id = 1 canal.ip = canal.port = 11111 canal.metrics.pull.po rt = 11112 canal.zkServers = # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = false # tcp, kafka, RocketMQ canal.serverMode = kafka # flush meta cursor/parse position to file 修改 kafka 集群的地址 ######### Kafka ############# kafka.bootstrap.servers = hadoop102:9092 修改 instance.properties 输出到 Kafka 的主题以及分区数 # mq config canal.mq.topic=canal_test 默认还是输出到指定 Kafka 主题的一个分区, 因为多个分区并行可能会打乱 binlog 的顺序, 如果要提高并行度, 首先设置 kafka 的分区数 \u003e 1, 然后设置canal.mq.partitionHash属性. 启动 canal $ ./bin/startup.sh 然后测试: [root@hadoop102 kafka_2.12-2.8.1]# ./bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic canal_test {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589672000,\"id\":2,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589672852,\"type\":\"DELETE\"} {\"data\":[{\"id\":\"5\",\"name\":\"aria\",\"gender\":\"female\"}],\"database\":\"canal_test\",\"es\":1658589697000,\"id\":3,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"gender\":\"varchar(255)\"},\"old\":null,\"pkNames\":null,\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"gender\":12},\"table\":\"user_info\",\"ts\":1658589697570,\"type\":\"INSERT\"} ","date":"2022-07-26","objectID":"/canal/:4:2","tags":["canal","big data"],"title":"Canal","uri":"/canal/"},{"categories":["scala"],"content":"分支控制 ","date":"2022-07-15","objectID":"/scala_4/:1:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"if-else 单分支: if{} 双分支: if{} else{} 多分支: if{} else if{} Scala 中的if-else是有返回值的, 具体取决于满足条件的代码块的最后一行内容 Scala 中是没有三元运算符的, 但是可以用if-else代替 ","date":"2022-07-15","objectID":"/scala_4/:1:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"switch Scala 中是没有switch的, 而是使用模式匹配来处理的 ","date":"2022-07-15","objectID":"/scala_4/:1:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"循环控制 ","date":"2022-07-15","objectID":"/scala_4/:2:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"for Scala 中为for提供了很多的特性: 1. 范围数据循环 (to) // 前后闭合: [1,3] for (i \u003c- 1 to 3) { println(i) } 2. 范围数据循环 (until) // 前闭后开: [1, 3) for (i \u003c- 1 until 3) { println(i) } 3. 循环守卫 for (i \u003c- 1 to 3 if i != 2) { println(i) } // 上面的代码就相当于: for (i \u003c- 1 to 3) { if (i != 2) { println(i) } } 4. 循环步长 for (i \u003c- 1 to 10 by 2) { println(i) } 5. 嵌套循环 // 因为没有关键字, 所以一定要加 `;` 来进行分割 for (i \u003c- 1 to 3; j \u003c- 1 to 3) { println(\"i=\" + i + \", j=\" + j) } // 上面的代码相当于: for (i \u003c- 1 to 3) { for (j \u003c- 1 to 3) { println(\"i=\" + i + \", j=\" + j) } } 6. 引入变量 for (i \u003c- 1 to 3; j = 4 - i) { println(\"i=\" + i + \", j=\" + j) } // for 推导式有一个不成文的规定: // 1. 仅包含单一表达式时, 使用圆括号 // 2. 当包含多个表达式时, 一般每一行一个表达式, 并且用花括号 for { i \u003c- 1 to 3 j = 4 - i } { println(\"i=\" + i + \", j=\" + j) } 7. 循环返回值 val res = for (i \u003c- 1 to 5) yield { i * 2 } // 输出 2, 4, 6, 8, 10 println(res) 8. 倒序 for (i \u003c- 1 to 10 reverse) { println(i) } ","date":"2022-07-15","objectID":"/scala_4/:2:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"while和do..while Scala 中的while和do..while和 Java 中的用法一致 while 循环条件是返回一个布尔值的表达式 while先判断再执行 与for不同, while没有返回值, 即整个while语句的结果是Unit类型 do..while 循环条件是返回一个布尔值的表达式 do..while先执行再判断 ","date":"2022-07-15","objectID":"/scala_4/:2:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"循环中断 Scala 内置控制结构特地去掉了break和continue. 是因为更好的适应函数式编程, 推荐使用函数式的风格解决break和continue, 而不是一个关键字. Scala 中使用breakable控制结构来实现break和continue功能. ","date":"2022-07-15","objectID":"/scala_4/:3:0","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"异常的方式退出 def main(args: Array[String]): Unit = { try { for (elem \u003c- 1 to 10) { println(elem) if (elem == 5) throw new RuntimeException } } catch { case e =\u003e } println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:1","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"Scala 自带函数退出 import scala.util.control.Breaks def main(args: Array[String]): Unit = { Breaks.breakable( for (ele \u003c- 1 to 10) { println(ele) if (ele == 5) Breaks.break() } ) println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:2","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"对break进行省略 import scala.util.control.Breaks._ def main(args: Array[String]): Unit = { breakable( for (ele \u003c- 1 to 10) { println(ele) if (ele == 5) break } ) println(\"结束循环\") } ","date":"2022-07-15","objectID":"/scala_4/:3:3","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":"continue import scala.util.control.Breaks._ def main(args: Array[String]): Unit = { for (ele \u003c- 1 to 10) { breakable( if (ele % 2 == 1) break else println(ele) ) } println(\"结束循环\") } 这里的breakable和上面的区别是将其放入到了循环内部, 这样可以实现结束本次执行而不是整个循环结束, 从而实现continue的功能. ","date":"2022-07-15","objectID":"/scala_4/:3:4","tags":["scala"],"title":"Scala控制流程","uri":"/scala_4/"},{"categories":["scala"],"content":" Scala 运算符的使用和 Java 的基本相同, 只有个别细节上不同 ","date":"2022-07-14","objectID":"/scala_3/:0:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"算数运算符 运算符 运算 🌰 结果 + 正号 +3 3 - 负号 b=4;-b -4 + 加 5+5 10 - 减 6-4 2 * 乘 3*4 12 / 除 5/5 1 % 取模(取余) 7%5 2 + 字符串相加 \"Must\"+\"ard\" “Mustard” ","date":"2022-07-14","objectID":"/scala_3/:1:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"关系运算符(比较运算符) 运算符 运算 🌰 结果 == 相等于 4==3 false != 不等于 4!=3 true \u003c 小于 4\u003c3 false \u003e 大于 4\u003e3 true \u003c= 小于等于 4\u003c=3 false \u003e= 大于等于 4\u003e=3 true Java 和 Scala 中关于==的区别 Java: ==比较两个变量本身的值, 即两个对象在内存中的首地址 equals比较字符串中包含的内容是否相同 Scala: ==更类似于 Java 中的equals def main(args: Array[String]): Unit = { val s1 = \"abc\" val s2 = new String(\"abc\") // true println(s1 == s2) // false println(s1.eq(s2)) } ","date":"2022-07-14","objectID":"/scala_3/:2:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"逻辑运算符 运算符 描述 实例 (A=true, B=false) \u0026\u0026 逻辑与 A \u0026\u0026 B = false ` ` ! 逻辑非 !(A \u0026\u0026 B) = true ","date":"2022-07-14","objectID":"/scala_3/:3:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"位运算符 运算符 描述 实例 (a=60, b=13) =\u003e (a: 0011 1100, b: 0000 1101) \u0026 按位与 a \u0026 b = 12 =\u003e 0000 1100 ` ` 按位或 ^ 按位异或 a ^ b = 49 =\u003e 0011 0001 ~ 按位取反 ~a = -61 =\u003e 1100 0011 \u003c\u003c 左移 a \u003c\u003c 2 = 240 =\u003e 0011 0000 \u003e\u003e 右移 a \u003e\u003e 2 = 15 =\u003e 0000 1111 \u003e\u003e\u003e 无符号右移 a \u003e\u003e\u003e 2 = 15 =\u003e 0000 1111 ","date":"2022-07-14","objectID":"/scala_3/:4:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"赋值运算符 Scala中的赋值运算符就是 算数运算符/位运算符+= Scala 中没有++, --操作, 这点Swift和这里是相同的, 都是因为觉得这两个运算符并不符合面向对象的思想. 需要通过+=和-=来实现. ","date":"2022-07-14","objectID":"/scala_3/:5:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["scala"],"content":"Scala 运算符的本质 看源码可以知道, Scala 中其实是没有运算符的, 所有的运算符都是方法 object TestOpt { def main(args: Array[String]): Unit = { // 标准的加法运算 val i: Int = 1.+(1) // 1. 当调用对象的方法时, `.`可以省略 val j: Int = 1 + (1) // 2. 如果函数参数只有一个, 或者没有参数, `()`可以省略 val k: Int = 1 + 1 println(1.toString()) println(1 toString()) println(1 toString) } } ","date":"2022-07-14","objectID":"/scala_3/:6:0","tags":["scala"],"title":"Scala运算符","uri":"/scala_3/"},{"categories":["big data"],"content":" 官方文档: 传送门 本文档针对 SeaTunnel v2.1.2 编写 ","date":"2022-07-12","objectID":"/seatunnel_use/:0:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel是什么 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"概述 先来看一下官方文档上是怎么说的: SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data. It can synchronize tens of billions of data stably and efficiently every day, and has been used in the production of nearly 100 companies. 翻译一下: SeaTunnel 是一个使用起来非常简单, 性能非常高效的分布式数据集成平台. 它支持海量数据的实时同步. 它可以每天稳定高效的同步数百亿的数据, 并且已经用于近百个公司的生产中. 可以从上面提炼出几个关键词: very easy-to-use: 使用非常简单. 其实 SeaTunnel 并不是对 Flink 或是 Spark 或是以后支持的其他技术的二次开发, 而是在其之上封装了一层, 使得这些技术使用起来会更简便. ultra-high-performance: 超高性能. real-time synchronization of massive data: 海量数据的实时同步. 以下是从官方文档摘抄: ","date":"2022-07-12","objectID":"/seatunnel_use/:1:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"为什么我们需要 SeaTunnel SeaTunnel 尽所能为您解决海量数据同步中可能遇到的问题： 数据丢失与重复 任务堆积与延迟 吞吐量低 应用到生产环境周期长 缺少应用运行状态监控 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"SeaTunnel 使用场景 海量数据同步 海量数据集成 海量数据的 ETL 海量数据聚合 多源数据处理 ","date":"2022-07-12","objectID":"/seatunnel_use/:1:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境依赖 组件 版本 备注 Java \u003e=8 Flink 1.13 因为当前版本SeaTunnel还未适配Flink1.14, 所以使用Flink1.13版本进行文档编写 Spark 2.x 如果是集群方式的话, Yarn/Standalone 都是支持的 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"环境配置 config目录下seatunnel-env.sh中可以配置Spark和Flink的环境 # Home directory of spark distribution. SPARK_HOME=${SPARK_HOME:-/opt/spark} # Home directory of flink distribution. FLINK_HOME=${FLINK_HOME:-/opt/flink} :-代表: 若未找到之前的地址, 则用之后的地址 ","date":"2022-07-12","objectID":"/seatunnel_use/:2:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"官方示例 首先来进行一个官方示例简单的运行一下. 在config目录下创建文件example.conf: # 配置 Spark 或 Flink 的参数 env { # You can set flink configuration here execution.parallelism = 1 #execution.checkpoint.interval = 10000 #execution.checkpoint.data-uri = \"hdfs://hadoop102:9092/checkpoint\" } # 在 source 所属的块中配置数据源 # 默认端口: 9999 source { SocketStream{ host = hadoop102 result_table_name = \"fake\" field_name = \"info\" } } # 在 transform 的块中声明转换插件 # 这里需要说明的是: Split是不会立即生效的, 只有当sql插件中的sql语句中调用了split函数才会真正的作用在数据上 transform { Split{ separator = \"#\" fields = [\"name\",\"age\"] } sql { sql = \"select info, split(info) as info_row from fake\" } } # 在 sink 块中声明要输出到哪 sink { ConsoleSink {} } 然后cd到seatunnel目录在shell中执行: ./bin/start-seatunnel-flink.sh --config config/example.conf 用nc -lk 9999模拟一下socket连接 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"sql执行顺序 在source块中, 利用SocketStream插件读取出数据, 命名为fake表, 字段名为info 拿到info字段, 利用Split插件进行切分 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"启动命令 ./bin/start-seatunnel-flink.sh -h Usage: start-seatunnel-flink.sh [options] Options: -t, --check check config (default: false) * -c, --config Config file -h, --help Show the usage message -r, --run-mode job run mode, run or run-application (default: RUN) (values: [RUN, APPLICATION_RUN]) -i, --variable variable substitution, such as -i city=beijing, or -i date=20190318 (default: []) 其中, --config是必填参数 -i 当其中上面的sql改为: sql = \"select * from (select info, split(info) as info_row from fake) where age \u003e \"${age}\"\" 启动命令改为: ./bin/start-seatunnel-flink.sh --config config/example02.conf -i age=18 -r 执行 flink 自带的命令参数, 可以cd到 flink 下面 -\u003e ./bin/flink run -h查看 ","date":"2022-07-12","objectID":"/seatunnel_use/:3:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"应用配置的4个基本组件 一个完整的SeaTunnel配置文件应包含四个配置组件: env{}` `source{}` --\u003e `transform{}` --\u003e `sink{} ","date":"2022-07-12","objectID":"/seatunnel_use/:4:0","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"env块 env块中可以直接写 spark 或 flink 支持的配置项. 比如并行度, 检查点时间, 检查点 hdfs 路径等. 以 Flink 为例, 在 SeaTunnel 源码的ConfigKeyName类中声明了所有可用的key: package org.apache.seatunnel.flink.util; public class ConfigKeyName { private ConfigKeyName() { throw new IllegalStateException(\"Utility class\"); } public static final String TIME_CHARACTERISTIC = \"execution.time-characteristic\"; public static final String BUFFER_TIMEOUT_MILLIS = \"execution.buffer.timeout\"; public static final String PARALLELISM = \"execution.parallelism\"; public static final String MAX_PARALLELISM = \"execution.max-parallelism\"; public static final String CHECKPOINT_INTERVAL = \"execution.checkpoint.interval\"; public static final String CHECKPOINT_MODE = \"execution.checkpoint.mode\"; public static final String CHECKPOINT_TIMEOUT = \"execution.checkpoint.timeout\"; public static final String CHECKPOINT_DATA_URI = \"execution.checkpoint.data-uri\"; public static final String MAX_CONCURRENT_CHECKPOINTS = \"execution.max-concurrent-checkpoints\"; public static final String CHECKPOINT_CLEANUP_MODE = \"execution.checkpoint.cleanup-mode\"; public static final String MIN_PAUSE_BETWEEN_CHECKPOINTS = \"execution.checkpoint.min-pause\"; public static final String FAIL_ON_CHECKPOINTING_ERRORS = \"execution.checkpoint.fail-on-error\"; public static final String RESTART_STRATEGY = \"execution.restart.strategy\"; public static final String RESTART_ATTEMPTS = \"execution.restart.attempts\"; public static final String RESTART_DELAY_BETWEEN_ATTEMPTS = \"execution.restart.delayBetweenAttempts\"; public static final String RESTART_FAILURE_INTERVAL = \"execution.restart.failureInterval\"; public static final String RESTART_FAILURE_RATE = \"execution.restart.failureRate\"; public static final String RESTART_DELAY_INTERVAL = \"execution.restart.delayInterval\"; public static final String MAX_STATE_RETENTION_TIME = \"execution.query.state.max-retention\"; public static final String MIN_STATE_RETENTION_TIME = \"execution.query.state.min-retention\"; public static final String STATE_BACKEND = \"execution.state.backend\"; public static final String PLANNER = \"execution.planner\"; } ","date":"2022-07-12","objectID":"/seatunnel_use/:4:1","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Row 在说明source块, transform块和sink块之前, 需要先了解一下 SeaTunnel 中的核心数据结构: Row Row 是 SeaTunnel 中数据传递的核心数据结构. 对来 Flink 说, source 插件需要给下游的转换插件返回一个 DataStream\u003cRow\u003e, 转换插件接到上游的 DataStream\u003cRow\u003e进行处理后需要再给下游返回一个 DataStream\u003cRow\u003e. 最后 Sink 插件将转换插件处理好的DataStream\u003cRow\u003e输出到外部的数据系统. 因为 DataStream可以很方便地和 Table 进行互转, 所以将 Row 当作核心数据结构可以让转换插件同时具有使用代码 (命令式) 和 sql (声明式) 处理数据的能力. 可以看一下上面示例中, 读取数据的源码: package org.apache.seatunnel.flink.socket.source; import ... @AutoService(BaseFlinkSource.class) public class SocketStream implements FlinkStreamSource { ... @Override public DataStream\u003cRow\u003e getData(FlinkEnvironment env) { final StreamExecutionEnvironment environment = env.getStreamExecutionEnvironment(); return environment.socketTextStream(host, port) .map((MapFunction\u003cString, Row\u003e) value -\u003e { Row row = new Row(1); row.setField(0, value); return row; }).returns(new RowTypeInfo(Types.STRING())); } } 感兴趣的话, 也可以看到源码中的 Split, sql, sink 都是用DataStream\u003cRow\u003e进行数据传递的 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:2","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"source块 source{}是可以配置多个 source 插件的 # 伪代码 env { ... } source { hdfs { ... } jdbc { ... } elasticsearch { ... } } transform { sql { sql = \"\"\" select ... from hdfs_table join es_table on hdfs_table.uid = es_table.uid where ...\"\"\" } } sink { elasticsearch { ... } } 需要注意的是: 所有的 source 插件中都可以声明result_table_name. 如果声明了result_table_name. SeaTunnel 会将 source 插件输出的DataStream\u003cRow\u003e转换为 Table 并注册在 Table 环境中. 当指定了result_table_name那么还可以指定field_name, 在注册时, 给 Table重设字段名. 因为每个 source 所需要的配置是不一致的 (result_table_name和field_name为共有非必填参数), 所以配置的时候查找官方文档会好一点 当前支持的source source 支持Spark 支持Flink 备注 文档地址 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ❌ 传送门 Fake ✔️ ✔️ 改类型主要是用于方便生成指定的数据, 用作 SeaTunnel 的功能验证, 测试和性能测试. 传送门 Feishu Sheet ✔️ ❌ 传送门 File ✔️ ✔️ 从本地或者 hdfs 中读取. 传送门 HBase ✔️ ❌ 传送门 Hive ✔️ ❌ 传送门 Http ✔️ ✔️ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDb ❌ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ Kafka 版本 \u003e= 0.10.0, 目前源码中发现 Schema 的解析有问题(原因为社区把 fastjson 换成 Jackon 引起的). 传送门 Kudu ✔️ ❌ 兼容 Kerberos 认证 传送门 MongoDb ✔️ ❌ 传送门 Neo4j ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 Socket ✔️ ✔️ 传送门 Tidb ✔️ ❌ 传送门 WebhookStream ✔️ ❌ 提供 http 接口推送数据, 仅支持 POST 请求 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:3","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"transform块 目前社区对 transform 插件做了很多规划, 但截至 v2.1.2 版本, 可用的插件有3个: Split, Sql和Json. 其中Json只适配 Spark 可用. transform{}中可以声明多个转换插件. 所有的转换插件都可以使用source_table_name, 和result_table_name. 同样, 如果声明了result_table_name, 那么就能声明field_name. Split插件 这里着重说一下 Split 插件: @Override public DataSet\u003cRow\u003e processBatch(FlinkEnvironment env, DataSet\u003cRow\u003e data) { return data; } @Override public DataStream\u003cRow\u003e processStream(FlinkEnvironment env, DataStream\u003cRow\u003e dataStream) { return dataStream; } @Override public void registerFunction(FlinkEnvironment flinkEnvironment) { if (flinkEnvironment.isStreaming()) { flinkEnvironment .getStreamTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } else { flinkEnvironment .getBatchTableEnvironment() .registerFunction(\"split\", new ScalarSplit(rowTypeInfo, num, separator)); } } 从源码中可以发现 Split 插件并没有对数据流进行任何的处理, 而是将它直接return了. 反之, 它向表环境中注册了一个名为 split 的 UDF(用户自定义函数). 而且, 函数名是写死的. 这意味着, 如果声明了多个 Split 后面的 UDF 还会把前面的覆盖. 从 Split 插件中就能看出了, 这个插件其实是通过注册方法的方式来调用的. 但是, transform 接口其实是预留了直接操作数据的能力的 (比如 Sql 插件中的处理方式), 也就是processStream方法. 那么, 一个 transform 插件其实同时履行了 process 和 UDF 的职责, 这是违背单一职责原则的. 所以要判断一个 transform 插件在做什么就只能从源码和文档的方面来加以区分了. Sql 插件 sql插件中需要特别说明的是, 指定source_table_name对于 sql 插件的意义不大, 因为可以通过from子句来决定从哪个表里抽取数据. ","date":"2022-07-12","objectID":"/seatunnel_use/:4:4","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["big data"],"content":"Sink块 sink块里可以声明多个 sink 插件, 每个 sink 插件都可以指定source_table_name. 当前支持的sink source 支持Spark 支持Flink 备注 文档地址 Clickhouse ✔️ ✔️ 使用 Clickhouse-jdbc 根据字段名称对应数据源, 并将其写入. 使用前需创建对应的数据表. 传送门 ClickhouseFile ✔️ ✔️ 通过 clickhouse-local 程序生成 Clickhouse 数据文件, 然后将其发送到 Clickhouse 服务器, 也称为是 bulk load (批量加载). 传送门 Console ✔️ ✔️ 将数据输出到标准终端或 Flink 的 TaskManager. 通常用于调试和易于观察的数据. 传送门 Doris ✔️ ✔️ 传送门 Druid ❌ ✔️ 传送门 Elasticsearch ✔️ ✔️ Spark 支持的 Elasticsearch \u003e= 2.0 并且 \u003c 7.0.0; Flink 支持的 Elasticsearch = 7.x, 如果要用 Elasticsearch 6.x, 需用源码执行命令 mvn clean package -Delasticsearch=6重新打包. 传送门 Email ✔️ ❌ 支持通过 email 附件输出数据. 传送门 File ✔️ ✔️ 传送门 Hbase ✔️ ✔️ 使用 hbase-connectors 将数据输出到 Hbase(\u003e=2.1.0) 和 Spark(\u003e=2.0.0) 版本兼容性取决于 hbase-connectors. 传送门 Hive ✔️ ❌ 传送门 Hudi ✔️ ❌ 传送门 Iceberg ✔️ ❌ 传送门 InfluxDB ❌️ ✔️ 传送门 Jdbc ✔️ ✔️ 传送门 Kafka ✔️ ✔️ 传送门 Kudu ✔️ ❌ 传送门 MongoDB ✔️ ❌ 传送门 Phoenix ✔️ ❌ 传送门 Redis ✔️ ❌ 传送门 TiDb ✔️ ❌️ 传送门 ","date":"2022-07-12","objectID":"/seatunnel_use/:4:5","tags":["SeaTunnel","big data"],"title":"Seatunnel使用手册","uri":"/seatunnel_use/"},{"categories":["Scala"],"content":"注释 Scala 的注释和 Java 的完全一样: // 1. 单行注释 /* 2. 多行注释 */ /** * 3. 文档注释 */ ","date":"2022-07-07","objectID":"/scala_2/:1:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"变量和常量 ","date":"2022-07-07","objectID":"/scala_2/:2:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 // var 变量名 [: 变量类型] = 初始值 var i: Int = 10 // val 常量名 [: 常量类型] = 初始值 val j: Int = 20 能用常量的地方就不要用变量 声明变(常)量时, 类型可以省略, 编译器自动推导, 即类型推导 类型确定后, 就不能更改, 因为 Scala 是强数据类型语音 变量声明时, 必须有初始值 ","date":"2022-07-07","objectID":"/scala_2/:2:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"标识符的命名规范 ","date":"2022-07-07","objectID":"/scala_2/:3:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"命名规范 Scala 中的标识符声明, 基本和Java是一致的, 但是细节上会有变化: 以字母或下划线开头, 后接字母、数字、下划线 以操作符开头, 且只包含操作符(+-*/#!等) 用反引号`…`包括的任意字符串, 即使是 Scala 关键字(39个)也可以 package, import, class, object, trait, extends, with, type, forSom private, protected, abstract, sealed, final, implicit, lazy, override try, catch, finally, throw if, else, match, case, do, while, for, return, yield def, var, val this, super new true, false, null 看几个特殊一点的🌰 object Hello { def main(args: Array[String]): Unit = { // ok 因为在 Scala 中 Int 是预定义的字符, 不是关键字, 但是不推荐 var Int: String = \"\" // ok 单独一个下划线不可作为标识符, 因为 _ 被认为是一个方法 var _: String = \"str\" // 会报错 println(_) // ok var -+*/#! : String = \"\" // error 以操作符开否, 必都是操作符 var -_*/#!1 : String = \"\" // error var if: String = \"\" // ok var `if`: String = \"\" } } ","date":"2022-07-07","objectID":"/scala_2/:3:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"字符串输出 ","date":"2022-07-07","objectID":"/scala_2/:4:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"基本语法 字符串, 通过+号连接 printf用法: 字符串，通过%传值。 字符串模板(插值字符串): 通过$获取变量值 来看看🌰: object Hello { def main(args: Array[String]): Unit = { val name = \"mustard\" val age = 18 printf(\"name=%s, age=%d\", name, age) /** * 多行字符串， 在 Scala 中，利用三个双引号包围多行字符串就可以实现。 * 输入的内容，带有空格、 \\t 之类，导致每一行的开始位置不能整洁对齐。 * 应用 scala 的 stripMargin 方法，在 scala 中 stripMargin 默认是 \"|\" 作为连接符， * 在多行换行的行头前面加一个 \"|\" 符号即可。 */ val sql1 = \"\"\" |select | name | age |from user |where name = \"mustard\" \"\"\".stripMargin println(sql1) // 如果需要对变量进行运算，那么可以加 ${} val sql2 = s\"\"\" |select | name | age |from user |where name = \"$name\" and age = ${age + 2} \"\"\".stripMargin println(sql2) val s = s\"name=$name\" println(s) } } ","date":"2022-07-07","objectID":"/scala_2/:4:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"数据类型 ","date":"2022-07-07","objectID":"/scala_2/:5:0","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Java数据类型 先来回顾一下 Java 的数据类型: 基本数据类型(8种): char, byte, short, int, long, float, double, bollean 引用类型: (对象类型) 由于 Java 有基本类型, 并且基本类型并不是真正意义的对象, 所以 Java 语言并不是真正意义的面向对象. 注意哈: Java 中基本类型和引用类型没有共同的祖先 ","date":"2022-07-07","objectID":"/scala_2/:5:1","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["Scala"],"content":"Scala数据类型 Scala 中一切都是对象, 都是Any的子类 Scala 中数据类型分为两大类: 数值类型 (AnyVal), 引用类型 (AnyRef), 但是都是对象 Scala 数据类型仍然遵守隐式转换 (低精度的值向高精度的值自动转换) Scala 中的StringOps是对 Java 中的String增强 Unit: 对应 Java 中的void, 用于方法返回值的位置, 表示方法没有返回值. Unit是一个数据类型, 只有一个对象就是(). Void不是数据类型, 只是一个关键字. Null是一个类型, 只有一个对象就是null. 它是所有引用类型(AnyRef)的子类. Nothing: 是所有数据类型的子类, 主要用在一个函数没有明确返回值时使用, 因为这样可以把抛出的返回值, 返回给任何的变量或者函数. 整数类型 数据类型 描述 Byte[1] 8位有符号补码整数. 数值区间: -128 到 127 Short[2] 16位有符号补码整数. 数值区间: -32768 到 32767 Int[4] 32位有符号补码整数. 数值区间: -2147483648 到 2147483647 Long[8] 64位有符号补码整数. 数值区间: -2^64 到 2^64-1 Scala 的整型，默认为Int型，声明Long型，须后加l或L 浮点类型 数据类型 描述 FLoat[4] 32位 Double[8] 64位 object TestDataType { def main(args: Array[String]): Unit = { // 这是个 Float var n1 = 1.23456789f // 这是个 Double var n2 = 1.23456789 } } 字符类型 (Char) 布尔类型 (Boolean) 占 1 个字节 Unit 类型、Null 类型和 Nothing 类型 数据类型 描述 Unit 表示无值, 和其他语言中void等同. 用作不返回任何结果的方法的结果类型. Unit只有一个实例: (), 且没有实际意义 Null null, Null类型只有一个实例: null. Null可以赋值给任意引用类型(AnyRef), 但是不能赋值给值类型(AnyVal) Nothing Nothing类型在 Scala 的类层级最低端; 它是任何其他类型的子类型. 当一个函数, 确定没有正常的返回值时, 可以用Nothing来指定返回类型. Nothing的这种机制有一个好处: 可以把返回的值(异常)赋给其它的函数或者变量(兼容性) object TestDataType { def main(args: Array[String]): Unit = { var cat = new Cat() // 正确 cat = null // 错误 var n1: Int = null def test(): Nothing = { throw new Exception() } test } } 类型转换 数值类型自动转换: 精度小的类型自动转换为精度大的数值类型. (隐式转换) Byte \u003c Short \u003c Int \u003c Long \u003c Float \u003c Double 1. 自动提升原则: 有多种类型的数据混合运算时, 首先自动将所有数据转换成精度大的数据类型, 然后再计算 2. 把精度大的数值类型赋值给精度小的数值类型时, 会报错 3. (byte, short)和char之间不会相互自动转换 4. byte,short,char三者可以计算, 但会先转换为int类型 强制类型转换 可能造成精度降低或溢出, 所以需要特别注意 var num: Int = 2.7.toInt 数值类型和String类型间的转换 数值 -\u003e String: + \"\"就行, 和 Java 一样 String -\u003e 数值: s1.toInt, s1.toFloat, s1.toDouble, s1.toByte, s1.toLong, s1.toShort ","date":"2022-07-07","objectID":"/scala_2/:5:2","tags":["Scala"],"title":"Scala变量和数据类型","uri":"/scala_2/"},{"categories":["scala"],"content":" Scala这门语言是怎么发展过来的, 网上有很多资料, 这里就不赘述了. ","date":"2022-07-06","objectID":"/scala_1/:0:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"Scala和Java的关系 一般来说, 学习 Scala 之前都会或多或少的接触过 Java, 而 Scala 是基于 Java 的, 因此在学习 Scala 之前, 要先弄清楚 Java, Scala 和 JVM 的关系是很有用的. Java, Scala和JVM的关系 ","date":"2022-07-06","objectID":"/scala_1/:1:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置Scala环境 以 Windows 为例 首先确保 JDK1.8 安装成功 下载对应的 Scala 安装文件: 传送门 (我下载的是zip) 解压 zip 配置 Scala 的环境变量 配置 SCALA_HOME 在 Path 中添加 ","date":"2022-07-06","objectID":"/scala_1/:2:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["scala"],"content":"配置IDEA 安装插件 Scala 创建项目之后, 右键项目目录 -\u003e Add Framework Support -\u003e 选择 Scala ","date":"2022-07-06","objectID":"/scala_1/:3:0","tags":["scala"],"title":"Scala概述","uri":"/scala_1/"},{"categories":["big data"],"content":"数据仓库与数据湖的区别 说到湖仓一体, 就要先了解一下数据仓库和数据湖的区别是什么. 下面这个表格就是AWS(传送门)上的对比: 特性 数据仓库 数据湖 数据 来自事务系统、运营数据和业务线应用程序的关系数据 来自IoT设备、网站、移动应用程序、社交媒体和企业应用程序的非关系和关系数据 Schema 设计数据仓库实施之前(写入型Schema) 写入在分析时(读取型Schema) 性价比 更快查询结果会带来较高存储成本 更快查询结果只需较低存储成本 数据质量 可作为重要事实依据的高度监管数据 任何可以或无法进行监管的数据(例如原始数据) 用户 业务分析师 数据科学家、数据开发人员和业务分析师(使用监管数据) 分析 批处理报告、BI和可视化 机器学习、预测分析、数据发现和分析 从上面这个表就能看出来数据仓库和数据湖的差别还是很明显的. 在企业中, 两者的作用是互补的, 所以不应该认为数据湖的出现是为了取代数据仓库, 毕竟两者的作用是截然不同的. ","date":"2022-06-16","objectID":"/data_lakehouse/:1:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体是怎么诞生的 对于数据而言, 数据仓库就像是一个大型图书馆, 里面的数据需要按照规范放好, 可以按照类别找到想要的信息. 而数据湖就像是一个大型仓库, 可以存储任何形式(结构化, 半结构化和非结构化)和任何格式(文本, 图像, 音频, 视频)的原始数据. 在产品的角度上来说, 数据仓库一般是独立标准化产品, 数据湖更像是一种架构指导, 需要配合着系列周边工具来实现业务需要. 也就是说, 数据湖的灵活性对于前期开发和前期部署是友好的; 数据仓库的规范性对于大数据后期的运行和长期发展是友好的. 那有没有一种新架构能兼具数据仓库和数据湖的优点? 然后, 湖仓一体就诞生了. 依据DataBricks公司对Lakehouse 的定义, 湖仓一体是一种结合了数据湖和数据仓库优势的新范式, 在用于数据湖的低成本存储上, 实现与数据仓库中类似的数据结构和数据管理功能. 湖仓一体是一种更开放的新型架构, 有人把它做了一个比喻, 就类似于在湖边搭建了很多小房子, 有的负责数据分析, 有的运转机器学习, 有的来检索音视频等, 至于那些数据源流, 都可以从数据湖里轻松获取. 需要注意的是: 数据湖 + 数据仓库 ≠ 湖仓一体 湖仓一体诞生的目的, 总结起来就是: 打通数据的存储与计算 灵活性与成长性兼得 灵活性与成长性 ","date":"2022-06-16","objectID":"/data_lakehouse/:2:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"湖仓一体的好处 数据重复性：如果一个组织同时维护了一个数据湖和多个数据仓库，这无疑会带来数据冗余。在最好的情况下，这仅仅只会带来数据处理的不高效，但是在最差的情况下，它会导致数据不一致的情况出现。湖仓一体的结合，能够去除数据的重复性，真正做到了唯一。 高存储成本：数据仓库和数据湖都是为了降低数据存储的成本。数据仓库往往是通过降低冗余，以及整合异构的数据源来做到降低成本。而数据湖则往往使用大数据文件系统和Spark在廉价的硬件上存储计算数据。湖仓一体架构的目标就是结合这些技术来最大力度降低成本。 报表和分析应用之间的差异：数据科学倾向于与数据湖打交道，使用各种分析技术来处理未经加工的数据。而报表分析师们则倾向于使用整合后的数据，比如数据仓库或是数据集市。而在一个组织内，往往这两个团队之间没有太多的交集，但实际上他们之间的工作又有一定的重复和矛盾。而当使用湖仓一体架构后，两个团队可以在同一数据架构上进行工作，避免不必要的重复。 数据停滞：在数据湖中，数据停滞是一个最为严重的问题，如果数据一直无人治理，那将很快变为数据沼泽。我们往往轻易的将数据丢入湖中，但缺乏有效的治理，长此以往，数据的时效性变得越来越难追溯。湖仓一体的引入，对于海量数据进行治理，能够更有效地帮助提升分析数据的时效性。 潜在不兼容性带来的风险：数据分析仍是一门兴起的技术，新的工具和技术每年仍在不停地出现中。一些技术可能只和数据湖兼容，而另一些则又可能只和数据仓库兼容。湖仓一体的架构意味着为两方面做准备。 ","date":"2022-06-16","objectID":"/data_lakehouse/:3:0","tags":["big data","data lakehouse"],"title":"湖仓一体","uri":"/data_lakehouse/"},{"categories":["big data"],"content":"什么是数据湖 看了网上很多的资料, 关于数据湖的定义有很多, 我们先来看看AWS(传送门)的定义: 数据湖是一个集中式存储库，允许您以任意规模存储所有结构化和非结构化数据。您可以按原样存储数据（无需先对数据进行结构化处理），并运行不同类型的分析 – 从控制面板和可视化到大数据处理、实时分析和机器学习，以指导做出更好的决策。 再来看一下Wikipedia(传送门)的定义: 指使用大型二进制对象或文件这样的自然格式储存数据的系统 。它通常把所有的企业数据统一存储，既包括源系统中的原始副本，也包括转换后的数据，比如那些用于报表, 可视化, 数据分析和机器学习的数据。数据湖可以包括关系数据库的结构化数据(行与列)、半结构化的数据(CSV，日志，XML, JSON)，非结构化数据 (电子邮件、文件、PDF)和 二进制数据(图像、音频、视频)。 比较统一的一点是数据湖存储的是未经加工的原始数据, 包含结构化(如关系型数据库中的表)、半结构化(如CSV、日志、XML、JSON)和非结构化(如电子邮件、文档、PDF)的各类数据. 因为是原始数据, 所以也就保持着数据在业务系统中原来的样子. 这就是使得数据湖一定要具备完善的管理能力, 也就是要有完善的元数据, 可以管理各类数据相关的要素，包括数据源、数据格式、连接信息、数据schema、权限管理等. 因为数据湖是为了分析数据而演化来的, 也就要存储各类分析处理的中间结果, 并且还要记录完整的分析过程. 数据沼泽是一个劣化的数据湖, 用户无法访问, 或是没什么价值 ","date":"2022-06-15","objectID":"/date_lake/:1:0","tags":["big data","data lake"],"title":"数据湖","uri":"/date_lake/"},{"categories":["big data"],"content":"数据仓库的基本概念 ","date":"2022-06-14","objectID":"/data_warehouse/:1:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"概念 英文名为Data Warehouse, 简写为DW或DWH. 出现的目的是构建面向分析的集成化数据环境, 为企业提供决策支持(Decision Support). 因为分析性报告和决策支持目的而出现的技术. 之所以叫做\"仓库“而不是”工厂“就是因为DW本身是不生产或消费任何数据, 数据来源于外部, 并且开放给外部应用. ","date":"2022-06-14","objectID":"/data_warehouse/:1:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"基本特征 DW是面向主题的, 集成的, 非易失的和时变的数据集合, 用以支持决策. 面向主题(Subject Oriented) 在传统数据库中, 最大的特点就是面向应用进行数据的组织, 各个系统可能是相互分离的, 而数据仓库则是面向主题的. 抽象上来说, 主题就是在较高层次上讲企业信息系统中数据进行综合、归类分析利用. 每一个主题基本对应一个宏观分析领域. 逻辑上来说, 主题是对应企业中某一个宏观分析领域所涉及的分析对象. 集成的(Integrate) 通过对分散、独立、异构的数据库数据进行抽取、清理、转换和汇总便得到了数据仓库的数据， 这样能保证整个企业的数据的一致性, 避免了产生了信息孤岛. 数据仓库中的综合数据不能从原有的数据库系统直接得到. 所以在数据进入到DW之前, 必然要经过统一与综合(抽取和清洗), 这就是DW建设中, 最关键、最复杂的一步. 非易失性(Non-Volatile) 非易失性也可称为稳定性或不可更新性. 数据仓库的数据反映的是相当一段时间内的历史数据的内容, 是不同时点的数据库的快照的集合, 以及基于这些快照进行统计、综合和重组的导出数据. 基于这个特点, DW一般有大量的查询操作, 但修改和删除操作很少. 通常只需要定期的加载和更新. 时变(Time Variant) 数据仓库包含各种粒度的历史数据. 虽然说DW的用户不能修改数据, 但并不是说数据仓库的数据就是永远不变的. 分析的结果只能反映过去的情况, 当业务变化后, 挖掘出的模式会失去时效性. 所以说, DW中的数据需要更新, 以适应决策的需要. DW的数据时限一般要远远长于操作型数据的数据时限. 操作型系统存储的是当前的数据, 而数据仓库中的数据是历史数据. 数据仓库中的数据是按照时间顺序追加的, 它们都带有时间属性. ","date":"2022-06-14","objectID":"/data_warehouse/:1:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库与数据库的区别 这两者的区别其实就是OLAP与OLTP的区别. 联机事务处理OLTP (On-Line Transaction Processing), 也可以叫做面向交易的处理系统. 是针对具体业务在数据库联机的日常操作, 通常对少数记录进行查询、修改. 用户较为关心的是响应时间、数据的安全性、完整性和并发支持的用户数等问题. 例如MySQL, Oracle等关系型数据库一般属于OLTP 联机分析处理OLAP(On-Line Analytical Processing)一般针对某些主题的历史数据进行分析, 支持管理决策. 通过这两个的对比就能发现, 数据仓库的出现并不是为了替代数据库的. 数据库设计是尽量避免冗余, 一般是针对某一业务进行设计的. 而数据仓库在设计时有意引入冗余, 依照分析需求、分析维度、分析指标进行设计. 总的来说数据库是为了捕获数据而设计, 数据存库是为了分析数据而设计. ","date":"2022-06-14","objectID":"/data_warehouse/:2:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层架构 按照数据流入流出的过程, DW架构可分为: 数据运营层、数据仓库层、数据服务层. ","date":"2022-06-14","objectID":"/data_warehouse/:3:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据运营层 (ODS) Operation Data Store 数据准备区, 也叫做贴源层, 源数据. 数据仓库源头系统的数据表通常会原封不动的存储一份, 是后续数据仓库加工数据的来源. 来源方式: 业务库 经常会使用SQOOP来进行抽取, 🌰: 每天定时抽取一次 实时方面, 可以考虑用canal/FlinkCDC监听MySQL的binlog 埋点日志 日志一般是以文件的形式保存, 可以用flume定时同步 可以用spark streaming或Flink实时接入 kafka也可以 消息队列 来自ActiveMQ、Kafka的数据. ","date":"2022-06-14","objectID":"/data_warehouse/:3:1","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库层 (DW) DW数据分层, 由下到上为DWD, DWB, DWS. DWD:data warehouse details 细节数据层, 是业务层与数据仓库的隔离层. 主要对ODS数据层做一些数据清洗(去除空值、脏数据、超过极限范围的)和规范化的操作. DWB: data warehouse base 数据基础层, 存储的是客观数据, 一般用作中间层, 可以认为是大量指标的数据层. DWS: data warehouse service 数据服务层, 基于DWB上的基础数据, 整合汇总成分析某一个主题域的服务数据层, 一般是宽表(字段多的表). 用于提供后续的业务查询, OLAP, 数据分发等. 用户行为, 轻度聚合 主要对ODS/DWD层数据做一些轻度的汇总 ","date":"2022-06-14","objectID":"/data_warehouse/:3:2","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据服务层/应用层 (ADS) application data service 应用数据服务, 该层主要是提供数据产品和数据分析使用的数据, 一般会存储在ES、mysql等系统中供线上系统使用 一般会将大宽表, 比如报表数据放在这里 ","date":"2022-06-14","objectID":"/data_warehouse/:3:3","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"ETL 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL, 换句话说就是描述数据从源端经过**抽取(Extra)、转换(Transfer)、加载(Load)**到目的端的过程. ETL就是数据仓库的流水线, 而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定. ","date":"2022-06-14","objectID":"/data_warehouse/:3:4","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库分层的目的 用空间换时间: 通过大量的预处理来提升应用系统的用户体验(效率), 因此数据仓库会存在大量冗余的数据. 如果不分层的话, 如果源业务系统的业务规则发生变化将会影响整个数据清洗过程, 工作量巨大. 简化数据清洗的过程: 把原来一步的工作分成多个步骤来完成. 每一层的处理逻辑都相对简单和容易理解了, 这样比较容易保证每个步骤的正确性. ","date":"2022-06-14","objectID":"/data_warehouse/:3:5","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["big data"],"content":"数据仓库元数据的管理 元数据(Meta data), 一句话就能总结: 关于数据的数据. 数据仓库中的元数据**主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态. 并且是整个数据仓库系统运行的基础, 元数据把数据仓库系统中各个松散的组件联系起来, 组成一个有机的整体. ** ","date":"2022-06-14","objectID":"/data_warehouse/:4:0","tags":["big data","data warehouse"],"title":"数据仓库","uri":"/data_warehouse/"},{"categories":["Operating System"],"content":"地址空间 之前已经说过了, 地址空间分为两种: 物理地址空间: 硬件支持的地址空间 这部分地址空间的管理和控制是由硬件来完成的. 逻辑地址空间: 一个运行的程序所拥有的内存范围 相对于物理地址空间而言, 程序所能\"看到的\"逻辑地址空间更简单一点, 就是一个一维的线性的地址空间. 但是最终, 逻辑地址和物理地址都是需要对应上的, 这部分就是由OS来完成的. ","date":"2022-03-17","objectID":"/os_address/:1:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"地址生成 用一个C语言的函数来举个🌰 逻辑地址生成 从最开始的**‘符号逻辑地址’到最终的‘具体逻辑地址’**, 经过了上面的这些转换过程, 而这些过程是基本上不需要操作系统来帮助的, 而是通过应用程序、编译器或者Loader就可以完成. 但是当把这个地址放入到内存中之后也是逻辑地址而不是物理地址. 再把之前的操作系统架构中的图片拿出来: 物理地址生成 CPU方面: 运算器需要在逻辑地址的内存内容 内存管理单元(MMU)寻找在逻辑地址和物理地址之间的映射 控制器从总线发送在物理地址的内存内容的请求 内存方面: 内存发送物理地址内存的内容给CPU 操作系统方面: 建立逻辑地址和物理地址之间的映射 **操作系统的一个重要的作用就是: 确保放在内存中的程序相互之间不能相互干扰, 每个程序去访问地址空间是合法的. 换而言之, 确保每个程序访问地址空间是在一个范围之内的. ** ","date":"2022-03-17","objectID":"/os_address/:2:0","tags":["Operating System"],"title":"内存中的地址空间","uri":"/os_address/"},{"categories":["Operating System"],"content":"计算机基本硬件结构 CPU主要是完成对程序的控制 内存主要放置程序的代码和处理的数据 设备 计算机基本硬件结构 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:1:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"内存分层体系 放上这张图(这张图应该很多人都看到过) 内存延时🌰 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:0","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统需要完成的四个目标 抽象: 逻辑地址空间 我们希望应用程序在内存中运行的时候, 不用考虑很多细节(物理内存地址在什么地方, 外设在什么地方), 只需要访问一个连续的地址空间就可以了, 即逻辑地址空间. 保护: 独立地址空间 因为在内存中运行着多个不同的应用程序. 运行的过程中, 有可能会访问其他应用程序的地址空间并造成破坏, 所以就需要将不同应用程序的运行空间进行隔离. 共享: 访问相同内存 不同的程序之间除了隔离之外, 还会有交互. 所以操作系统提供共享的内存空间来完成. 虚拟化: 更多的地址空间 当内存中的程序过多的时候, 有可能会出现内存不够的情况. 这样, 就会把急需要地址空间的程序放在内存中, 暂时不需要地址空间的程序先放在硬盘上去. ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:1","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"操作系统中管理内存的不同方法 程序重定位 分段 分页 虚拟内存 按需分页虚拟内存 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:2","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"实现高度依赖于硬件 必须知道内存架构 MMU(内存管理单元): 硬件组件负责处理CPU的内存访问请求 ","date":"2022-03-16","objectID":"/os_computerarchitecture/:2:3","tags":["Operating System"],"title":"计算机体系结构及内存分层体系","uri":"/os_computerarchitecture/"},{"categories":["Operating System"],"content":"系统调用, 异常, 中断的特点 ","date":"2022-03-16","objectID":"/os_interface/:1:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"来源 系统调用: 应用程序 异常: 应用程序 中断: 外设 ","date":"2022-03-16","objectID":"/os_interface/:1:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"处理时间 系统调用: 异步或同步 异常: 同步 中断: 异步 ","date":"2022-03-16","objectID":"/os_interface/:1:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"响应 系统调用: 等待和持续 异常: 杀死或重新执行意想不到的应用程序指令 中断: 持续, 对用户应用程序是透明的 ","date":"2022-03-16","objectID":"/os_interface/:1:3","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断和异常的处理机制 中断陈外设的事件 异常是内部CPU的事件 中断和异常迫使CPU访问一些被中断和异常服务访问的功能 ","date":"2022-03-16","objectID":"/os_interface/:2:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"中断 硬件 设置中断标记**[CPU初始化]** 将内部、外部事件设置中断标记 中断时间的ID 软件 保存当前处理状态: 为了确保之后能从打断的地方能够继续执行 中断服务程序处理 清除中断标记 恢复之前保存的处理状态 ","date":"2022-03-16","objectID":"/os_interface/:2:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"异常 保存现场 异常处理 杀死产生异常的程序 重新执行异常指令 (这种情况下, 对于用户来说, 异常就是透明的) 恢复现场 ","date":"2022-03-16","objectID":"/os_interface/:2:2","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"系统调用 系统调用和前面的中断和异常从名字上看就不一样(认真脸). 应用程序需要系统提供一些服务, 而这些服务不能由应用程序直接执行, 需要由操作系统来执行, 这个过程就需要接口, 这个接口就称为系统调用接口. 举个🌰, C语言中的printf()就会触发系统调用write(). 程序访问主要是通过高层次的API接口, 而不是直接进行系统调用. Win32 API 用于 Windows POSIX API 用于 POSIX-based systems (包括UNIX, Linux, Mac OS 的所有版本) Java API 用于 JVM 通常, 每个系统调用相关的序号. 系统调用接口根据这些需要来维护表的索引. 系统调用接口调用内核态中预期的系统调用, 并返回系统调用的状态和其他任何返回值 用户不需要知道系统调用是如何实现的, 只需要获取API和了解操作系统将什么作为返回结果. 操作系统接口的细节大部分都隐藏在API中, 通过运行程序支持的库来管理(包括编译器的库来创建函数集). 系统调用和传统的函数调用是有区别的: 当应用程序发出函数调用的时候, 是在一个栈空间完成了参数的传递和参数的返回. 系统调用的执行过程中, 应用程序和OS是有各自的堆栈. 当应用程序发出系统调用的时候, 当切换到内核中执行之后也需要切换堆栈. 同时, 也需要完成特权级的转换(从用户态转为内核态). 也就是说系统调用的开销是比函数调用的开销大得多, 但是这样的做法也提高了安全性. ","date":"2022-03-16","objectID":"/os_interface/:3:0","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"跨越操作系统边界的开销 通过上面可以了解到, 其实系统调用, 异常和中断就是操作系统和应用程序, 以及操作系统和外设之间跨越了边界. 在执行时间上的开销超过程序调用 开销: 建立中断/异常/系统调用号与对应服务例程映射关系的初始化开销 建立内核堆栈 验证参数, 因为操作系统是不信任应用程序的 内核态映射到用户态的地址空间, 更新页面映射权限. 这是一个拷贝的过程, 因为不能将内核态中的数据简单的用传递指针的方式传递给用户态. 内核态独立地址空间, TLB ","date":"2022-03-16","objectID":"/os_interface/:3:1","tags":["Operating System"],"title":"系统调用、异常和中断","uri":"/os_interface/"},{"categories":["Operating System"],"content":"首先要知道, OS并没有放在内存当中, 而是放在了DISK中的. 当开机的时候, 首先是由BIOS(Basic Input Output System)检测各种各样的外设, 通过了之后才会去加载OS(由BootLoader完成). BIOS在内存中是有一个固定的地址的, 以x86为例, BIOS是存放在CS:IP = 0xf000:fff0这个地址中的(CS: 段寄存器; IP: 指令寄存器) . BootLoader启动过程 ","date":"2022-03-15","objectID":"/os_bootloader/:0:0","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"操作系统与设备和程序交互 操作系统的Interface包含三个: 系统调用(system call): 来源于应用程序主动向操作系统发出服务请求 异常(exception): 来源于不良的操作程序, 非法指令或者其他坏的处理状态(🌰 内存出错) 中断(interrupt): 来源于外设. 来自不同的硬件设备的计时器和网络的中断 ","date":"2022-03-15","objectID":"/os_bootloader/:0:1","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Operating System"],"content":"那为什么应用程序不能直接去访问外设呢, 而是通过操作系统? 首先从安全的角度, OS是一个特殊的应用软件, 和其他的应用程序最大的不同就是, OS对整个计算机有控制权, 它是可信任的. 如果应用程序直接访问外设的话, 很容易造成整个系统的崩溃. 另一个方面, 希望通过操作系统, 给上层的应用提供简单, 一致的接口, 使得应用程序不用关注底层设备的复杂性和差异性. ","date":"2022-03-15","objectID":"/os_bootloader/:0:2","tags":["Operating System"],"title":"操作系统的启动","uri":"/os_bootloader/"},{"categories":["Data Structure"],"content":"首先, 看一个🌰. 现在有一个五子棋程序, 其中有一个存盘退出和续上盘的功能. 二维数组记录棋盘 从上面这张图就能看到, 很多值就是默认值(0), 也就是说记录了很多没有意义的值, 所以就引出了稀疏数组. ","date":"2022-03-07","objectID":"/sparsearray/:0:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"基本介绍 当一个数组中大部分元素都是同一个值的数组时, 可以使用稀疏数组来保存该数组. 稀疏数组的处理方法是: 记录数组一共几行几列, 有多少个不同的值 把有不同值的元素的行列及值记录在一个小规模的数组中, 从而所辖程序的规模 // 一个正常的二维数组, 一共有7行8列, 其中大部分都是0 int[][] array = { {0,1,0,0,0,0,0,0}, {0,0,0,2,0,0,0,0}, {0,0,0,0,0,0,3,0}, {0,-4,0,0,0,-5,0,0}, {0,0,0,0,0,0,0,0}, {0,0,0,0,-6,0,0,0}, {0,0,7,0,0,0,0,0}, } 行(row) 列(column) 值(value) 7 8 7 0 1 1 1 3 2 2 6 3 3 1 -4 3 5 -5 5 4 -6 6 2 7 表中第一行记录了一共几行几列以及多少个非零值 从上面这个🌰就能看出来, 将原本需要7*8=56个空间变为了3*8=24个空间. ","date":"2022-03-07","objectID":"/sparsearray/:1:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"实现思路 二维数组 转 稀疏数组 遍历原始的二维数组, 得到有效数据的个数sum 根据sum就可以创建稀疏数组int[sum+1][3] 将二维数组的有效数据存入到稀疏数组 稀疏数组 转 二维数组 先读取稀疏数组第一行, 根据第一行的数组创建原始二维数组 在读取稀疏数组后面的数据, 并赋值给二维数组 ","date":"2022-03-07","objectID":"/sparsearray/:2:0","tags":["Data Structure","Java"],"title":"稀疏数组","uri":"/sparsearray/"},{"categories":["Data Structure"],"content":"线性结构 作为最常用的数据结构, 特点是数据元素之间存在一对一的线性关系 有两种不同的存储结构: 顺序存储结构和练市存储结构. 顺序存储的线性表称为顺序表, 顺序表中的存储元素是连续的. 链式存储的线性表称为链表, 链表中的存储元素不一定是连续的, 元素节点中存放数据元素以及相邻元素的地址信息. 线性结构常见的有: 数据, 队列, 链表和栈. ","date":"2022-03-07","objectID":"/linear_nonlinear/:1:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Data Structure"],"content":"非线性结构 已经不是一对一的关系了, 其结构包括: 二维数组, 多维数组, 广义表, 树结构, 图结构. ","date":"2022-03-07","objectID":"/linear_nonlinear/:2:0","tags":["Data Structure"],"title":"线性结构和非线性结构","uri":"/linear_nonlinear/"},{"categories":["Syntactic"],"content":"什么是fail-fast 百度百科上是这么写的: fail-fast是Java集合(Collection)中的一种错误机制. 我觉得不太全面. 下面来看一下维基百科上这怎么写的: In systems design, a fail-fast system is one which immediately reports at its interface any condition that is likely to indicate a failure. Fail-fast systems are usually designed to stop normal operation rather than attempt to continue a possibly flawed process. Such designs often check the system’s state at several points in an operation, so any failures can be detected early. The responsibility of a fail-fast module is detecting errors, then letting the next-highest level of the system handle them. 从上面这段话就能看出来, fail-fast是在系统设计当中的一种错误检测机制, 一旦检测到可能发生错误, 就立即抛出异常, 程序不再继续运行. ","date":"2022-03-06","objectID":"/failfast/:1:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"集合中的fail-fast 首先, 来复现这个错误: List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 运行上面的代码就会抛出这样的异常: Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) 上面的代码是在for-each中要删除集合中的元素而抛出的异常. 同样的, 增加(add())元素也会抛出这个异常. ","date":"2022-03-06","objectID":"/failfast/:2:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"异常产生的原因 来看一下源码: public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // ... } public boolean remove(Object o) { if (o == null) { for (int index = 0; index \u003c size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u003c size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } private void fastRemove(int index) { modCount++; // ... } 从这段代码可以发现, 对ArrayList的add(), remove(), clear()方法, 只要涉及到改变集合中的元素的个数的方法都会导致modCount的改变, 但是没有对expectedModCount进行改变. final void checkForComodification() { if (expectedModCount != modCount) throw new ConcurrentModificationException(); } 当expectModCount和modCount不同的时候, 就会抛出一开始出现的异常. ","date":"2022-03-06","objectID":"/failfast/:3:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"避免产生异常 ","date":"2022-03-06","objectID":"/failfast/:4:0","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"1. 使用普通for循环进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (int i = 0; i \u003c nameList.size(); i++) { String name = nameList.get(i); if (name.equals(\"丑八怪\")) { nameList.remove(name); } } 这样虽然不会报错, 但是可能会产生漏删的情况出现. ","date":"2022-03-06","objectID":"/failfast/:4:1","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"2. 使用Iterator进行操作 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; Iterator\u003cString\u003e iterator = nameList.iterator(); while (iterator.hasNext()) { if (iterator.next().equals(\"丑八怪\")) { iterator.remove(); } } ","date":"2022-03-06","objectID":"/failfast/:4:2","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"3. 其实使用for-each也可以 List\u003cString\u003e nameList = new ArrayList\u003cString\u003e() {{ add(\"张三\"); add(\"李四\"); add(\"小王\"); add(\"丑八怪\"); }}; for (String name : nameList) { if (name.equals(\"丑八怪\")) { nameList.remove(name); break; } } 当集合中只删除一个元素的时候, 在删除操作之后立即break, 使循环不进入下一次遍历就可以了. ","date":"2022-03-06","objectID":"/failfast/:4:3","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"4. 使用fail-safe的集合类 ","date":"2022-03-06","objectID":"/failfast/:4:4","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Syntactic"],"content":"5. 使用stream中的filter","date":"2022-03-06","objectID":"/failfast/:4:5","tags":["Syntactic"],"title":"Fail-fast","uri":"/failfast/"},{"categories":["Data Structure"],"content":" 前两天在LeetCode做题的时候, 做到了二分查找(Binary Search). 现在来做一下梳理. 地址: LeetCode-Binary Search 二分查找作为程序员的一个基本技能, 也是面试的时候有可能面试官会问到的一种算法. 可以达到O(log n)的时间复杂度. 一般来说, 当出现这几个条件的时候, 就应该用到二分查找: 待查找的数组是有序的或者是部分有序的 要求时间复杂度低于O(n), 或者直接说明时间复杂度是O(log n) 而且二分查找也有很多的变体. 在使用的时候, 要注意好查找条件, 判断条件以及左右边界的条件变更方式. 这三个地方没有注意好, 很容易就会出现死循环或是遗漏. 今天来梳理一下这几种: 标准的二分查找 二分查找左边界 二分查找右边界 二分查找的左右边界 二分查找极值点 ","date":"2022-03-01","objectID":"/binarysearch/:0:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"标准的二分查找 上面这个题目就用标准的二分查找来实现, 我们先来看标准的二分查找的模板: public int search(int[] array, int target) { int left = 0, right = array.length-1; // 1. 因为循环中包含了 left == right 的条件, 所以每次循环的时候, left或right都要有变化 while (left \u003c= right) { // 这句话其实等同于 (right+left)/2, 但是这样的写法可以避免溢出 int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] == target) { return mid; } else if (array[mid] \u003c target) { // 左边界更新 left = mid + 1; } else { // 右边界更新 right = mid - 1; } } // 未查找到目标值 return -1; } ","date":"2022-03-01","objectID":"/binarysearch/:1:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界 使用这种变体的时候通常有这几种特性: 数组有序, 包含重复元素 数组部分有序, 包含重复元素 数组部分有序, 不包含重复元素 ","date":"2022-03-01","objectID":"/binarysearch/:2:0","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界① 这种分类包含了上面的1,3两种情况. 既然是查找左边界, 就要从右侧开始, 然后不断左移. 也就是说, 即使找到了array[mid] == target, 这个mid值也不见得就是要查找的左边界. 所以还是要继续收缩: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else { right = mid; } } return array[left] == target ? left : -1; } 可以很明显的看出来和标准的有何不同: 查找条件变为了left \u003c right 因为在最后left与right相邻的时候, mid和left处在同一位置. 所以下一步, left, mid, right都会在同一位置. 也就是说, 如果判断条件还是left \u003c= right的话, 可能最后就会进入死循环. 右边界更新为right = mid 因为需要在查找到目标值之后继续向左移动. ","date":"2022-03-01","objectID":"/binarysearch/:2:1","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Data Structure"],"content":"二分查找左边界② 这种分类包含了上面的2情况, 也就是数组部分有序, 包含重复元素. 这种条件下, 右边界在向左移动的时候, 不能简单的令right = mid. 因为有重复的元素, 这样就可能会造成遗漏: public int search(int[] array, int target) { int left = 0, right = array.length-1; while (left \u003c right) { int mid = ((right - left) \u003e\u003e 1) + left; if (array[mid] \u003c target) { left = mid + 1; } else if (array[mid] \u003e target) { right = mid; } else { --right; } } return array[left] == target ? left : -1; } ","date":"2022-03-01","objectID":"/binarysearch/:2:2","tags":["Algorithm","Data Structure"],"title":"二分法查找","uri":"/binarysearch/"},{"categories":["Java"],"content":" 今天进行了一场面试, 面试官在问我关于HashMap的时候, 感觉自己回答的不是很好, 所以现在索性就梳理一下Java关于集合的这部分知识. 主要是问了这么几个问题: HashMap是线程安全的么 那线程安全的map是哪种? 在定义HashMap的时候会有定义长度的习惯么? HashMap的底层是怎么实现的? HashMap是如何存储的? HashMap最大长度是多少? 或者说是达到多大的长度就需要扩容了? (这个没答上来…😭) 说到Java的Collection就一定会放出这张神图 根据这张图能发现, 这一切的一切都起始于Iterable接口. ","date":"2022-02-24","objectID":"/java_collection/:0:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Iterable 从源码里能看到, 这个接口允许对象成为for-each的循环目标, 也就是增强型for循环, 是Java中的一种语法糖. List\u003cObject\u003e list = new ArrayList(); // 补充: 数组也可`for-each`遍历 // Object[] list = new Object[5]; for (Object obj: list) {} 其他遍历方式 JDK 1.8 之前, Iterable只有一个方法: Iterator\u003cT\u003e iterator(); 这个接口能够创建一个轻量级的迭代器, 用于安全的遍历元素, 移除元素, 添加元素. 其中涉及了一个概念就是fail-fast. 总结起来就是: 能创建迭代器进行元素添加和删除的话, 就尽量使用迭代器进行添加和删除操作. for (Iterator it = list.iterator(); it.hasNext(); ) { System.out.println(it.next()); } ","date":"2022-02-24","objectID":"/java_collection/:1:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"顶层接口 Collection是一个顶层接口, 主要是用来定义集合的约定. List接口也是一个顶层接口, 继承了Collection接口, 同时也是ArrayList, LinkedList等集合元素的父类. Set接口位于与List接口同级的层次上, 它同时也继承了Collection接口. Set接口提供了额外的规定. 对add(), equals(), hashCode()方法提供了额外的标准. Queue是和List, Set接口并列的Collection的三大接口之一. Queue的设计用来在处理之前保持元素的访问次序. 除了Collection基础的操作外, 对立提供了额外的插入, 读取, 检查操作. SortSet接口直接继承与Set接口, 使用Comparable对元素进行自然排序或者使用Comparator在创建时对元素提供定制的排序规则. set的迭代器将按升序元素顺序遍历集合. Map是一个支持key-value存储的对象, Map不能包含重复的key, 每个键最多映射一个值. 这个接口替代了Dictionary类, Dictionary是一个抽象类而不是接口. ","date":"2022-02-24","objectID":"/java_collection/:2:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"ArrayList ArrayList是实现List接口的可扩容数组(动态数组), 它的内部是基于数组实现的, 具体的定义: public class ArrayList\u003cE\u003e extends AbstractList\u003cE\u003e implements List\u003cE\u003e, RandomAccess, Cloneable, java.io.Serializable {...} ArrayList可以实现所有可选择的列表操作, 允许所有元素 (包括 null). ArrayList还提供了内部存储list的方法, 它能够完全替代Vector, 只有一点例外, ArrayList不是线程安全的容器. 下面会说到Vector ArrayList有一个容量的概念, 这个数组的容量就是List用来存储元素的容量. 在不声明容量的时候, 默认的是10. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.5, 即: 旧容量 * 1.5 ==\u003e 10-\u003e15-\u003e22-\u003e33... ArrayList的上限为Integer.MAX_VALUE - 8(232 - 8). ArrayList不是线程安全的容器, 所以可以使用线程安全的List: List list = Collections.synchronizedList(new ArrayList\u003c\u003e()); ArrayList具有fail-fast快速失败机制, 当在迭代集合的过程中, 该集合发成了改变的时候, 就可能会发生fail-fast, 抛出ConcurrentModificationException异常. ","date":"2022-02-24","objectID":"/java_collection/:3:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Vector Java中的Vector类是允许不同类型共存的变长数组, Java.util.Vector提供了向量(Vector)类以实现类似动态数组的功能. 在相对于ArrayList来说, Vector线程是安全的, 也就是说是同步的. 因为Vector对内部的每个方法都是简单粗暴的上锁, 所以访问元素的效率远远低于ArrayList. 还有一点在扩容上, ArrayList扩容后的数组长度会增加50%, 而Vector的扩容长度后数组是翻倍. ","date":"2022-02-24","objectID":"/java_collection/:4:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedList LinkedList是一个双向链表, 允许所有元素 (包括 null): LinkedList所有的操作都可以表现为双向性, 索引到链表的操作将遍历从头到尾, 看那个距离短为遍历顺序. LinkedList不是线程安全的容器, 所以可以使用线程安全的Set: List list = Collections.synchronizedList(new LinkedList\u003c\u003e()); 因为LinkedList是一个双向链表, 所以没有初始化大小, 没有扩容机制. ","date":"2022-02-24","objectID":"/java_collection/:5:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"Stack 堆栈(Stack)就是常说的后入先出的容器. 它继承了Vector类, 提供了常用的pop, push和peek操作, 以及判断stack是否为空的empty方法和寻找与栈顶距离的search方法. ","date":"2022-02-24","objectID":"/java_collection/:6:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashSet HashSet是Set接口的实现类, 有哈希表支持 (实际上HashSet是HashMap的一个实例), 不能保证集合的迭代顺序. 允许所有元素 (包括 null). HashSet不是线程安全的容器, 所以可以使用线程安全的Set: Set set = Collections.synchronizedSet(new HashSet\u003c\u003e()); 支持fail-fast机制. 因为HashSet的底层实际使用HashMap实现的, 所以和HashMap的容量和扩容机制是一致的: 在不声明容量的时候, 默认的是16. 当达到当前容量上限的时候, 就会进行扩容, 负载因子为0.75, 即: 旧容量 * 1.75 ==\u003e 16-\u003e28-\u003e49-\u003e85... ","date":"2022-02-24","objectID":"/java_collection/:7:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeSet TreeSet是一个基于TreeMap的NavigableSet实现. 这些元素使用他们的自然排序或者在创建时提供的Comparator进行排序, 具体取决于使用的构造函数. 此实现为基本操作add, remove, contains提供了log(n)的时间成本. HashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:8:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashSet LinkedHashSet继承体系 LinkedHashSet是Set接口的Hash表和LinkedList的实现. 但是这个实现不同于HashSet的是, 它维护者一个贯穿所有条目的双向列表. 此链表定义了元素插入集合的顺序. 注意: 如果元素重新插入, 则插入顺序不会受到影响. LinkedHashSet有两个影响其构成的参数: 初始容量和负载因子. 它们的定义与HashSet完全相同. 但是对于LinkedHashSet, 选择过高的初始容量值的开销要比HashSet小, 因为LinkedHashSet的迭代次数不收容量影响. LinkedHashSet不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:9:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"PriorityQueue PriorityQueue(优先级队列)是AbstractQueue的实现类, 其中的元素根据自然排序(最小元素最先出)或者通过构造函数时期提供的Comparator来排序, 具体根据构造器判断. 注意: PriorityQueue不允许null元素. 队列的头在某种意义上是指定顺序的最后一个元素. 队列查找操作poll, remove, peek和element访问队列头部元素. PriorityQueue是无界队列(无限制的), 但是有内部capacity, 用户控制用于在队列中存储元素的数组大小. 该类以及迭代器实现了Collection, Iterator接口的所有可选方法. 这个迭代器提供了iterator()方法不能保证以任何特定顺序遍历PriorityQueue. 如果需要有序遍历的话, 可以考虑使用Arrays.sort(pq.toArray()). PriorityQueue必须存储的可比较的对象, 如果不是的话, 则必须指定比较器. PriorityQueue不是线程安全的容器, 而线程安全的类是PriorityBlockingQueue. ","date":"2022-02-24","objectID":"/java_collection/:10:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashMap HashMap是一个利用哈希表原理来存储元素的集合, 允许空的key-value键值对. HashMap的默认初始用量和负载因子和HashSet一致. HashMap不是线程安全的容器. ","date":"2022-02-24","objectID":"/java_collection/:11:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"TreeMap 一个基于NavigableMap实现的红黑树. 这个map根据key自认排序存储, 或者通过Comparator进行定制排序. TreeMap为containsKey,get,put和remove方法提供了log(n)的时间开销. TreeMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:12:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"LinkedHashMap LinkedHashMap是Map接口的哈希表和链表的实现. 这个实现与HashMap的不同之处在于它维护了一个贯穿其所有条目的双向链表. 这个链表中定义的顺序, 通常是插入的顺序. 提供了一个特殊的构造器: LinkedHashMap(int,float,boolean), 其遍历的顺序是其最后一次访问的顺序. 可以重写removeEldestEntry(Map.Entry)方法, 以便在将新映射添加到map时强制删除过期映射的策略. 这个类提供了所有可选择的map操作, 并且允许null元素. 由于维护链表的额外开销, 性能可能会低于HashMap, 有一条除外: 遍历LinkedHashMap中的collection-views需要与map.size成正比, 无论其容量如何. HashMap的迭代看起来开销更大, 因为还要求时间与其容量成正比. LinkedHashMap有两个因素影响了它的构成: 初始容量和负载因子. LinkedHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:13:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"HashTable 与HashMap不同的是, HashTable是线程安全的. 任何非空对象都可以用作键或值. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:14:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"IdentityHashMap IdentityHashMap 是一个比较小众的Map实现类. IdentityHashMap不是一个通用的Map实现, 虽然这个类实现了Map接口, 但是它故意违反了Map的约定, 该约定要求在比较对象时使用equals方法, 此类仅适用于需要引用相等语义的极少数情况. IdentityHashMap不是线程安全的容器. 支持fail-fast机制. ","date":"2022-02-24","objectID":"/java_collection/:15:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"WeakHashMap WeakHashMap类基于哈希表的Map基础实现, 带有弱键. WeakHashMap中的entry当不再使用时还会自动移除. 也就是说, WeakHashMap中的entry不会增加其引用计数. 基于map接口, 是一种弱键相连, WeakHashMap里面的键会自动回收. 支持null键和null值. 支持fail-fast机制 WeakHashMap经常用作缓存. ","date":"2022-02-24","objectID":"/java_collection/:16:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Java"],"content":"集合实现类特征图 下面这个表格汇总了部分集合框架的主要实现类的特征 集合 排序 随机访问 key-value存储 重复元素 空元素 线程安全 ArrayList ✅ ✅ ❌ ✅ ✅ ❌ LinkedList ✅ ❌ ❌ ✅ ✅ ❌ HashSet ❌ ❌ ❌ ❌ ✅ ❌ TreeSet ✅ ❌ ❌ ❌ ❌ ❌ HashMap ❌ ✅ ✅ ❌ ✅ ❌ TreeMap ✅ ✅ ✅ ❌ ❌ ❌ Vector ✅ ✅ ❌ ✅ ✅ ✅ HashTable ❌ ✅ ✅ ❌ ❌ ✅ ConcurrentHashMap ❌ ✅ ✅ ❌ ❌ ✅ Stack ✅ ❌ ❌ ✅ ✅ ✅ CopyOnWriteArrayList ✅ ✅ ❌ ✅ ✅ ✅ ","date":"2022-02-24","objectID":"/java_collection/:17:0","tags":["Java"],"title":"Java集合","uri":"/java_collection/"},{"categories":["Security"],"content":"什么是SQL注入 SQL注入攻击是通过将恶意的SQL语句插入到应用的输入参数中, 再在后台SQL服务器上解析执行的攻击. 是目前对数据库进行攻击的最常用手段之一. 主要原因是程序对用户输入数据的合法性没有判断和处理. ","date":"2022-02-24","objectID":"/sqlinjection/:0:1","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"原理 恶意拼接查询 都知道SQL语句是用;进行分隔两个语句的: SELECT * FROM users WHERE user_id = $user_id; 其中, user_id是传入参数, 但如果传入的参数变为1;DELETE FROM users;, 那么语句最终就会变为: SELECT * FROM users WHERE user_id = 1;;DELETE FROM users; 如果执行了上面的语句, 那么user表中所有数据都被删除了. 利用注释执行非法命令 SQL语句中可以添加注释: SELECT * FROM users WHERE user_gender='男' AND user_age=$age 如果user_age中包含了恶意的字符串20 OR 25 AND SLEEP(500)--, 那么语句最终会变为: SELECT * FROM users WHERE user_gender='男' AND user_age=20 OR 25 AND SLEEP(500)-- 上面这条语句只是想耗尽系统资源, SLEEP(500)会一直执行, 但是如果其中添加了修改, 删除数据的语句, 将会造成更大的破坏. 传入非法参数 SQL语句中的字符串是用单引号包裹的, 但是如果其本身包含单引号而没有处理, 那么就可能篡改SQL语句的作用: SELECT * FROM users WHERE user_name=$user_name 如果user_name传入参数值M'ustard, 那么语句最终会变为: SELECT * FROM users WHERE user_name='M'ustard' 一般情况下, 执行上面语句就会报错, 但是这种方式可能会产生恶意的SQL语句. 添加额外条件 在SQL语句中添加一些额外添加, 来改变执行行为. 条件一般为真值表达式: UPDATE users SET user_password=$user_password where user_id=$user_id 如果user_id传入的是1 OR TRUE, 那么语句最终会变为: UPDATE users SET user_password='123456' where user_id=1 OR TRUE 如果执行了上面的语句, 那么所有用户的密码都被更改了. ","date":"2022-02-24","objectID":"/sqlinjection/:0:2","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"防御手段 过滤输入内容, 校验字符串 过滤掉用户输入中的不合法字符剔除掉, 可以使用编程语言提供的处理函数或自己封装的函数来过滤, 也可以使用正则表达式来进行匹配. 也要验证参数的类型, 比如字符串或者整型. 参数化查询 参数化查询是目前被视作预防SQL注入攻击最有效的方法. 指的设计数据库连接并访问数据时, 在需要填入数据的地方, 使用参数(Parameter)来给值. MySQL的参数格式是以?加上参数名称而成: UPDATE table_1 SET row_1=?row_1, row_2=row_2 WHERE row_3=?row_3 在使用参数化查询下, 数据库不会将参数的内容视为SQL语句的一部分来处理, 而是在数据库完成SQL语句的编译之后, 才套用参数执行. 因此就算参数中含有破坏性的指令, 也不会被数据库所运行. 安全测试, 安全审计 避免使用动态SQL 不要将敏感数据保留在纯文本中 限制数据库权限和特权 避免直接向用户显示数据库错误 ","date":"2022-02-24","objectID":"/sqlinjection/:0:3","tags":["Security","MySQL"],"title":"SQL注入","uri":"/sqlinjection/"},{"categories":["Security"],"content":"什么是CSRF攻击 CSRF攻击全称跨站请求伪造(Cross-site request forgery), 也被称为one-click attack或session riding, 通常缩写为CSRF或者XSRF. 尽管CSRF和XSS攻击很像, 但是两者的方式截然不同, 甚至可以说是相左的. XSS利用的是用户对指定网站的信任, CSRF利用的是网站对用户网页浏览器的信任. 我理解的是XSS是攻击者对浏览器下手, CSRF是对用户下手. 可以这么理解: 攻击者盗用受信任用户的身份, 以该用户的名已发送恶意请求. ","date":"2022-02-23","objectID":"/csrf/:0:1","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"CSRF攻击的原理 用户1访问受信任的网站A, 输入用户名和密码登录网站A. 登录成功之后, 网站A产生Cookie信息并返回给浏览器. 用户1在未退出登录网站A的情况下, 在同一浏览器, 访问网站B. 网站B接收到请求后, 携带网站A的Cookie信息, 发出一个请求去访问网站A 网站A在接收到网站B的请求之后, 会以用户1的权限处理该请求, 从而导致用户1的隐私泄漏和财产安全受到威胁. ","date":"2022-02-23","objectID":"/csrf/:0:2","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"常见的几种类型 GET类型的CSRF: 这种类型的CSRF利用非常简单, 只需要一个HTTP请求: \u003cimg src=http://xxx.com/csrf?user=xxx /\u003e 在访问这个img的页面后, 成功发出了一次HTTP请求. POST类型的CSRF: 这种类型的CSRF没有GET型的大, 利用起来通常使用的是一个自动提交的表单: \u003cform action=http://xxx.com/csrf.php method=POST\u003e \u003cinput type=\"text\" name=\"user\" value=\"xxx\" /\u003e \u003c/form\u003e \u003cscript\u003edocument.forms[0].submit();\u003c/script\u003e 其他类型CSRF: \u003cimg src=http://admin:admin@192.168.1.1 /\u003e 在访问这个img的页面后, 路由器会给用户一个合法的SESSION, 就可以进行下一步操作了. ","date":"2022-02-23","objectID":"/csrf/:0:3","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":"防御手段 目前防御CSRF攻击主要有三种策略: 检查HTTP Referer字段 根据HTTP协议, 在HTTP头中有一个字段叫Referer, 它记录了该HTTP请求的来源地址. 优势: 简单易行, 网站的普通开发人员不需要操心CSRF的漏洞, 只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查Referer的值就可以了. 特别是对于已有的系统来说, 不需要更改当前系统的任何代码和逻辑, 没有风险. 劣势: 这种方式是把安全性都依赖于浏览器来保障, 对于一些低版本的浏览器来说, 是可以篡改Referer值的. 对于某些最新的浏览器, 虽然不能篡改Referer的值, 但是用户有些时候会认为Referer值会记录下用户的访问来源, 所以用户自己可以设置浏览器在发送请求的时候不再提供Referer. 这种情况下, 此种方式就会认为是CSRF攻击, 从而拒绝合法用户的访问. 添加校验token CSRF之所以能够成功是因为攻击者获取到了Cookie信息. 如果能够不只是依靠Cookie中的信息来抵御CSRF攻击, 那么就可以防御了. 所以这种方式就是在HTTP请求中以参数的形式加入一个随机产生的token, 并且在服务器端建立一个拦截器来验证这个token. 优势: 这种方法比检查Referer更安全一些, token可以在用户登录之后产生并放与SESSION中, 然后每次请求时把token从SESSION中取出来进行比对. 劣势: 难以保证token本身的安全. 疑问: 可以做到每次验证token成功之后, 产生一个新的token是否可以避免此种方法可能产生的漏洞. 在HTTP头中自定义属性并验证 这种方法也是使用token进行验证, 但是不是放在参数中, 而是放在HTTP请求头的自定义属性中. 通过XMLHttpRequest这个类, 可以一次性给所有该类请求加上csrftoken这个HTTP头属性, 并把token放进去. 优势: 不会记录到浏览器的地址栏, 统一管理token输入输出, 可以保证token的安全性. 劣势: 无法在非异步的请求上实施. ","date":"2022-02-23","objectID":"/csrf/:0:4","tags":["Security","Network","HTTP"],"title":"CSRF攻击","uri":"/csrf/"},{"categories":["Security"],"content":" 昨天的时候, 部门老大提到session和OnceToken的时候, 说到了xss攻击和csrf攻击. 今天记录一下学习的内容. ","date":"2022-02-23","objectID":"/xss/:0:0","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"什么是XSS攻击 XSS攻击全称跨站脚本攻击(Cross Site Scripting), 是一种在Web应用中的计算机安全漏洞, 它允许恶意Web用户将代码植入到提供给其他用户使用的正常页面中. 之所以缩写是XSS, 是因为如果是CSS会和层叠样式表混淆(Cascading Style Sheets). 举个简单的🌰, 恶意服务器嵌套了正常服务器中某页面的某form表单. 当用户登录了正常服务器之后, 在恶意服务器上也可以提交这个表单, 甚至拿到更高的权限. XSS是最普遍的Web应用安全漏洞. 可以做到劫持用户会话, 插入恶意内容, 重定向用户, 使用恶意软件劫持用户浏览器, 繁殖XSS蠕虫, 甚至是破坏网站, 修改路由器配置信息等. 所以XSS攻击的危害还是很严重的. ","date":"2022-02-23","objectID":"/xss/:0:1","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"XSS原理 首先看看XSS可以插入哪里: script标签内容 HTML注释内容 HTML标签的属性名 HTML标签的属性值 HTML标签的名字 直接插入到CSS里 看到XSS可以插入到这些地方之后, 就更能理解它的原理. XSS通过一些被特殊对待的文本和标记(🌰, 小于符号\u003c 被看做是HTML标签的开始), 使得用户浏览器将这些误认为是插入了正常的内容, 所以就会在用户浏览器中被执行. 也就是说, 当这些特殊字符不能被动态页面检查或检查出现失误时, 就将会产生XSS漏洞. ","date":"2022-02-23","objectID":"/xss/:0:2","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"分类 存储型XSS攻击(持久型): 最直接的危害类型. 将XSS代码提交存储到服务器端(数据库, 内存, 文件系统等). 这样下次请求目标页面时就不用再提交XSS代码, 会从服务器获取. 一般出现在留言, 评论, 博客日志等交互. 存储型XSS攻击 反射性XSS攻击(非持久型): 最普遍的类型. 通过特定手法(🌰email), 诱使用户访问一个包含恶意代码的地址, 当用户点击这些链接的时候, 恶意代码会在用户的浏览器执行. 一般出现在搜索栏, 用户登录口, 常用来窃取客户端Cookies或进行钓鱼欺骗. 反射型XSS攻击 DOM-based 型XSS攻击: 通过/xss修改页面的DOM(Document Object Model)结构, 是纯粹发生在客户端的攻击. 在整个攻击过程中, 服务器响应的页面没有发生变化, 取出和执行恶意代码都由浏览器端完成, 属于前端自身的安全漏洞. ","date":"2022-02-23","objectID":"/xss/:0:3","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":["Security"],"content":"防御手段 总体思路: 对用户的输入(和URL参数)进行过滤, 对输出进行编码. 也就是说, 对用户提交的所有内容进行过滤, 对URL中的参数进行过滤, 过滤掉会导致脚本执行的相关内容; 然后对动态输出到页面的内容进行html编码, 使脚本无法在浏览器中执行. 还可以服务端设置会话Cookie的HTTP Only属性, 这样客户端的JS脚本就不能获取Cookie信息了. ","date":"2022-02-23","objectID":"/xss/:0:4","tags":["Security","Network","HTTP"],"title":"XSS(跨站脚本攻击)","uri":"/xss/"},{"categories":null,"content":"Java Overview (Java Platform SE 8 ) (oracle.com) Java 8 中文版 Overview (Java SE 11 \u0026 JDK 11 ) (oracle.com) Java 11 中文版 Nacos Maven各版本地址 ","date":"2022-02-22","objectID":"/documents/:1:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Big Data Apache Kafka The Scala Programming Language (scala-lang.org) Apache Flink: Stateful Computations over Data Streams Apache SeaTunnel ","date":"2022-02-22","objectID":"/documents/:2:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"iOS Swift - Resources - Apple Developer Vapor Xcode Releases ","date":"2022-02-22","objectID":"/documents/:3:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":null,"content":"Other 力扣（LeetCode）中国官网 Markdown 入门基础 | Markdown 官方教程 飞桨PaddlePaddle-源于产业实践的开源深度学习平台 Elasticsearch Guide 8.0 ","date":"2022-02-22","objectID":"/documents/:4:0","tags":null,"title":"文档","uri":"/documents/"},{"categories":["MySQL"],"content":"有了数据库之后, 还需要先进行压测 拿到一个数据库之后, 首先得先对这个数据库进行一个较为基本的基准压测. 也就是说, 你得基于一些工具模拟一个系统每秒发出1000个请求到数据库上去, 观察一下他的CPU负载、磁盘IO负载、网络 IO负载、内存复杂, 然后数据库能否每秒处理掉这1000个请求, 还是每秒只能处理500个请求? 这个过程, 就是压测. 那为什么不等到Java系统都开发完之后, 直接让Java系统连接上MySQL数据库, 然后直接对Java系统进行压测呢? 因为数据库的压测和它上面的Java系统的压测, 其实是两回事, 首先得知道数据库最大能抗多大压力, 然后再去看Java系统能抗多大压力. 因为有一种可能是, 数据库每秒可以抗下2000个请求, 但是Java系统每秒只能抗下500个请求. 所以不能光是对Java系统去进行压测, 在那之前也得先对数据库进行压测, 做到心里有个数. ","date":"2022-01-10","objectID":"/mysql_4/:1:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"QPS和TPS到底有什么区别 既然是要压测, 那么肯定得先明白一点, 每秒能抗下多少个请求, 其实是有专业术语的, 分别是QPS和TPS. QPS: Query Per Second. 也就是说数据库每秒可以处理多少个请求, 大致可以理解为一次请求就是一条SQL语句, 也就是说数据库可以每秒处理多少个SQL语句. Java系统或者中间件系统在进行压测的时候, 也可以使用这个指标. TPS: Transaction Per Second. 指的是每秒可以处理的事务量. 就是说数据库可以每秒处理多少次事务提交或者回滚. ","date":"2022-01-10","objectID":"/mysql_4/:2:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"IO相关的压测性能指标 IOPS: 指的是机器的随机IO并发处理能力. 举个🌰: 机器可以达到200 IOPS, 意思就是说每秒可以执行200个随机IO读写请求. 这个指标很关键, 因为在之前说过, 在内存中更新的脏数据库, 最后都由后台IO线程在不确定的时间, 刷回到磁盘中去, 这就是随机IO的过程. 也就是说, 如果IOPS指标太低, 那么就会导致内存里的脏数据库刷回磁盘的效率不高. 吞吐量: 指的是机器的磁盘存储每秒可以读写多少字节的数据量. 这个指标也很关键, 之前也说过, 在执行各种SQL语句的时候, 提交事务的时候, 其实都是大量的会写redo log之类的日志的, 这些日志都会直接写磁盘文件. 所以一台机器的存储每秒可以读写多少字节的数据量, 就决定了他每秒可以把多少redo log之类的日志写入到磁盘里去. 一般来说, 我们写redo log之类的日志, 都是对磁盘文件进行顺序写入的, 也就是一行接着一行的写, 不会说进行随机的读写, 那么一般普通磁盘的顺序写入的吞吐量每秒都可以达到200MB左右. 所以通常而言, 机器的磁盘吞吐量都是足够承载高并发请求的. latency: 指的是往磁盘里写入一条数据的延迟. 这个指标同样重要, 因为执行SQL语句的和提交事务的时候, 都需要顺序写redo log磁盘文件, 所以此时写一条日志到磁盘文件里去, 到底是延迟1ms, 还是延迟100us, 这就对数据库的SQL语句执行性能是有影响的. ","date":"2022-01-10","objectID":"/mysql_4/:3:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"压测的时候要关注的其他性能指标 CPU负载: 这个也是一个很重要的指标. 因为假设数据库压测到了3000 QPS, 可能其他指标都还正常, 但是此时CPU负载特别高, 那么也说明你的数据库不能继续往下压测更高的QPS了, 否则CPU是吃不消的. 网络负载: 这个就是看机器带宽情况下, 在压测到一定的QPS和TPS的时候, 每秒钟机器的网卡会输入多少MB数据, 会输出多少MB数据. 因为有可能网络带宽最多每秒传输100MB的数据, 那么可能QPS到1000的时候, 网卡就打满了, 已经每秒传输100MB的数据了, 此时即使其他指标还正常, 也不能继续压测下去了. 内存负载: 这个就是看压测到一定情况下的时候, 机器内存损耗了多少, 如果说机器内存损耗过高了, 说明也不能继续压测下去了. ","date":"2022-01-10","objectID":"/mysql_4/:4:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"推荐压测工具 sysbench, 这个工具可以自动帮你在数据库里构建出来大量的数据. 然后也可以模拟几千个线程并发的访问数据库, 模拟各种sql语句, 包括各种书屋提交到数据库里, 甚至可以模拟出几十万的TPS对数据库进行压测. ","date":"2022-01-10","objectID":"/mysql_4/:5:0","tags":["Database","MySQL"],"title":"MySQL压力测试","uri":"/mysql_4/"},{"categories":["MySQL"],"content":"生产数据库一般用什么配置的机器 首先要明确的一点, 如果系统是一个没什么并发访问量, 用户就几十个人或者几百个人的系统, 那么其实选择什么样的机器去部署数据库, 影响不大. 哪怕是个人笔记本电脑去部署一个MySQL数据库, 其实也能支撑地并发系统的运行. 因为这种系统可能每个几分钟才会有一波请求发到数据库上, 而且数据库里一张表也许就几百条, 几千条或者是几万条. 数据量很小, 并发量很小, 操作频率很低, 用户量很小, 并发量很小, 只不过可能系统的业务逻辑很复杂而已. 对于这类系统的数据库机器选型, 什么样都可以. ","date":"2021-12-27","objectID":"/mysql_3/:1:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"普通的Java应用系统部署在机器上能抗多少并发 通常来说, Java应用系统部署的时候常选用的机器配置大致是2核4G和4核8G的较多一些, 数据库部署的时候常选用的机器配置最低在8核16G以上, 正常在16核32G. 那么以大量的高并发线上系统的生产经验观察下来而言, 一般Java应用系统部署在4核8G的机器上, 每秒钟抗下500左右的并发访问量, 差不多是比较合适的. 当然这个也不是绝对的, 假设每个请求花费1s可以处理完, 那么一台机器每秒也许只可以处理100个请求, 但是如果每个请求只要花费100ms就可以处理完, 那么一台机器每秒也许就可以处理几百个请求. 所以一台机器能抗下每秒多少请求, 往往是跟每个请求处理耗费多长时间关联的. 但是大体上来说, 根据大量的经验观察而言, 4核8G的机器部署普通的Java应用系统, 每秒大致能抗下几百的并发访问, 从每秒一两百请求到每秒七八百请求, 都是有可能的, 关键是每个请求耗费多长时间. ","date":"2021-12-27","objectID":"/mysql_3/:2:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"高并发场景下, 数据库应该用什么样的机器 对于数据库而言, 上文也说了, 通常推荐的数据至少是选用8核16G以上的机器更加合适. 因为要考虑一个问题, 对于我们的Java应用系统, 主要耗费时间的是Java系统和数据库之间的网络通信. 对Java系统自己而言, 如果仅仅只是系统内部运行一些普通的业务逻辑, 纯粹在自己内存中完成一些业务逻辑, 这个性能是极高极高的. 对于Java系统受到的每个请求, 耗时最多的还是发送网络请求到数据库上去, 等待数据库执行一些SQL语句, 返回结果给Java系统. 所以其实常说的Java系统压力很大, 负载很高. 其实主要的压力和复杂都是集中在依赖的那个MySQL数据库上! 因为执行大量的增删改查的SQL语句的时候, MySQL数据库需要对内存和磁盘文件进行大量的IO操作, 所以数据库往往是负载最高的! 通过经验而言, 一般8核16G的机器部署的MySQL数据库, 每秒抗个一两千并发请求是没问题的, 但是如果并发量再高一些, 假设每秒有几千并发请求, 那么可能数据库就会有危险了, 因为数据库的CPU、磁盘、IO、内存的负载都会很高, 弄不好数据库压力过大就会宕机. 对于16核32G的机器部署的MySQL数据库而言, 每秒两三千, 甚至三四千的并发也都是可以的, 但是如果达到每秒上万请求, 也是会有宕机的危险. 如果可以的话, 数据库机器周最好用SSD的硬盘而不是机械硬盘. ","date":"2021-12-27","objectID":"/mysql_3/:3:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"申请机器机器之后做好心中有数, 交给专业的DBA部署 数据库机器申请下来之后, 作为架构师要对机器做到心中有数. 比如申请的是8核16G的机器, 心里大致就该知道这个数据库每秒抗个一两千请求是可以的, 如果申请的是16核32G的机器, 那心里知道妥妥可以抗个每秒两三千, 甚至三四千的请求. 其次, 申请一台机器下来之后, 接着这台机器在有一定规模的公司里, 一定是交给公司专业的DBA去安装、部署和启动MySQL的. DBA这个时候会按照他国王的经验, 用自己的MySQL生产调优参数模板, 直接放到MySQL里去, 然后用一个参数模板去启动这个MySQL, 往往这里很多参数都是调优过的. 而且DBA还可能对linux机器一些OS内核参数进行一定的调整, 比如说最大文件句柄之类的参数, 这些参数往往也都是需要调整的. ","date":"2021-12-27","objectID":"/mysql_3/:4:0","tags":["Database","MySQL"],"title":"MySQL生产经验","uri":"/mysql_3/"},{"categories":["MySQL"],"content":"什么是InnoDB? InnoDB是第一个提供外键约束的存储引擎, 而且它对事务的处理能力是其它存储引擎无法与之比拟的. MySQL在5.5版本之后, 默认存储引擎由MyISAM修改为InnoDB. 目前, InnoDB是最重要的, 也是使用最广泛的存储引擎. 1. InnoDB优势: 支持事务安装 灾难恢复性好 使用行级锁 实现了缓冲处理 支持外键 适合需要大型数据库的网站 2. 物理存储 数据文件(表数据和索引数据): 共享表空间 独立表空间 日志文件 ","date":"2021-12-23","objectID":"/mysql_2/:1:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新语句在MySQL中是如何执行的? 首先假设有一条语句是这样的: UPDATE users SET name='xxx' WHERE id=10 这条语句是如何执行的呢? 首先肯定是系统通过一个数据库连接发送到了MySQL上, 然后经过SQL接口、解析器、优化器、执行器几个环节, 解析SQL语句, 生成执行计划, 接着由执行器负责这个计划的执行, 调用InnoDB存储引擎的接口去执行. ","date":"2021-12-23","objectID":"/mysql_2/:2:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"InnoDB的重要内存结构: 缓冲池 前面提到了InnoDB的一个优势就是\"实现了缓冲处理\", 就是通过InnoDB存储引擎中的一个非常重要的放在内存里的组件实现的, 就是缓冲池(Buffer Pool). 这个里面会存很多数据, 便于以后的查询, 要是缓冲池里有数据, 就不会去磁盘查询. 所以当执行上面那条更新语句的时候, 就会现将id=10这一行数据看看是否在缓冲池里, 如果不在的话, 那么会直接从磁盘里加载到缓冲池里来, 而且还会对这行记录加锁. ","date":"2021-12-23","objectID":"/mysql_2/:3:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"undo日志文件: 如何让你更新的数据可以回滚 接着下一步, 假设id=10这行数据的name原来是\"zhangsan\", 现在我们更新为\"xxx\", 那么此时得现将要更新的原来的值\"zhangsan\"和id=10这些信息, 写入到undo日志文件中去. 其实大家都知道, 如果执行一个更新语句是在一个事务里的话, 那么事务提交之前我们都是可以对数据进行**回滚(rollback)**的. ","date":"2021-12-23","objectID":"/mysql_2/:4:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"更新buffer pool中的缓存数据 当我们把要更新的那行记录从磁盘文件加载到缓冲池, 也对其进行加锁之后, 并且还把更新前的旧值写入undo日志文件之后, 就可以正式开始更新这行记录了. 更新的时候, 先是会更新缓冲池中的记录, 此时这个数据就是脏数据了. 为什么是脏数据: 因为此时的磁盘中id=10这行数据的name还是\"zhangsan\", 还不是\"xxx\". ","date":"2021-12-23","objectID":"/mysql_2/:5:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"Redo Log Buffer: 万一系统宕机, 如何避免数据丢失 如果按照上面的操作进行更新, 现在已经把内存里的数据进行了修改, 但是磁盘上的数据还没有修改. 就在这个时候, 系统宕机了, 该怎么办? 这个时候就必须要把对内存所做的修改写入到一个Redo Log Buffer里去, 这也是一个内存的缓冲区, 用来存放redo日志的. redo日志用来记录对什么记录进行了修改, 比如对id=10这行记录修改了name为\"xxx\", 这就是一个日志. 这个redo日志其实就是用来在MySQL突然宕机的时候, 用来恢复更新过的数据的. ","date":"2021-12-23","objectID":"/mysql_2/:6:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"如果还没提交事务, MySQL宕机了怎么办 如果还没有提交事务, 那么此时如果MySQL崩溃, 必然导致内存里Buffer Pool中的修改过的数据都丢失, 同时写入Redo Log Buffer中的redo日志也会消失. 其实此时数据丢失是不要紧的, 因为一个更新语句, 没提交事务, 就代表还没有执行成功, 此时MySQL宕机虽然导致内存里的数据都丢失了, 但是会发现, 磁盘上的数据怡然居还停留在原样子. 换句话说, id=10那行数据的name字段的值还是旧值\"zhangsan\", 所以此时这个事务就是执行失败了, 没能成功完成更新, 会收到一个数据库的异常. 然后当MySQL重启之后, 数据并没有任何变化. 所以, 如果还没提交事务时, MySQL宕机了, 不会有任何问题. ","date":"2021-12-23","objectID":"/mysql_2/:7:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"提交事务的时候将redo日志写入磁盘中 接着俩要提交一个事务了, 此时就会根据一定的策略把redo日志从redo log buffer里刷入到磁盘文件里去. 这个策略是通过innodb_flush_log_at_trx_commit来配置的, 它有几个选项. 当这个参数的值为0的时候, 那么当提交事务的时候, 不会把redo log buffer里的数据刷入磁盘文件, 此时可能都提交事务了, 结果MySQL宕机了, 然后内存里的数据全部丢失了. 这就相当于提交事务成功了, 但是由于MySQL宕机, 导致内存中的数据和redo日志都丢失了. 当这个参数的值为1的时候, 那么当提交事务的时候, 就必须把redo log buffer从内存刷入到磁盘文件里去, 只有事务提交成功, 那么redo log就必然在磁盘里了. 所以只有提交事务成功之后, redo日志一定在磁盘文件里. 也就是说, 哪怕此时buffer pool中更新过的数据还没刷新到磁盘里去, 此时内存里的数据已经是更新过name=\"xxx\", 然后磁盘上的数据还是没更新过的name=\"zhangsan\". 当MySQL重启之后, 可以根据redo日志去恢复之前做过的修改. 当这个参数的值是2的时候, 那么当提交事务的时候, 把redo日志写入磁盘文件对应的os cache里去, 而不是直接进入磁盘文件, 可能1秒之后才会吧os cache里的数据写入到磁盘文件里去. 这种模式下, 提交事务之后, redo log可能仅仅停留在os cache内存缓存里, 没实际进入磁盘文件, 玩意此时要是机器宕机了, 那么os cache里的redo log就会丢失, 同样会感觉提交事务了, 但是结果数据丢了. ","date":"2021-12-23","objectID":"/mysql_2/:8:0","tags":["Database","MySQL"],"title":"InnoDB存储引擎的架构设计","uri":"/mysql_2/"},{"categories":["MySQL"],"content":"什么是CRUD? CRUD是指在做计算处理时的增加(Create), 读取查询(Retrieve), 更新(Update)和删除(Delete). 主要是被用在描述软件系统中DataBase或者持久层的基本操作. ","date":"2021-12-13","objectID":"/mysql_1/:1:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"什么是数据库驱动? 想要访问数据库, 就需要和数据库建立一个网络连接. 那么, 建立这个网络连接的就是数据库驱动. 所以对于MySQL来说, 对应每种常见的编程语言(e.g. Java, PHP, .NET, Python, Ruby等), MySQL都会提供对应语言的MySQL驱动. 其实数据库驱动就是中间件. ","date":"2021-12-13","objectID":"/mysql_1/:2:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"数据库连接池是用来干嘛的? 首先要知道的是, 一个系统和数据库建立的连接往往都不止一个. 但是每次访问数据库的时候都建立一个连接, 然后执行SQL语句, 然后再销毁这个连接, 这种方式显然是不合适的. 因为每次建立一个数据库连接都很耗时, 效率会很低下. 所以, 就出现了数据库连接池这个东西. 一个数据库连接池里会维持多个连接, 让多个线程使用里面的不同的数据库连接去执行SQL语句, 执行完语句之后, 不是销毁这个连接, 而是把它放回池子里, 后面还能继续用. 常见的数据库连接池有DBCP, C3P0, Druid ","date":"2021-12-13","objectID":"/mysql_1/:3:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"MySQL是如何执行一条SQL语句的? ","date":"2021-12-13","objectID":"/mysql_1/:4:0","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第一步 线程: 接收SQL语句 首先, 假设数据库服务器的连接池中的某个连接收到了网络请求, 假设就是一条SQL语句, 这个工作一定是一个线程来进行处理的, 来监听请求以及读取请求数据. 当MySQL的工作线程接收到SQL语句之后, 会转交给SQL接口去执行. SQL接口(SQL Interface)就是MySQL内部提供的一个组件, 是一套执行SQL语句的接口. ","date":"2021-12-13","objectID":"/mysql_1/:4:1","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第二步 SQL接口: 解析SQL语句 那么, SQL接口又是如何执行SQL语句的呢? 比如要执行下面这条语句: SELECT id, name, age FROM users WHERE id = 1; MySQL本身也是一个系统, 是一个数据库管理系统(DBMS), 是没法直接理解这些语句的, 所以就需要**查询解析器(Parser)**出场了! 这个查询解析器是负责对SQL语句进行解析的, 比如上面的语句拆解一下, 可以拆解为一下几个部分: 我们现在要从users表里查询数据 查询id字段的值等于1的那行数据 对查出来的哪行数据要提取里面的id, name, age三个字段 所谓的SQL解析, 就是按照既定的SQL语法, 对我们按照SQL语法规则编写的SQL语句进行解析, 然后理解这个SQL语句要干什么事情. ","date":"2021-12-13","objectID":"/mysql_1/:4:2","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第三步 查询优化器: 选择最优查询路径 当通过解析器理解了SQL语句要干什么之后, 接着就会找查询优化器(Optimizer)来选择一个最优的查询路径. ","date":"2021-12-13","objectID":"/mysql_1/:4:3","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第四步 存储引擎: 调用存储引擎接口, 真正执行SQL语句 最后一步, 就是把查询优化器选择的最有查询路径交给底层的存储引擎去真正的执行. 存储引擎是MySQL架构设计中很有特色的一个环节. 在真正执行SQL语句的时候, 要不是更新数据, 要不是查询数据, 但是具体的数据是存放在内存里还是在磁盘里呢? 这个时候就需要存储引擎了, 存储引擎其实就是执行SQL语句的, 它是按照一定的步骤去查询内存缓存数据, 更新磁盘数据, 查询磁盘数据等等, 执行诸如此类的一系列的操作. MySQL的架构设计中, SQL接口, SQL解析器, 查询优化器其实都是通用的, 就是一套组件而已. 但是是支持各种各样的存储引擎的, 比如常见的InnoDB, MyISAM, Memory等等, 我们是可以选择使用哪种存储引擎来负责具体的SQL语句执行的. ","date":"2021-12-13","objectID":"/mysql_1/:4:4","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":["MySQL"],"content":"第五步 执行器: 根据执行计划调用存储引擎的接口 现在回过头来看一个问题, 存储引擎可以访问内存和磁盘上的数据, 那么是谁来调用存储引擎的接口呢? 其实还漏了一个执行器的概念, 执行器会根据优化器选择的执行方案, 去调用存储引擎的接口按照一定的顺序和步骤, 就把SQL语句的逻辑给执行了. 举个🌰: 比如执行器可能会先调用存储引擎的一个接口, 去获取users表中的第一行数据, 然后判断一下这个数据的id字段的值是否等于我们期望的值, 如果不是的话, 就继续调用存储引擎的接口, 去获取users表的下一行数据. 基于上述的思路, 执行器就会去根据优化器生成的一套执行计划, 然后不停的调用存储引擎的各种接口去完成SQL语句的执行计划, 大致就是不停的更新或者提取一些数据出来. ","date":"2021-12-13","objectID":"/mysql_1/:4:5","tags":["Database","MySQL"],"title":"MySQL总览","uri":"/mysql_1/"},{"categories":null,"content":"初衷 Since: 2021-11-20 08:27:00 ","date":"2021-11-20","objectID":"/about/:1:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"期许 不卑不亢，不矜不伐，戒骄戒躁 不嗔不怒，不争不弃，独善其身 ","date":"2021-11-20","objectID":"/about/:2:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"About me 在职: iOS开发程序猿, Java开发程序猿, 大数据开发小学徒 用我所学, 学我所用. 不盲目堆叠技术栈, 保持谦逊, 保持探索欲, 砥砺前行. ","date":"2021-11-20","objectID":"/about/:3:0","tags":null,"title":"关于 Buli Home","uri":"/about/"},{"categories":null,"content":"Other Annual Summary /years/ ","date":"2021-11-20","objectID":"/about/:4:0","tags":null,"title":"关于 Buli Home","uri":"/about/"}]